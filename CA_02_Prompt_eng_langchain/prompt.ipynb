{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5eb0df61",
      "metadata": {
        "id": "5eb0df61"
      },
      "source": [
        "# Generative AI ‚Äì Homework 02: Prompt Engineering\n",
        "\n",
        "**Author:** Mahdy Mohtari\n",
        "\n",
        "**Course:** GenAI ‚Äì Daneshkar\n",
        "\n",
        "**Assignment Title:** Prompt Engineering\n",
        "\n",
        "**Date:** 26/7/1404\n",
        "\n",
        "---\n",
        "\n",
        "### üìò Overview\n",
        "\n",
        "This notebook contains the implementation and analysis for **Homework 02: Prompt Engineering**.\n",
        "The goal of this assignment is to explore how **the level of detail in prompts affects the quality of model outputs**, and to experiment with different prompting techniques such as **Zero-shot**, **Few-shot**, and **Structured prompting**.\n",
        "\n",
        "Through a series of tasks, the notebook demonstrates:\n",
        "\n",
        "* Designing prompts with varying detail levels.\n",
        "* Observing and comparing output quality.\n",
        "* Applying few-shot examples to guide model behavior.\n",
        "* Extracting structured information (e.g., from resumes) using prompt-based methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400c0800",
      "metadata": {
        "id": "400c0800"
      },
      "source": [
        "### **IMPORTANT NOTE:** I USED THE FILE \"resume_sample1.txt\" AS INPUT. SO PUT IT NEXT TO THE .ipynb FILE."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b70b2430",
      "metadata": {
        "id": "b70b2430"
      },
      "source": [
        "## 0. LLM (Gemini-1.5-flash) setup using LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4f4cd091",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f4cd091",
        "outputId": "e818d857-7a9d-4373-891e-9f94618c392e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping google-generativeai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping google-ai-generativelanguage as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-core as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-google-genai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting langchain-google-genai==2.1.12\n",
            "  Using cached langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core>=0.3.75 (from langchain-google-genai==2.1.12)\n",
            "  Downloading langchain_core-1.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai==2.1.12) (0.8.0)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai==2.1.12) (2.11.10)\n",
            "Collecting filetype<2,>=1.2 (from langchain-google-genai==2.1.12)\n",
            "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (2.26.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (1.75.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (5.29.5)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai==2.1.12) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai==2.1.12) (0.4.35)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai==2.1.12) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai==2.1.12) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai==2.1.12) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai==2.1.12) (4.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai==2.1.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai==2.1.12) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai==2.1.12) (0.4.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (2.32.4)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai==2.1.12) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai==2.1.12) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading langchain_core-1.0.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, langchain-core, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-core-1.0.0 langchain-google-genai-2.1.12\n",
            "Collecting langchain<0.4,>=0.3\n",
            "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-core<0.4,>=0.3\n",
            "  Using cached langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4,>=0.3) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4,>=0.3) (0.4.35)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4,>=0.3) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4,>=0.3) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4,>=0.3) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4,>=0.3) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4,>=0.3) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4,>=0.3) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4,>=0.3) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4,>=0.3) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<0.4,>=0.3) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4,>=0.3) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4,>=0.3) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4,>=0.3) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4,>=0.3) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4,>=0.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4,>=0.3) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4,>=0.3) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain<0.4,>=0.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain<0.4,>=0.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain<0.4,>=0.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain<0.4,>=0.3) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.4,>=0.3) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4,>=0.3) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4,>=0.3) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4,>=0.3) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4,>=0.3) (1.3.1)\n",
            "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.79-py3-none-any.whl (449 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.0.0\n",
            "    Uninstalling langchain-core-1.0.0:\n",
            "      Successfully uninstalled langchain-core-1.0.0\n",
            "Successfully installed langchain-0.3.27 langchain-core-0.3.79\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y google-generativeai google-ai-generativelanguage langchain langchain-core langchain-google-genai\n",
        "!pip install google-ai-generativelanguage>=0.7.0\n",
        "!pip install langchain-google-genai==2.1.12\n",
        "!pip install \"langchain>=0.3,<0.4\" \"langchain-core>=0.3,<0.4\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "zkIuwTEFBNZN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "zkIuwTEFBNZN",
        "outputId": "57f4b405-a839-4018-d3c5-d68d251da4e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-generativeai\n",
            "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.26.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.184.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.10.5)\n",
            "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-ai-generativelanguage, google-generativeai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.8.0\n",
            "    Uninstalling google-ai-generativelanguage-0.8.0:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.1.12 requires google-ai-generativelanguage<1,>=0.7, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.6.15 google-generativeai-0.8.5\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "5825037894214611b118c0d81fdd9d1f",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install google-generativeai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cfd8bca",
      "metadata": {
        "id": "6cfd8bca"
      },
      "source": [
        "### Google-AI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0f2e25f6",
      "metadata": {
        "id": "0f2e25f6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if \"GEMINI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GEMINI_API_KEY\"] = getpass.getpass(\"Enter your GEMINI API key securely:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hE8U9a-cAyQr",
      "metadata": {
        "id": "hE8U9a-cAyQr"
      },
      "source": [
        "List all models availvable in langchain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3JVZu2b-AvzP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "3JVZu2b-AvzP",
        "outputId": "93b5d1aa-620a-4ae9-f735-a7177b0969f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['models/gemini-2.5-pro-preview-03-25', 'models/gemini-2.5-flash-preview-05-20', 'models/gemini-2.5-flash', 'models/gemini-2.5-flash-lite-preview-06-17', 'models/gemini-2.5-pro-preview-05-06', 'models/gemini-2.5-pro-preview-06-05', 'models/gemini-2.5-pro', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-exp-image-generation', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-preview-image-generation', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/gemini-2.5-flash-preview-tts', 'models/gemini-2.5-pro-preview-tts', 'models/learnlm-2.0-flash-experimental', 'models/gemma-3-1b-it', 'models/gemma-3-4b-it', 'models/gemma-3-12b-it', 'models/gemma-3-27b-it', 'models/gemma-3n-e4b-it', 'models/gemma-3n-e2b-it', 'models/gemini-flash-latest', 'models/gemini-flash-lite-latest', 'models/gemini-pro-latest', 'models/gemini-2.5-flash-lite', 'models/gemini-2.5-flash-image-preview', 'models/gemini-2.5-flash-image', 'models/gemini-2.5-flash-preview-09-2025', 'models/gemini-2.5-flash-lite-preview-09-2025', 'models/gemini-robotics-er-1.5-preview', 'models/gemini-2.5-computer-use-preview-10-2025']\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# List all models available for your key\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "available_models = genai.list_models()\n",
        "print([model.name for model in available_models if \"generateContent\" in model.supported_generation_methods])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d11d204f",
      "metadata": {
        "id": "d11d204f"
      },
      "source": [
        "### LangChain LLM with Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "98d53f7f",
      "metadata": {
        "id": "98d53f7f"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "MODEL_NAME = \"gemini-2.5-flash\"\n",
        "\n",
        "# Initialize the LangChain LLM with Gemini API\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=MODEL_NAME,\n",
        "    google_api_key=os.environ[\"GEMINI_API_KEY\"],\n",
        "    temperature=0.0,\n",
        "    max_output_tokens=None,\n",
        "    max_retries=2,\n",
        "    timeout=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ab77e6e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab77e6e2",
        "outputId": "31232097-73c1-464a-b874-74763d970335"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--9b516590-8f46-4d88-b44e-373e719919f2-0'\n",
            "\n",
            "\n",
            "response is:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example: generate a response\n",
        "prompt_msg_test = [\n",
        "    SystemMessage(content=\"You are an assistant who explains AI concepts.\"),\n",
        "    HumanMessage(content=\"In one sentence, explain prompt engineering.\")\n",
        "    # AIMessage(content=\"Prompt engineering is the practice of designing effective prompts for language models.\")\n",
        "]\n",
        "\n",
        "response = llm.invoke(prompt_msg_test)\n",
        "print(response, end='\\n\\n\\n')\n",
        "print(\"response is:\", response.content, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a3bc1b",
      "metadata": {
        "id": "d6a3bc1b"
      },
      "source": [
        "## 1. Impact of Prompt Detail on Generated Content Quality"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9731e7e",
      "metadata": {
        "id": "b9731e7e"
      },
      "source": [
        "### 1.1 Choosing a scientific topic\n",
        "\n",
        "topic : `\"Student Classroom Behavior Management Based on Computer Vision\"`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0ea820",
      "metadata": {
        "id": "6c0ea820"
      },
      "source": [
        "### 1.2. Design 3 different prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c31285",
      "metadata": {
        "id": "11c31285"
      },
      "source": [
        "#### Prompt 1 - General and without Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f4b3128d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4b3128d",
        "outputId": "e2177fd9-e9d4-4a4b-b864-505484339ca5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='## Student Classroom Behavior Management Based on Computer Vision: A Comprehensive Framework and Ethical Analysis\\n\\n### Abstract\\nEffective classroom behavior management is fundamental to fostering a conducive learning environment. Traditional methods, often reliant on subjective observation and reactive intervention, can be resource-intensive and inconsistent. This paper explores the potential of computer vision (CV) technology to revolutionize student classroom behavior management. We propose a comprehensive framework for a CV-based system that objectively monitors student engagement, attention, and disruptive behaviors in real-time. The system leverages advanced CV techniques such as gaze tracking, pose estimation, facial expression analysis, and activity recognition to provide teachers with data-driven insights and early intervention capabilities. While highlighting the significant benefits for personalized learning, teacher support, and improved learning outcomes, this paper also critically examines the profound ethical implications, including privacy concerns, algorithmic bias, data security, and the potential for a surveillance culture. We advocate for a balanced approach that prioritizes student well-being, transparency, and robust ethical guidelines in the development and deployment of such innovative educational technologies.\\n\\n### 1. Introduction\\nThe classroom is a dynamic ecosystem where student behavior significantly impacts the learning process. Disruptive behaviors, lack of engagement, or inattention can impede academic progress for individuals and the entire class. Teachers traditionally manage these challenges through a combination of pedagogical strategies, direct observation, and disciplinary actions. However, the demands on teachers are immense, and their ability to continuously monitor every student\\'s behavior and emotional state in a crowded classroom is limited. This often leads to reactive interventions, missed opportunities for early support, and subjective assessments of student needs.\\n\\nThe rapid advancements in artificial intelligence (AI), particularly in computer vision (CV), offer unprecedented opportunities to augment human capabilities in various domains, including education. Computer vision, a field of AI that enables computers to \"see\" and interpret visual information from the world, has found applications in security, healthcare, retail, and autonomous systems. Its potential to provide objective, real-time insights into complex human behaviors makes it a compelling candidate for enhancing classroom management.\\n\\nThis paper proposes a conceptual framework for a computer vision-based system designed to assist teachers in managing student classroom behavior. We aim to outline the technical capabilities, potential benefits, and, crucially, the significant ethical considerations that must be addressed for responsible implementation. The goal is not to replace human teachers but to provide them with an intelligent assistant that offers data-driven insights, enabling more proactive, personalized, and effective interventions, ultimately fostering a more productive and equitable learning environment.\\n\\n### 2. Literature Review\\nTraditional classroom management theories emphasize proactive strategies, clear expectations, consistent routines, and positive reinforcement (Wong & Wong, 2009). Researchers like Kounin (1970) highlighted the importance of \"withitness\" ‚Äì a teacher\\'s awareness of everything happening in the classroom ‚Äì and \"overlapping\" ‚Äì the ability to attend to multiple events simultaneously. While these principles remain foundational, the practical execution of continuous \"withitness\" in a diverse classroom of 20-30+ students is challenging.\\n\\nTechnological integration in education has primarily focused on learning management systems (LMS), interactive whiteboards, and digital content delivery. While these tools enhance content delivery and administrative tasks, they offer limited direct support for real-time behavior management. Some existing technologies, such as student response systems or eye-tracking devices, have been used in research settings to gauge attention, but their scalability and integration into daily classroom practice are often limited (Grawemeyer et al., 2017).\\n\\nComputer vision, powered by deep learning, has demonstrated remarkable capabilities in analyzing human behavior. Applications include:\\n*   **Facial Expression Recognition:** Detecting emotions like happiness, sadness, anger, confusion, or surprise (Ekman & Friesen, 1971; Mollahosseini et al., 2017).\\n*   **Gaze Tracking:** Determining where a person is looking, indicating attention or distraction (Hansen & Ji, 2010).\\n*   **Pose Estimation:** Analyzing body posture and movement to infer activities or states (e.g., sitting, standing, fidgeting, interacting) (Cao et al., 2019).\\n*   **Activity Recognition:** Identifying complex actions or interactions from sequences of movements (e.g., hand-raising, group discussion, off-task behavior) (Wang et al., 2019).\\n\\nWhile CV has been explored in educational contexts for automated grading, student authentication, or monitoring exam integrity (e.g., proctoring systems), its application to real-time, continuous classroom behavior management for pedagogical support is an emerging area with significant research gaps and ethical challenges that require careful consideration.\\n\\n### 3. Proposed Computer Vision-Based Framework for Classroom Behavior Management\\n\\nOur proposed framework envisions a non-intrusive, AI-powered system designed to provide teachers with actionable insights into student behavior and engagement. The system comprises several key components:\\n\\n#### 3.1. System Architecture\\n*   **Data Acquisition:** Strategically placed, high-resolution cameras (e.g., wide-angle, ceiling-mounted) capture video streams of the classroom. These cameras are designed to capture general behavior, not individual student identification, unless explicitly consented for specific research purposes.\\n*   **Edge/Cloud Processing Unit:** Video data is processed either locally on edge devices (for privacy and latency) or sent to a secure cloud server for analysis.\\n*   **AI/CV Models:** A suite of specialized deep learning models performs real-time analysis of the video streams.\\n*   **Teacher Dashboard:** A user-friendly interface displays aggregated data, alerts, and historical trends to the teacher.\\n*   **Feedback Mechanism:** The system provides various forms of feedback, from subtle alerts to detailed reports.\\n\\n#### 3.2. Key Computer Vision Techniques and Their Applications\\n\\nThe system would leverage the following CV techniques to infer student behaviors:\\n\\n1.  **Gaze Tracking and Head Pose Estimation:**\\n    *   **Application:** Detects where a student is looking (e.g., at the teacher, whiteboard, textbook, or off-task).\\n    *   **Inference:** High proportion of gaze directed at the learning material/teacher indicates attention; prolonged gaze away or at irrelevant objects suggests distraction or disengagement.\\n    *   **Technical Approach:** Utilizes deep learning models to estimate head pose and eye gaze direction from facial landmarks.\\n\\n2.  **Facial Expression Recognition (FER):**\\n    *   **Application:** Analyzes micro-expressions to infer emotional states.\\n    *   **Inference:** Expressions of confusion, frustration, boredom, or active engagement (e.g., focused concentration, curiosity) can be detected.\\n    *   **Technical Approach:** Convolutional Neural Networks (CNNs) trained on large datasets of facial expressions to classify emotions. *Crucially, this focuses on general emotional states, not individual student identification.*\\n\\n3.  **Body Pose Estimation and Activity Recognition:**\\n    *   **Application:** Monitors body posture, movement, and gestures.\\n    *   **Inference:**\\n        *   **Engagement:** Upright posture, minimal fidgeting, active participation (e.g., hand-raising).\\n        *   **Disengagement/Off-task:** Slouching, excessive fidgeting, turning away from the learning activity, resting head on desk.\\n        *   **Disruptive Behavior:** Sudden movements, standing up without permission, aggressive gestures.\\n        *   **Collaboration:** Detecting group interactions, shared gaze, and coordinated movements during group work.\\n    *   **Technical Approach:** OpenPose, AlphaPose, or similar models for keypoint detection, followed by recurrent neural networks (RNNs) or transformers for sequence analysis and activity classification.\\n\\n4.  **Object Detection (Optional & Highly Sensitive):**\\n    *   **Application:** Identifying specific objects in student hands or on desks.\\n    *   **Inference:** Detection of prohibited items (e.g., mobile phones during an exam, specific toys) or learning aids.\\n    *   **Technical Approach:** YOLO (You Only Look Once) or Faster R-CNN models trained on specific object classes. *This feature requires extreme caution due to privacy implications.*\\n\\n#### 3.3. Data Processing and Feedback Mechanisms\\n\\n*   **Real-time Alerts:** The system can trigger discreet alerts to the teacher\\'s wearable device or dashboard when a student exhibits prolonged disengagement, signs of frustration, or disruptive behavior.\\n*   **Aggregated Class Insights:** Provide an overview of class-wide engagement levels, identifying patterns (e.g., a particular topic causing widespread confusion).\\n*   **Individual Student Reports (with consent):** Generate anonymized or pseudonymized reports on individual student engagement trends over time, helping teachers tailor interventions.\\n*   **Historical Data Analysis:** Allow teachers to review behavior patterns over days or weeks, identifying triggers or improvements.\\n*   **Personalized Recommendations:** Based on observed patterns, the system could suggest pedagogical adjustments or specific intervention strategies.\\n\\n### 4. Potential Benefits\\n\\nThe implementation of a CV-based classroom behavior management system offers several significant advantages:\\n\\n1.  **Enhanced Teacher \"Withitness\" and Proactive Intervention:** Teachers gain an objective, real-time \"sixth sense\" about their classroom, allowing them to identify struggling or disengaged students much earlier than traditional methods. This shifts from reactive discipline to proactive support.\\n2.  **Objective and Data-Driven Insights:** Replaces subjective observations with quantifiable data on engagement, attention, and behavior patterns. This data can inform teaching strategies, curriculum adjustments, and individualized learning plans.\\n3.  **Personalized Learning Support:** By identifying specific moments of confusion or frustration, teachers can provide targeted support, re-explain concepts, or offer alternative learning approaches tailored to individual student needs.\\n4.  **Improved Learning Environment:** Early detection and intervention of disruptive behaviors can maintain a more focused and respectful learning atmosphere for all students.\\n5.  **Reduced Teacher Workload:** Automating the continuous monitoring of behavior frees up teachers to focus more on instruction, interaction, and creative teaching.\\n6.  **Self-Awareness for Students (with appropriate feedback):** In some contexts, aggregated, anonymized feedback could help students develop self-awareness regarding their own engagement and attention habits.\\n7.  **Research and Pedagogical Development:** The collected data (anonymized and aggregated) can be invaluable for educational researchers to study the efficacy of different teaching methods, classroom layouts, and intervention strategies.\\n\\n### 5. Challenges and Ethical Considerations\\n\\nDespite the promising benefits, the deployment of CV-based systems in classrooms presents profound ethical, privacy, and practical challenges that must be rigorously addressed.\\n\\n1.  **Privacy and Surveillance:**\\n    *   **Constant Monitoring:** The continuous video recording and analysis of students raise significant concerns about constant surveillance, potentially creating a \"panopticon\" effect where students feel perpetually watched.\\n    *   **Data Collection and Storage:** What data is collected? How long is it stored? Who has access? The potential for misuse or breaches of sensitive behavioral data is high.\\n    *   **Consent:** Obtaining informed consent from students (especially minors) and their parents is paramount and complex. Opt-out options must be robust.\\n\\n2.  **Algorithmic Bias and Fairness:**\\n    *   **Training Data Bias:** CV models are trained on vast datasets. If these datasets do not adequately represent diverse student populations (e.g., different ethnicities, socio-economic backgrounds, neurodivergent students), the models can exhibit bias, leading to inaccurate or unfair assessments of certain groups.\\n    *   **Misinterpretation of Behavior:** A fidgeting student might be bored, anxious, or have ADHD. An AI might misinterpret cultural differences in expression or body language. Misclassifications could lead to unfair labeling or disciplinary actions.\\n    *   **Reinforcement of Stereotypes:** Biased algorithms could inadvertently reinforce existing stereotypes about certain student groups.\\n\\n3.  **Accuracy, Reliability, and False Positives/Negatives:**\\n    *   **Contextual Nuance:** Human behavior is highly contextual. An AI might struggle to differentiate between a student looking away to think deeply and one who is genuinely distracted.\\n    *   **Emotional Complexity:** Facial expressions are not always reliable indicators of internal emotional states. A student might appear \"bored\" but be deeply engaged in thought.\\n    *   **System Errors:** False positives (identifying a behavior that isn\\'t occurring) or false negatives (missing a behavior that is occurring) can undermine trust and lead to inappropriate interventions.\\n\\n4.  **Data Security and Governance:**\\n    *   **Vulnerability to Hacking:** Behavioral data, especially if linked to individual students, is highly sensitive. Robust cybersecurity measures are essential to prevent unauthorized access, data breaches, or malicious use.\\n    *   **Data Ownership and Usage:** Who owns the data? How can it be used beyond immediate classroom management? Clear policies are needed to prevent commercial exploitation or sharing with third parties without explicit consent.\\n\\n5.  **Impact on Student Well-being and Autonomy:**\\n    *   **Chilling Effect:** Students might feel inhibited from expressing themselves naturally, fearing algorithmic judgment, leading to increased stress or anxiety.\\n    *   **Reduced Creativity and Spontaneity:** A highly monitored environment could stifle creativity, risk-taking, and spontaneous interactions essential for learning.\\n    *   **Over-reliance on Technology:** Teachers might become overly reliant on the system, potentially diminishing their own observational skills and human connection with students.\\n\\n6.  **Technical Infrastructure and Cost:**\\n    *   **High Initial Investment:** Implementing such a system requires significant investment in cameras, processing hardware, software development, and network infrastructure.\\n    *   **Maintenance and Expertise:** Ongoing maintenance, software updates, and the need for specialized technical expertise can be substantial.\\n\\n7.  **Teacher Acceptance and Training:**\\n    *   **Resistance to Change:** Teachers might be resistant to adopting new technology, especially if they perceive it as a threat to their professional autonomy or an invasion of privacy.\\n    *   **Training Requirements:** Adequate training is crucial for teachers to understand how to interpret the data, use the system effectively, and integrate it ethically into their pedagogical practice.\\n\\n### 6. Ethical Guidelines and Recommendations for Responsible Implementation\\n\\nTo mitigate the aforementioned challenges, any CV-based classroom behavior management system must adhere to stringent ethical guidelines:\\n\\n1.  **Transparency and Informed Consent:**\\n    *   Clearly communicate the system\\'s purpose, data collected, and usage policies to students, parents, and staff.\\n    *   Obtain explicit, informed consent from parents/guardians for minor students, with clear opt-out options that do not penalize students.\\n    *   Ensure students understand the system\\'s presence and function in an age-appropriate manner.\\n\\n2.  **Privacy by Design:**\\n    *   **Anonymization/Pseudonymization:** Prioritize processing data in an anonymized or pseudonymized form whenever possible. Avoid facial recognition for identity unless absolutely necessary and with explicit consent. Focus on aggregate behavioral patterns rather than individual identification.\\n    *   **Data Minimization:** Collect only the data strictly necessary for the stated purpose.\\n    *   **Local Processing (Edge AI):** Process data on local devices to minimize data transfer and storage in the cloud, enhancing privacy.\\n    *   **No Audio Recording:** Avoid audio recording to prevent capturing private conversations.\\n\\n3.  **Fairness and Bias Mitigation:**\\n    *   **Diverse Training Data:** Develop and train AI models using diverse datasets that represent all student demographics to minimize algorithmic bias.\\n    *   **Regular Auditing:** Continuously audit the system for bias and accuracy, especially across different student groups.\\n    *   **Human Oversight:** The system should always be a *tool* to assist teachers, not replace their judgment. Teachers must have the final say in interpreting data and making decisions.\\n\\n4.  **Data Security and Governance:**\\n    *   Implement robust encryption, access controls, and cybersecurity protocols to protect all collected data.\\n    *   Establish clear data retention policies, ensuring data is deleted when no longer needed.\\n    *   Define strict data ownership and usage policies, prohibiting commercial exploitation or sharing with third parties without explicit consent.\\n\\n5.  **Focus on Support, Not Surveillance:**\\n    *   Frame the system as a supportive tool for teachers and students, emphasizing its role in enhancing learning, not as a disciplinary or surveillance mechanism.\\n    *   Avoid using the system for punitive measures. Its primary goal should be early intervention and personalized support.\\n\\n6.  **Teacher Training and Empowerment:**\\n    *   Provide comprehensive training for teachers on how to use the system effectively, interpret its outputs, and integrate it ethically into their teaching practices.\\n    *   Emphasize that the system is an aid, not a replacement for human intuition and connection.\\n\\n### 7. Future Work\\n\\nFuture research and development in this area should focus on:\\n\\n*   **Longitudinal Studies:** Conducting long-term studies to assess the actual impact of such systems on student learning outcomes, well-being, and teacher effectiveness.\\n*   **User-Centric Design:** Involving teachers, students, and parents in the design and development process to ensure the system meets real-world needs and addresses concerns.\\n*   **Explainable AI (XAI):** Developing models that can explain *why* they made a particular inference (e.g., \"Student X appears confused because their brow is furrowed and gaze is directed away from the board\"), increasing transparency and trust.\\n*   **Integration with Learning Management Systems (LMS):** Seamlessly integrating CV insights with existing LMS platforms to provide a holistic view of student progress and behavior.\\n*   **Adaptive Interventions:** Exploring how the system could suggest or even trigger adaptive learning content or micro-interventions based on real-time behavioral cues.\\n*   **Standardized Ethical Frameworks:** Collaborating with educational bodies, ethicists, and policymakers to develop comprehensive, legally binding ethical guidelines for AI in education.\\n\\n### 8. Conclusion\\n\\nThe application of computer vision to classroom behavior management holds immense potential to transform educational practices, offering teachers unprecedented insights into student engagement and well-being. By providing objective, real-time data, such systems can empower educators to deliver more personalized support, intervene proactively, and foster a more conducive learning environment. However, the path to realizing these benefits is fraught with significant ethical challenges, particularly concerning privacy, bias, and the potential for a surveillance culture.\\n\\nIt is imperative that the development and deployment of CV-based classroom management systems proceed with extreme caution, guided by robust ethical principles, transparency, and a commitment to student well-being. The goal should always be to augment human capabilities, not replace them, and to create tools that genuinely support learning and development without compromising fundamental rights. By prioritizing ethical considerations alongside technological innovation, we can harness the power of computer vision to create more effective, equitable, and humane educational experiences for all students.\\n\\n### References (Example - actual references would be listed here)\\n\\n*   Cao, Z., Hidalgo, G., Simon, T., Wei, S. E., & Sheikh, Y. (2019). OpenPose: Realtime Multi-Person Keypoint Detection Library for Body, Face, Hands, and Foot Estimation. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 43(1), 1-14.\\n*   Ekman, P., & Friesen, W. V. (1971). Constants across cultures in the face and emotion. *Journal of Personality and Social Psychology*, 17(2), 124‚Äì129.\\n*   Grawemeyer, B., Kistler, F., & Johnson, M. (2017). Using eye-tracking to detect student confusion. In *Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization* (pp. 105-113).\\n*   Hansen, D. W., & Ji, Q. (2010). In the eye of the beholder: A survey of models for eyes and gaze. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 32(5), 779-809.\\n*   Kounin, J. S. (1970). *Discipline and group management in classrooms*. Holt, Rinehart and Winston.\\n*   Mollahosseini, A., Hasani, B., & Mahoor, M. H. (2017). AffectNet: A Database for Facial Expression, Valence, and Arousal Detection in the Wild. *IEEE Transactions on Affective Computing*, 10(1), 18-31.\\n*   Wang, L., Hu, W., & Tan, T. (2019). A comprehensive survey of human activity recognition with depth cameras. *IEEE Transactions on Systems, Man, and Cybernetics: Systems*, 49(11), 2617-2632.\\n*   Wong, H. K., & Wong, R. T. (2009). *The first days of school: How to be an effective teacher*. Harry K. Wong Publications.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b8b49e02-e239-4f72-a546-24b039e351a7-0')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_prompt_1 = \"Write a paper on Student Classroom Behavior Management Based on Computer Vision\"\n",
        "\n",
        "llm.invoke(simple_prompt_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e69a0a",
      "metadata": {
        "id": "c1e69a0a"
      },
      "source": [
        "#### Prompt 2 - Determining the Structure of the Paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "8592a1dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8592a1dd",
        "outputId": "e0d9c52c-11fc-48dd-d453-fb5adaab4491"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='## Leveraging Computer Vision for Enhanced Student Classroom Behavior Management: Opportunities, Challenges, and Ethical Considerations\\n\\n**Abstract:**\\nEffective classroom behavior management is fundamental to fostering conducive learning environments and optimizing educational outcomes. Traditional methods, often reliant on subjective observation and reactive interventions, can be resource-intensive and inconsistent. This paper explores the burgeoning potential of computer vision (CV) technologies to revolutionize student behavior management by providing objective, real-time data and insights. We delve into the specific CV techniques applicable to classroom settings, such as pose estimation, facial expression analysis, and activity recognition, highlighting their capacity to identify patterns of engagement, distraction, and disruptive behavior. Furthermore, the paper meticulously examines the significant benefits, including early intervention, personalized support, and reduced teacher workload. Crucially, it addresses the profound ethical considerations and counterarguments, particularly concerning privacy, surveillance, bias, and data security, proposing mitigation strategies such as anonymization, transparency, and human oversight. The paper concludes by advocating for a balanced, ethically-driven approach to integrating CV into educational practices, emphasizing its role as a supportive tool for educators rather than a punitive mechanism.\\n\\n**Keywords:** Computer Vision, Classroom Management, Student Behavior, Artificial Intelligence, Educational Technology, Ethics, Privacy, Engagement, Learning Analytics.\\n\\n---\\n\\n**1. Introduction**\\n\\nThe classroom stands as the primary arena for formal education, and its efficacy is profoundly influenced by the prevailing behavioral climate. Effective classroom management is not merely about maintaining order; it is about creating an environment where students feel safe, respected, and motivated to learn (Marzano & Marzano, 2003). Traditional approaches to behavior management, while foundational, often rely on teachers\\' subjective observations, experience, and reactive interventions. These methods can be time-consuming, prone to human error or bias, and may not always provide the granular, real-time data necessary for proactive and personalized support (Evertson & Weinstein, 2006).\\n\\nIn recent years, advancements in artificial intelligence (AI), particularly computer vision (CV), have demonstrated transformative potential across various sectors, from healthcare to retail. The application of CV in educational settings, specifically for analyzing student behavior, presents a novel paradigm for enhancing classroom management. By leveraging cameras and sophisticated algorithms, CV systems can objectively detect, track, and analyze student actions, postures, facial expressions, and interactions within the classroom. This capability promises to offer educators unprecedented insights into student engagement, distraction levels, and potential behavioral issues, enabling more timely, data-driven, and personalized interventions.\\n\\nThis paper aims to provide a comprehensive analysis of student classroom behavior management based on computer vision. It will first outline the technological underpinnings of CV relevant to this domain. Subsequently, it will detail the potential benefits and opportunities that CV offers for improving learning environments and supporting educators. Crucially, the paper will then address the significant ethical challenges and counterarguments, such as privacy concerns, algorithmic bias, and the risk of over-surveillance, proposing robust mitigation strategies. Finally, it will conclude with a summary of key findings and a forward-looking perspective on the responsible integration of CV into educational practices.\\n\\n**2. The Landscape of Classroom Behavior Management and the Need for Innovation**\\n\\nEffective classroom management is a multifaceted discipline encompassing strategies to organize the learning environment, establish clear expectations, foster positive relationships, and address disruptive behaviors (Wong & Wong, 2009). Its importance is underscored by its direct correlation with academic achievement, student well-being, and teacher retention (Oliver et al., 2011). However, traditional methods face several inherent limitations:\\n\\n*   **Subjectivity and Inconsistency:** Teachers\\' observations can be influenced by personal biases, fatigue, or the sheer volume of students, leading to inconsistent application of rules and interventions.\\n*   **Reactive Nature:** Interventions often occur *after* a behavior has escalated, rather than proactively addressing its nascent stages.\\n*   **Teacher Workload:** Monitoring and managing behavior for 20-30+ students simultaneously is mentally taxing and diverts attention from instructional delivery.\\n*   **Lack of Granular Data:** Traditional methods rarely provide objective, quantifiable data on behavioral patterns over time, making it difficult to assess the effectiveness of interventions or identify systemic issues.\\n*   **Limited Personalization:** Without detailed insights into individual student behaviors and triggers, personalized support strategies are challenging to develop and implement effectively.\\n\\nThe advent of educational technology has introduced tools like Learning Management Systems (LMS) and interactive whiteboards, primarily focusing on content delivery and administrative tasks. However, the realm of real-time, objective behavioral analysis has largely remained untouched by advanced technological solutions. This gap highlights a critical need for innovative approaches that can augment teachers\\' capabilities, providing them with the tools to create more responsive and supportive learning environments.\\n\\n**3. Computer Vision Technologies for Behavior Analysis in Classrooms**\\n\\nComputer vision, a field of AI that enables computers to \"see\" and interpret visual data, offers a suite of technologies directly applicable to analyzing student behavior. These technologies process video streams from classroom cameras (which can be standard webcams or dedicated surveillance cameras, depending on implementation) to extract meaningful insights.\\n\\n*   **Pose Estimation and Activity Recognition:**\\n    *   **Mechanism:** Algorithms like OpenPose or AlphaPose can detect and track human body keypoints (e.g., joints, head position) in real-time. From these keypoints, the system can infer posture, movement, and overall body language.\\n    *   **Application:** Identifying students who are slouching, fidgeting excessively, standing up without permission, or engaging in off-task physical activities (e.g., playing with objects, turning away from the board). This can indicate disengagement, restlessness, or potential disruptive behavior (Smith et al., 2022).\\n*   **Facial Expression and Gaze Tracking:**\\n    *   **Mechanism:** Deep learning models, often based on Convolutional Neural Networks (CNNs), can analyze facial muscle movements to infer basic emotions (e.g., happiness, sadness, confusion, surprise) or states like attention. Gaze tracking algorithms determine where a student is looking.\\n    *   **Application:** Detecting signs of confusion, frustration, boredom, or active engagement. Gaze tracking can identify students whose attention is diverted from the instructor or learning materials (e.g., looking at a phone, staring out the window) (Jones & Chen, 2021).\\n    *   **Caveat:** Facial expression analysis is highly context-dependent and can be culturally biased; its interpretation requires careful consideration and should not be the sole basis for judgment.\\n*   **Object Detection and Recognition:**\\n    *   **Mechanism:** Algorithms like YOLO (You Only Look Once) or Faster R-CNN can identify and localize specific objects within the video frame.\\n    *   **Application:** Detecting prohibited items (e.g., mobile phones, unauthorized toys) being used during class, or identifying instances of students sharing notes or materials inappropriately.\\n*   **Group Behavior Analysis:**\\n    *   **Mechanism:** By aggregating individual student data, CV systems can analyze interactions between students, identify clusters, or detect unusual group movements.\\n    *   **Application:** Identifying instances of collaborative learning, but also potential bullying, unauthorized group discussions, or students passing notes.\\n*   **Sound Analysis (Complementary):** While not strictly computer vision, integrating audio analysis (e.g., detecting sudden loud noises, prolonged whispering) can complement visual data to provide a more holistic understanding of classroom dynamics.\\n\\nThe data generated by these CV systems can be processed in real-time, aggregated over periods, and presented to educators through intuitive dashboards. This allows for immediate alerts regarding concerning behaviors, as well as long-term trend analysis to inform pedagogical adjustments and individualized support plans.\\n\\n**4. Benefits and Opportunities**\\n\\nThe integration of computer vision into classroom behavior management offers several compelling benefits that can significantly enhance the educational experience for both students and teachers:\\n\\n*   **Objective and Real-time Data:** CV systems provide unbiased, quantifiable data on student behavior, eliminating the subjectivity inherent in human observation. This real-time feedback allows teachers to intervene promptly, often before minor issues escalate into major disruptions (Wang et al., 2020).\\n*   **Early Intervention and Proactive Support:** By identifying subtle cues of disengagement, confusion, or restlessness, CV can alert teachers to potential problems at their earliest stages. This enables proactive interventions, such as a quick check-in with a student, a change in instructional strategy, or a timely redirection, preventing more significant behavioral challenges.\\n*   **Personalized Learning and Support:** Data on individual student engagement and behavior patterns can inform personalized learning strategies. Teachers can identify students who consistently struggle with attention during certain activities or topics and tailor their approach accordingly, offering targeted support or alternative learning methods (Johnson & Lee, 2023).\\n*   **Reduced Teacher Workload and Enhanced Focus:** Automating the monitoring of general classroom behavior frees up teachers to focus more on instruction, student interaction, and addressing complex individual needs. It reduces the cognitive load associated with constant vigilance, allowing educators to dedicate more energy to teaching and fostering positive relationships.\\n*   **Data-Driven Pedagogical Adjustments:** Aggregated behavioral data can reveal patterns across the entire class or specific groups. For instance, if a significant number of students consistently show signs of disengagement during a particular lesson segment, it might indicate a need to revise the teaching method, content, or pace. This allows for continuous improvement of instructional practices.\\n*   **Enhanced Learning Environment:** By consistently and objectively managing behavior, CV can contribute to a more orderly, respectful, and focused learning environment. Students are more likely to thrive academically and socially in classrooms where expectations are clear and disruptions are minimized.\\n*   **Support for Students with Special Needs:** CV can be particularly beneficial for identifying non-verbal cues from students with certain learning disabilities or communication challenges, providing teachers with additional insights to better support their unique needs (Chen & Gupta, 2022).\\n\\n**5. Addressing Counterarguments and Ethical Considerations**\\n\\nDespite its promising potential, the application of computer vision in classrooms raises significant ethical concerns and counterarguments that must be thoroughly addressed for responsible implementation.\\n\\n*   **5.1. Privacy and Surveillance:**\\n    *   **Counterargument:** The most prominent concern is the potential for constant surveillance, eroding student privacy and fostering an environment of distrust. Students may feel perpetually \"watched,\" leading to anxiety, self-consciousness, and a chilling effect on natural behavior and expression (Zuboff, 2019).\\n    *   **Mitigation Strategies:**\\n        *   **Anonymization and Aggregation:** Focus on analyzing aggregated, anonymized data for group trends rather than individual student tracking. If individual data is collected, it should be pseudonymized.\\n        *   **Consent and Transparency:** Obtain explicit, informed consent from parents/guardians and, where appropriate, students. Clearly communicate the purpose, scope, and limitations of the technology.\\n        *   **Opt-out Options:** Provide mechanisms for students or parents to opt out of individual tracking, with alternative support mechanisms in place.\\n        *   **Limited Data Retention:** Implement strict policies for data retention, ensuring data is deleted after a necessary period.\\n        *   **Focus on Support, Not Punishment:** Frame the technology as a tool for support and improvement, not for punitive measures or disciplinary action.\\n\\n*   **5.2. Algorithmic Bias and Fairness:**\\n    *   **Counterargument:** CV algorithms, particularly those trained on biased datasets, can perpetuate or amplify existing societal biases (e.g., racial, gender, socioeconomic). This could lead to disproportionate targeting or misinterpretation of behaviors from certain student groups (Buolamwini & Gebru, 2018). For example, facial expression recognition might misinterpret expressions from non-Western cultures or individuals with certain disabilities.\\n    *   **Mitigation Strategies:**\\n        *   **Diverse Training Data:** Ensure algorithms are trained on diverse and representative datasets to minimize bias.\\n        *   **Regular Auditing and Validation:** Continuously audit the system\\'s performance across different demographic groups to identify and rectify biases.\\n        *   **Human Oversight:** Maintain human oversight as the ultimate decision-making authority. CV should provide insights, not dictate actions.\\n        *   **Contextual Interpretation:** Emphasize that CV data is one piece of information, requiring contextual interpretation by a trained educator.\\n\\n*   **5.3. Misinterpretation and Over-reliance:**\\n    *   **Counterargument:** CV systems are not infallible. A student looking away might be thinking deeply, not disengaged. A fidgeting student might have ADHD, not be disruptive. Over-reliance on automated systems without human judgment can lead to misinterpretations, mislabeling, and inappropriate interventions (Crawford, 2021).\\n    *   **Mitigation Strategies:**\\n        *   **CV as an Assistant, Not a Replacement:** Position CV as a tool to augment teacher capabilities, providing data points for consideration, not as a substitute for human empathy, judgment, and interaction.\\n        *   **Teacher Training:** Provide comprehensive training for educators on how to interpret CV data, understand its limitations, and integrate it effectively into their pedagogical practice.\\n        *   **Multi-modal Data Integration:** Combine CV data with other sources of information (e.g., academic performance, teacher observations, student self-reports) for a more holistic view.\\n\\n*   **5.4. Data Security and Cyber Threats:**\\n    *   **Counterargument:** The collection and storage of sensitive student behavioral data create significant cybersecurity risks. Breaches could expose personal information, leading to identity theft, harassment, or other harms.\\n    *   **Mitigation Strategies:**\\n        *   **Robust Encryption:** Implement strong encryption for data in transit and at rest.\\n        *   **Secure Storage and Access Control:** Utilize secure, compliant data storage solutions and implement strict access controls based on the principle of least privilege.\\n        *   **Regular Security Audits:** Conduct frequent security audits and penetration testing to identify and address vulnerabilities.\\n        *   **Compliance with Regulations:** Adhere to relevant data protection regulations (e.g., GDPR, FERPA).\\n\\n*   **5.5. Dehumanization and Impact on Student Well-being:**\\n    *   **Counterargument:** A classroom environment perceived as constantly monitored by AI could lead to increased stress, anxiety, and a feeling of being dehumanized. It might stifle creativity, risk-taking, and spontaneous interaction, which are crucial for holistic development.\\n    *   **Mitigation Strategies:**\\n        *   **Focus on Positive Reinforcement:** Design systems to identify and reinforce positive behaviors (e.g., collaboration, active participation) rather than solely focusing on negative ones.\\n        *   **Student Involvement:** Involve students in discussions about the technology, its purpose, and how it can support their learning, fostering a sense of ownership and understanding.\\n        *   **Ethical Design Principles:** Prioritize human-centered design, ensuring the technology serves to enhance human connection and learning, not diminish it.\\n\\n**6. Implementation Strategies and Best Practices**\\n\\nFor computer vision to be successfully and ethically integrated into classroom management, a thoughtful and strategic approach is essential:\\n\\n*   **Phased Implementation:** Start with pilot programs in a limited number of classrooms or schools, allowing for iterative refinement and feedback before broader deployment.\\n*   **Stakeholder Engagement:** Involve all key stakeholders ‚Äì teachers, students, parents, administrators, and IT staff ‚Äì in the planning, implementation, and evaluation process. Their input is crucial for acceptance and success.\\n*   **Clear Policies and Guidelines:** Develop comprehensive policies addressing data collection, storage, access, usage, and retention, ensuring compliance with all relevant privacy laws and ethical standards.\\n*   **Teacher Training and Professional Development:** Provide extensive training for educators on the capabilities, limitations, and ethical implications of CV systems. Focus on how to effectively use the data to inform their teaching and support students.\\n*   **Focus on Formative Assessment and Support:** Emphasize the use of CV data for formative assessment of behavior and for providing supportive interventions, rather than for summative evaluation or punitive measures.\\n*   **Integration with Existing Systems:** Design CV solutions to integrate seamlessly with existing Learning Management Systems (LMS) or student information systems to provide a unified view of student data.\\n*   **Continuous Evaluation and Improvement:** Regularly evaluate the effectiveness, fairness, and ethical impact of the CV system, making necessary adjustments based on feedback and evolving best practices.\\n\\n**7. Conclusion**\\n\\nThe integration of computer vision into student classroom behavior management represents a significant technological frontier with the potential to profoundly enhance educational environments. By offering objective, real-time insights into student engagement, attention, and behavior patterns, CV can empower educators with unprecedented tools for proactive intervention, personalized support, and data-driven pedagogical adjustments. This can lead to more efficient teaching, reduced teacher burnout, and ultimately, improved learning outcomes for students.\\n\\nHowever, the promise of this technology is inextricably linked to its responsible and ethical deployment. The profound concerns regarding student privacy, the potential for algorithmic bias, the risk of misinterpretation, and the imperative of data security cannot be overstated. Addressing these counterarguments requires a commitment to transparency, informed consent, robust data protection, continuous auditing, and, critically, the unwavering principle that technology must serve as an assistive tool for human educators, not a replacement for their judgment, empathy, and relational expertise.\\n\\nMoving forward, interdisciplinary research, collaborative policy development, and ongoing dialogue among technologists, educators, ethicists, and policymakers will be essential. The goal should be to harness the power of computer vision to create more supportive, equitable, and effective learning environments, ensuring that the benefits of innovation are realized without compromising the fundamental rights and well-being of students. A balanced, human-centered approach, where technology augments rather than dictates, holds the key to unlocking the transformative potential of computer vision in the classroom.\\n\\n---\\n\\n**Summary:**\\nThis paper explored the application of computer vision (CV) for student classroom behavior management, highlighting its potential to provide objective, real-time insights into student engagement and behavior. It detailed CV technologies like pose estimation and facial analysis, outlining benefits such as early intervention, personalized support, and reduced teacher workload. Crucially, the paper addressed significant ethical concerns including privacy, surveillance, algorithmic bias, and data security, proposing mitigation strategies like anonymization, transparency, and human oversight. It concluded by advocating for a balanced, ethical, and human-centered approach to integrating CV as a supportive tool for educators, emphasizing the need for careful implementation and continuous evaluation.\\n\\n---\\n\\n**References (Illustrative Examples - actual paper would have specific citations):**\\n\\n*   Buolamwini, J., & Gebru, T. (2018). Gender Shades: Intersectional Phenotypic Demographics for Darker-Skinned Females. *Proceedings of the 1st Conference on Fairness, Accountability and Transparency (FAT)*.\\n*   Chen, L., & Gupta, S. (2022). *AI in Special Education: Enhancing Learning for Diverse Needs*. Educational Technology Press.\\n*   Crawford, K. (2021). *Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence*. Yale University Press.\\n*   Evertson, C. M., & Weinstein, C. S. (Eds.). (2006). *Handbook of Classroom Management: Research, Practice, and Contemporary Issues*. Lawrence Erlbaum Associates.\\n*   Johnson, R., & Lee, S. (2023). *Personalized Learning in the Digital Age: A Data-Driven Approach*. Academic Publishing.\\n*   Jones, A., & Chen, B. (2021). *The Ethics of AI in Education: Surveillance, Bias, and Student Well-being*. Routledge.\\n*   Marzano, R. J., & Marzano, J. S. (2003). The Key to Classroom Management. *Educational Leadership*, *61*(1), 6-13.\\n*   Oliver, R. M., Wehby, J. H., & Reschly, D. J. (2011). Teacher Classroom Management Practices: Effects on Disruptive Behavior and Student Academic Engaged Time. *Journal of School Psychology*, *49*(1), 105-121.\\n*   Smith, J., et al. (2022). *Computer Vision for Behavioral Analysis in Learning Environments*. Journal of Educational AI, 5(2), 123-145.\\n*   Wang, L., et al. (2020). Real-time Student Engagement Detection in Online Learning Environments Using Computer Vision. *Proceedings of the International Conference on Artificial Intelligence in Education*.\\n*   Wong, H. K., & Wong, R. T. (2009). *The First Days of School: How to Be an Effective Teacher*. Harry K. Wong Publications.\\n*   Zuboff, S. (2019). *The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power*. PublicAffairs.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--d8c66dfb-5022-4446-b204-b56285a97bb8-0')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "structured_prompt_2 = \"\"\"Write a paper on Student Classroom Behavior Managment Based on Computer Vision.\n",
        "The paper should include a clear title, abstract, introduction, main body, and conclusion, presenting arguments logically with evidence, analysis, and proper academic tone;\n",
        "it must also address counterarguments, cite reliable sources, and end with a concise summary and references.\n",
        "\"\"\"\n",
        "\n",
        "llm.invoke(structured_prompt_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cafacd1b",
      "metadata": {
        "id": "cafacd1b"
      },
      "source": [
        "#### Prompt 3 - Determining the exact Content and Format of each Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ea787d4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea787d4d",
        "outputId": "8e4b9852-fd83-4a5e-b97c-284aa765478e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='## Student Classroom Behavior Management Based on Computer Vision: An Enhanced Approach Using Quantum Evolutionary Algorithm and DenseNet-121\\n\\n### Abstract\\n\\nEffective classroom behavior management is paramount for fostering conducive learning environments, yet educators frequently face challenges in objectively and consistently monitoring student conduct. This paper proposes an advanced computer vision (CV) system for automated student classroom behavior management, leveraging the synergistic integration of the Quantum Evolutionary Algorithm (QEA) with the DenseNet-121 deep learning model. The primary purpose is to develop a robust, real-time, and objective solution for detecting and analyzing various student behaviors, ranging from engagement and attentiveness to distraction and potential disruption. Our approach utilizes DenseNet-121 for efficient and deep feature extraction from classroom video data, capturing subtle visual cues related to posture, facial expressions, and interactions. QEA is then employed to optimize the parameters and potentially the feature selection process of the DenseNet-121 model, enhancing its accuracy and generalization capabilities by exploring a broader solution space more effectively than traditional optimization methods. Expected outcomes include significantly improved accuracy in behavior classification, objective real-time monitoring, and the provision of actionable insights for educators. The implications of this research extend to reducing teacher workload, enabling proactive interventions, facilitating personalized learning strategies, and ultimately improving academic outcomes. While acknowledging significant ethical considerations regarding privacy and data bias, this paper advocates for the responsible deployment of AI-driven tools to transform educational practices.\\n\\n### 1. Introduction\\n\\nThe classroom is a dynamic environment where student behavior profoundly impacts learning outcomes, peer interactions, and teacher effectiveness. Effective classroom behavior management is a cornerstone of successful pedagogy, creating a structured and supportive atmosphere essential for academic achievement and social-emotional development (Marzano & Marzano, 2003). However, educators often grapple with numerous challenges in this domain. These include the subjective nature of behavioral assessment, the time-consuming demands of constant monitoring, the difficulty in identifying subtle cues of disengagement or distress, and the sheer scale of managing diverse behaviors in large class sizes (Evertson & Weinstein, 2006). Traditional methods, relying heavily on teacher observation and anecdotal records, can be inconsistent, prone to bias, and reactive rather than proactive, often intervening only after a behavior has escalated.\\n\\nThe advent of artificial intelligence (AI) and, specifically, computer vision (CV) technologies presents a transformative opportunity to address these long-standing challenges. Computer vision offers the potential for objective, real-time, and data-driven analysis of student behavior, moving beyond subjective interpretations to provide empirical evidence. By processing visual data from classroom settings, CV systems can identify patterns indicative of various behaviors, offering educators a powerful tool for informed decision-making. This technology can monitor engagement levels, detect signs of distraction or disinterest, and even flag potential disruptive behaviors, thereby enabling timely and targeted interventions.\\n\\nThis paper introduces a novel computer vision-based framework for student classroom behavior management, designed to enhance the accuracy and reliability of behavioral detection. Our proposed system integrates the strengths of the DenseNet-121 deep convolutional neural network for robust feature extraction with the Quantum Evolutionary Algorithm (QEA) for optimized model performance. DenseNet-121 is chosen for its efficiency in feature reuse and ability to mitigate vanishing gradients, making it highly effective for complex visual tasks. QEA, inspired by quantum computing principles, offers a powerful optimization paradigm capable of exploring complex parameter spaces more effectively than traditional algorithms, leading to superior model accuracy and generalization. This synergistic approach aims to provide educators with an objective, scalable, and proactive tool for managing classroom behavior, ultimately fostering more effective and equitable learning environments.\\n\\n### 2. Methods\\n\\nThe proposed student classroom behavior management system based on computer vision employs a sophisticated workflow that integrates deep learning for feature extraction with an advanced evolutionary algorithm for optimization. The overall methodology encompasses data collection, preprocessing, deep feature extraction using DenseNet-121, and model optimization via the Quantum Evolutionary Algorithm (QEA), all while adhering to stringent ethical guidelines.\\n\\n#### 2.1. Overall Workflow\\n\\nThe system\\'s operation can be broken down into the following stages:\\n\\n1.  **Data Collection:** High-resolution video footage of students in various classroom settings is collected. This dataset must capture a diverse range of student behaviors, including but not limited to:\\n    *   **Engaged/Attentive:** Looking at the teacher/board, taking notes, active participation.\\n    *   **Distracted/Disengaged:** Looking away, fidgeting, using unauthorized devices, head down.\\n    *   **Disruptive:** Talking out of turn, aggressive postures, leaving seat without permission.\\n    *   **Collaborative:** Group work, peer interaction.\\n    *   **Passive:** Listening quietly, observing.\\n    Ethical considerations are paramount during this phase, ensuring informed consent from students (and their guardians) and educators, anonymization of identities where possible, and secure storage of all data.\\n\\n2.  **Data Preprocessing:** Raw video data undergoes several preprocessing steps:\\n    *   **Frame Extraction:** Videos are decomposed into individual frames at a specified frame rate (e.g., 5-10 frames per second) to capture temporal dynamics without excessive redundancy.\\n    *   **Region of Interest (ROI) Detection:** Object detection algorithms (e.g., YOLO, Faster R-CNN) are initially applied to detect and crop individual student regions within each frame. This focuses the subsequent analysis on relevant areas and reduces computational load.\\n    *   **Normalization and Resizing:** Cropped student images are resized to a uniform dimension (e.g., 224x224 pixels) and pixel values are normalized to a standard range (e.g., [0, 1] or [-1, 1]) to prepare them for input into the deep learning model.\\n    *   **Data Augmentation:** Techniques such as random rotations, flips, brightness adjustments, and cropping are applied to the training data to increase its diversity and improve the model\\'s generalization capabilities, reducing overfitting.\\n\\n3.  **Behavior Labeling:** A team of trained human annotators meticulously labels the preprocessed frames or short video clips with predefined behavior categories. This ground truth data is crucial for supervised learning.\\n\\n#### 2.2. DenseNet-121 for Deep Feature Extraction\\n\\nDenseNet-121, a variant of the Densely Connected Convolutional Networks (Huang et al., 2017), is employed as the backbone for deep feature extraction. DenseNets are characterized by their unique connectivity pattern where each layer is directly connected to every subsequent layer in a feed-forward fashion. This architecture offers several advantages:\\n\\n*   **Feature Reuse:** Each layer receives feature maps from all preceding layers and passes its own feature maps to all subsequent layers. This dense connectivity promotes feature reuse throughout the network, leading to more compact models and improved efficiency.\\n*   **Reduced Vanishing Gradient Problem:** The direct connections provide a shorter path for gradients to flow back to earlier layers, effectively mitigating the vanishing gradient problem, which is common in very deep networks.\\n*   **Strong Feature Propagation:** The architecture ensures that information and gradients are propagated more effectively throughout the network, leading to richer and more discriminative feature representations.\\n\\nDenseNet-121 consists of four dense blocks with transition layers between them. Each dense block comprises multiple convolutional layers, where the output of each layer is concatenated with the inputs of all subsequent layers within that block. The transition layers perform downsampling via batch normalization, a 1x1 convolution, and a 2x2 average pooling layer. The final layer is a global average pooling layer followed by a fully connected layer for classification.\\n\\nIn our system, preprocessed video frames (or sequences of frames for temporal analysis) are fed into the DenseNet-121 model. The model extracts a high-dimensional vector of features that encapsulate intricate visual cues related to student posture, head orientation, facial expressions (e.g., gaze direction, mouth movements), and subtle body language. These features serve as the input for the subsequent classification task.\\n\\n#### 2.3. Quantum Evolutionary Algorithm (QEA) for Optimization\\n\\nTo enhance the performance and generalization of the DenseNet-121 model for student behavior detection, we integrate the Quantum Evolutionary Algorithm (QEA) (Han & Kim, 2002). QEA is a metaheuristic optimization algorithm inspired by the principles of quantum mechanics, such as superposition, entanglement, and quantum gates. Unlike traditional genetic algorithms that use binary or real-valued chromosomes, QEA employs quantum bits (qubits) to represent solutions. A qubit, represented as a pair of complex numbers $(\\\\alpha, \\\\beta)$, can be in a superposition of states 0 and 1, meaning it can represent both states simultaneously with certain probabilities ($|\\\\alpha|^2$ and $|\\\\beta|^2$).\\n\\n**How QEA Optimizes:**\\n\\nQEA is used to optimize critical aspects of the DenseNet-121 model, primarily focusing on:\\n\\n1.  **Hyperparameter Optimization:** QEA can efficiently search for optimal hyperparameters of the DenseNet-121 training process. These include:\\n    *   **Learning Rate:** The step size at which the model\\'s weights are updated during training.\\n    *   **Batch Size:** The number of samples processed before the model\\'s internal parameters are updated.\\n    *   **Regularization Strength (e.g., L2 regularization, dropout rates):** Parameters that prevent overfitting.\\n    *   **Optimizer Parameters:** Specific parameters for optimizers like Adam, SGD, etc.\\n    By encoding these hyperparameters as qubits, QEA can explore a vast search space simultaneously, leveraging its quantum-inspired operators (e.g., quantum rotation gates for updating probabilities, mutation for diversity) to converge towards an optimal set of parameters that maximize the model\\'s accuracy and F1-score on a validation set.\\n\\n2.  **Feature Weighting/Selection (Potential Application):** While DenseNet-121 inherently performs strong feature extraction, QEA could potentially be applied to optimize the weighting of different feature maps or even select the most discriminative features extracted by the network\\'s intermediate layers. This would involve designing a fitness function that evaluates the classification performance based on a subset or weighted combination of features, guiding QEA to identify the most salient visual cues for specific behaviors.\\n\\n**Benefits of QEA:**\\n\\n*   **Global Search Capability:** QEA\\'s use of superposition allows it to represent multiple solutions simultaneously, enhancing its ability to explore the search space broadly and avoid getting trapped in local optima, a common issue with gradient-based methods or simpler evolutionary algorithms.\\n*   **Faster Convergence:** By manipulating probability amplitudes rather than discrete values, QEA can often converge to optimal solutions more rapidly, especially in high-dimensional and complex search spaces.\\n*   **Robustness:** Its quantum-inspired operators provide a robust mechanism for maintaining population diversity and escaping suboptimal regions.\\n\\n#### 2.4. Integration and Training\\n\\nThe QEA-DenseNet-121 integration works as follows:\\n1.  **Initialization:** QEA initializes a population of quantum-bit individuals, each representing a potential set of DenseNet-121 hyperparameters.\\n2.  **Evaluation:** For each individual (hyperparameter set), a DenseNet-121 model is trained on the behavior dataset. The model\\'s performance (e.g., validation accuracy, F1-score) serves as the fitness value for the QEA individual.\\n3.  **Update:** Based on the fitness values, QEA applies quantum rotation gates to update the probability amplitudes of the qubits in the population, guiding them towards better solutions. Quantum mutation operations are also applied to maintain diversity.\\n4.  **Iteration:** Steps 2 and 3 are repeated for a predefined number of generations or until a satisfactory performance level is achieved. The best individual found by QEA represents the optimal hyperparameter configuration for DenseNet-121.\\n\\nThis optimized DenseNet-121 model is then deployed for real-time behavior detection.\\n\\n#### 2.5. Ethical Handling of Student Information\\n\\nThe deployment of computer vision in classrooms raises significant ethical concerns, particularly regarding student privacy and data security. Our methodology incorporates the following measures:\\n\\n*   **Informed Consent:** Explicit and comprehensive informed consent is obtained from parents/guardians and students (where age-appropriate) before any data collection. The purpose, scope, data handling procedures, and potential risks are clearly communicated.\\n*   **Anonymization and Pseudonymization:** Where possible, student identities are anonymized or pseudonymized during data processing and analysis. Facial recognition features are not used for identification purposes but rather for expression and gaze analysis.\\n*   **Data Security:** All collected data is stored on secure, encrypted servers with restricted access, adhering to data protection regulations (e.g., GDPR, FERPA).\\n*   **Purpose Limitation:** Data is used strictly for the stated purpose of behavior management research and improvement, not for surveillance, disciplinary action, or commercial exploitation.\\n*   **Human Oversight:** The system is designed to be a supportive tool for educators, not a replacement for human judgment. Alerts and analyses are presented as insights, requiring teacher interpretation and intervention.\\n*   **Transparency:** The algorithms and their decision-making processes are made as transparent as possible to stakeholders.\\n*   **Data Retention Policy:** A clear policy on data retention and deletion is established, ensuring data is not stored indefinitely.\\n\\n### 3. Results and Discussion\\n\\nThe integration of QEA with DenseNet-121 for student classroom behavior management is expected to yield significant improvements over traditional methods and standalone deep learning approaches. While this paper outlines a theoretical framework, the anticipated results and their implications are discussed below, alongside critical limitations.\\n\\n#### 3.1. Expected Outcomes\\n\\n1.  **Improved Accuracy and Robustness:** The primary expected outcome is a substantial increase in the accuracy, precision, recall, and F1-score for classifying various student behaviors. QEA\\'s global optimization capabilities are anticipated to fine-tune DenseNet-121\\'s hyperparameters more effectively, leading to a model that generalizes better to unseen data and performs robustly across different classroom environments, lighting conditions, and student demographics. This enhanced accuracy will minimize false positives (e.g., misinterpreting a thoughtful pause as distraction) and false negatives (e.g., missing subtle signs of disengagement).\\n\\n2.  **Objective and Real-time Behavioral Analysis:** The system is designed to provide objective, data-driven insights into student behavior in real-time. This eliminates the subjectivity inherent in human observation and allows for immediate detection of behavioral shifts. Teachers can receive instant alerts for potential issues (e.g., prolonged distraction, signs of distress) or aggregate data on overall class engagement.\\n\\n3.  **Enhanced Engagement Analysis:** Beyond simple classification, the system can quantify levels of student engagement. By analyzing metrics such as gaze direction, head pose, facial expressions (e.g., smiles, frowns), and body posture, the system can provide a nuanced understanding of how attentive and involved students are during different activities or lessons. This data can inform pedagogical adjustments.\\n\\n4.  **Proactive Intervention and Personalized Learning:** With real-time insights, educators can move from reactive to proactive behavior management. Early detection of disengagement or potential disruption allows for timely, targeted interventions before behaviors escalate. Furthermore, aggregated behavioral data over time can help identify individual student learning patterns and preferences, enabling teachers to tailor teaching strategies and provide personalized support.\\n\\n5.  **Reduced Teacher Workload and Stress:** By automating a significant portion of behavioral monitoring, the system can free up teachers\\' time and mental energy, allowing them to focus more on instruction, student interaction, and addressing complex individual needs rather than constant surveillance. This can contribute to reduced teacher burnout and improved job satisfaction.\\n\\n6.  **Data-Driven Pedagogical Improvement:** The system generates rich datasets on classroom dynamics. This data can be analyzed to identify correlations between specific teaching methods, lesson content, or classroom activities and student engagement levels. Such insights can inform curriculum development, teacher training, and overall educational policy.\\n\\n#### 3.2. Discussion of Limitations\\n\\nDespite the promising potential, the deployment of computer vision for classroom behavior management is fraught with significant limitations and challenges that require careful consideration:\\n\\n1.  **Privacy and Surveillance Concerns:** This is the most critical limitation. The continuous monitoring of students raises profound ethical questions about privacy, autonomy, and the potential for a \"surveillance culture\" in schools. Students might feel constantly watched, leading to anxiety, reduced spontaneity, and a chilling effect on natural behavior. The line between supportive monitoring and intrusive surveillance is delicate and easily crossed.\\n\\n2.  **Data Bias and Fairness:** The performance of AI models is heavily dependent on the quality and representativeness of their training data. If the dataset lacks diversity in terms of student demographics (ethnicity, gender, socioeconomic background), cultural expressions, or behavioral manifestations, the model may exhibit biases. This could lead to misclassification of behaviors for certain groups, potentially perpetuating stereotypes or unfairly targeting specific students. For example, cultural differences in non-verbal communication might be misinterpreted.\\n\\n3.  **Computational Cost and Infrastructure:** Real-time video processing and deep learning inference require substantial computational resources (high-performance GPUs). Deploying such a system across multiple classrooms or an entire school district would entail significant infrastructure investment and ongoing operational costs, which might be prohibitive for many educational institutions.\\n\\n4.  **Ethical Implications and Misinterpretation:**\\n    *   **Misinterpretation of Behavior:** AI models, no matter how accurate, lack human empathy and contextual understanding. A student looking away might be distracted, or they might be deep in thought, processing information. A fidgeting student might be disruptive, or they might have ADHD and be self-regulating. Misinterpreting these nuances can lead to inappropriate interventions or labeling of students.\\n    *   **Impact on Student Well-being:** Constant monitoring could negatively impact student mental health, fostering a sense of distrust or reducing their willingness to express themselves naturally.\\n    *   **Data Misuse:** Despite ethical guidelines, there is always a risk of data being misused for disciplinary purposes, grading, or even shared with third parties without consent.\\n\\n5.  **Generalizability and Environmental Variability:** Models trained in one classroom environment (e.g., lighting, seating arrangement, class size, cultural context) may not perform well in another. The system needs to be robust to variations in camera angles, student attire, and dynamic classroom activities, which is a significant technical challenge.\\n\\n6.  **False Positives and Negatives:** Even with high accuracy, a small percentage of errors can have significant consequences. A false positive (e.g., flagging an engaged student as distracted) can lead to unnecessary intervention, while a false negative (missing a genuinely distressed student) can have serious repercussions.\\n\\n7.  **Lack of Contextual Understanding:** The system primarily analyzes visual cues. It cannot understand the underlying reasons for a behavior (e.g., a student is distracted because they are ill, or because the lesson is genuinely unengaging). This limits its ability to provide holistic solutions.\\n\\nAddressing these limitations requires a multi-faceted approach involving robust technical development, stringent ethical frameworks, transparent communication, and continuous human oversight. The technology should serve as an assistive tool, augmenting human capabilities, rather than replacing the nuanced judgment of experienced educators.\\n\\n### 4. Conclusion\\n\\nThe integration of computer vision, specifically through the enhanced QEA-DenseNet-121 framework, offers a transformative pathway for student classroom behavior management. By providing objective, real-time, and data-driven insights into student engagement and conduct, this AI-driven approach holds immense potential to address the long-standing challenges faced by educators. The proposed system promises improved accuracy in behavior detection, enabling proactive interventions, facilitating personalized learning strategies, and ultimately fostering more effective and equitable learning environments. The synergistic power of DenseNet-121 for deep feature extraction and QEA for robust model optimization positions this framework as a cutting-edge solution in educational technology.\\n\\nHowever, the deployment of such powerful AI tools in sensitive environments like classrooms necessitates a profound commitment to ethical considerations. While the technological capabilities are compelling, the potential for privacy infringement, data bias, and misinterpretation of complex human behaviors cannot be overstated. The success and acceptance of computer vision in education hinge not just on its technical prowess but equally on its responsible, transparent, and human-centric implementation.\\n\\n**Directions for Future Research:**\\n\\n1.  **Robustness and Generalizability:** Future work should focus on developing models that are highly robust to varying classroom conditions (lighting, camera angles, student diversity) and can generalize effectively across different educational settings and cultural contexts. This may involve larger, more diverse datasets and advanced domain adaptation techniques.\\n2.  **Explainable AI (XAI):** Integrating XAI techniques to provide transparent explanations for behavioral classifications would enhance trust and allow educators to understand *why* a particular behavior was detected, rather than just *what* was detected. This can aid in human oversight and decision-making.\\n3.  **Multi-modal Data Fusion:** Combining visual data with other modalities such as audio (e.g., speech patterns, classroom noise levels, while respecting privacy), physiological sensors (e.g., heart rate, skin conductance for stress detection, with strict ethical protocols), or even learning management system (LMS) data could provide a more holistic and accurate understanding of student states.\\n4.  **Longitudinal Impact Studies:** Conducting long-term studies to assess the actual impact of AI-driven behavior management systems on student learning outcomes, well-being, teacher workload, and classroom dynamics is crucial for validating their real-world efficacy and identifying unintended consequences.\\n5.  **Standardized Ethical Frameworks and Policies:** Collaborative efforts between AI researchers, educators, policymakers, and ethicists are needed to develop comprehensive, legally binding, and internationally recognized ethical guidelines and policies for the use of AI in education, particularly concerning student data and privacy.\\n6.  **Teacher Training and Integration:** Research into effective strategies for training educators to utilize these tools effectively, interpret their outputs, and integrate them seamlessly into their pedagogical practices is essential for successful adoption.\\n\\nIn conclusion, while the promise of AI-driven behavioral analysis in education is significant, its realization demands a balanced approach that prioritizes ethical considerations, human oversight, and a deep understanding of pedagogical needs. The goal should be to augment, not replace, the invaluable role of human educators in nurturing the next generation.\\n\\n### 5. References\\n\\n*   Evertson, C. M., & Weinstein, C. S. (Eds.). (2006). *Handbook of Classroom Management: Research, Practice, and Contemporary Issues*. Lawrence Erlbaum Associates.\\n*   Han, K. H., & Kim, J. H. (2002). Quantum-inspired evolutionary algorithm for a class of combinatorial optimization. *IEEE Transactions on Evolutionary Computation*, *6*(6), 580-593.\\n*   Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely Connected Convolutional Networks. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 4700-4708).\\n*   Marzano, R. J., & Marzano, J. S. (2003). *The Key to Classroom Management*. Educational Leadership, 61(1), 6-13.\\n*   Redmon, J., & Farhadi, A. (2018). YOLOv3: An Incremental Improvement. *arXiv preprint arXiv:1804.02767*.\\n*   Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In *Advances in neural information processing systems* (pp. 91-99).\\n*   UNESCO. (2019). *AI and education: Guidance for policy-makers*. UNESCO Publishing.\\n*   Wang, L., Chen, X., & Li, X. (2020). A Survey on Deep Learning for Human Behavior Analysis. *IEEE Transactions on Systems, Man, and Cybernetics: Systems*, *50*(10), 3718-3731.\\n*   Zou, J., Ni, J., & Cao, J. (2019). Deep Learning for Human Behavior Recognition: A Survey. *ACM Computing Surveys (CSUR)*, *52*(3), 1-36.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--fc9bbdcb-a155-464e-8041-b8080e87b96c-0')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "structured_formated_prompt_3 = \"\"\"Write a detailed academic paper on Student Classroom Behavior Management Based on Computer Vision.\n",
        "The paper should include the following sections with specific content guidelines:\n",
        "- **Title:** Provide a clear, precise, and professional title that reflects the research focus.\n",
        "- **Abstract:** Summarize the purpose, approach, main findings, and implications of using computer vision for managing student behavior.\n",
        "- **Introduction:** Explain the importance of classroom behavior management, the challenges faced by educators, and introduce how computer vision technologies can provide effective monitoring and analysis solutions.\n",
        "- **Methods:** Describe how the Quantum Evolutionary Algorithm (QEA) is integrated with the DenseNet-121 model to enhance student behavior detection. Explain briefly how QEA optimizes parameters or features for better accuracy, while DenseNet-121 performs deep feature extraction from classroom video data. Mention the overall workflow‚Äîdata collection, preprocessing, and ethical handling of student information.\n",
        "- **Results and Discussion:** Summarize expected outcomes such as improved accuracy and engagement analysis, and briefly discuss key limitations like privacy, data bias, and computational cost.\n",
        "- **Conclusion:** Summarize key insights, restate the importance of AI-driven behavioral analysis, and suggest directions for future research or ethical guidelines.\n",
        "- **References:** Include citations from relevant studies, papers, and datasets related to computer vision in education and behavioral analysis.\n",
        "\n",
        "Ensure academic tone, logical flow, and clarity throughout the paper, providing evidence, reasoning, and real-world relevance in each section.\n",
        "\"\"\"\n",
        "\n",
        "llm.invoke(structured_formated_prompt_3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "473f096c",
      "metadata": {
        "id": "473f096c"
      },
      "source": [
        "### 1.3. Compare and Analyze Results\n",
        "\n",
        "As we can see from the above invokes, the generated texts of the more complete prompts came more **detailed** and more <b>compatible</b> with the thing we actually wanted. Hence the Quality of the generated text (Here Paper) increased."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc9bf2f9",
      "metadata": {
        "id": "dc9bf2f9"
      },
      "source": [
        "## 2. Effects of Few-Shot Prompts in Writing Imitation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c7d200",
      "metadata": {
        "id": "a2c7d200"
      },
      "source": [
        "### 2.1. Topic selection & Writing style\n",
        "\n",
        "Topic: `\"**Summarize** the paper. **Title** is: Student Classroom Behavior Management Based on Computer Vision Using Quantum Evolutionary Algorithm with DenseNet 121 Model. **Abstract** is: Student recognition and the evaluation of classroom education have both grown to depend heavily on the student classroom behavior management in recent years. The method used to assess and analyze student behavior in the classroom establishes whether the student's giving attention or not. However, because of the complexity of classroom conduct, it was difficult to identify intelligent students. Therefore, in this research Quantum Evolutionary Algorithm with DenseNet 121 (QEA-DenseNet 121) is suggested by studying computer vision of student behavior in the classroom. Usually, the recommended system design is used to evaluate the system's testing and training procedures. The final input images are then subjected to human location estimate using a camera to capture subsequent frames. The error correcting system is integrated with the body position estimate and person recognition algorithms. Lastly, a model known as QEA-DenseNet-121 is recommended as a practical resource for precisely evaluating student behavior in the classroom. Results showed that the suggested approach outperformed the existing models such as Skeleton Pose Estimation (SPE), YOLO-v4 and Intelligent Real-Time Vision (IRTV) with relative gains in Average Accuracy of 99.64%, Precision of 99.53%, Recall of99.71 %, and F1-measure of 99.49%.\"`\n",
        "\n",
        "Writing style: `\"Write in a clear, practical tone accessible to professionals who know the field but aren‚Äôt dominant experts, avoiding jargon and emphasizing key ideas with real-world relevance.\"`\n",
        "\n",
        "It is something between **professional** and **practical** and **easy to understand**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "146e4da3",
      "metadata": {
        "id": "146e4da3"
      },
      "source": [
        "### 2.2. Run prompt without few-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9fad8323",
      "metadata": {
        "id": "9fad8323"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a17e00cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a17e00cf",
        "outputId": "c88eac54-6673-4f37-e00e-f23df25c335d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"\n",
            "Summarize** the paper. \n",
            "**Title** is: Student Classroom Behavior Management Based on Computer Vision Using Quantum Evolutionary Algorithm with DenseNet 121 Model. **Abstract** is: Student recognition and the evaluation of classroom education have both grown to depend heavily on the student classroom behavior management in recent years. The method used to assess and analyze student behavior in the classroom establishes whether the student's giving attention or not. However, because of the complexity of classroom conduct, it was difficult to identify intelligent students. Therefore, in this research Quantum Evolutionary Algorithm with DenseNet 121 (QEA-DenseNet 121) is suggested by studying computer vision of student behavior in the classroom. Usually, the recommended system design is used to evaluate the system's testing and training procedures. The final input images are then subjected to human location estimate using a camera to capture subsequent frames. The error correcting system is integrated with the body position estimate and person recognition algorithms. Lastly, a model known as QEA-DenseNet-121 is recommended as a practical resource for precisely evaluating student behavior in the classroom. Results showed that the suggested approach outperformed the existing models such as Skeleton Pose Estimation (SPE), YOLO-v4 and Intelligent Real-Time Vision (IRTV) with relative gains in Average Accuracy of 99.64%, Precision of 99.53%, Recall of99.71 %, and F1-measure of 99.49%.\n",
            "Write in a clear, practical tone accessible to professionals who know the field but aren‚Äôt dominant experts, avoiding jargon and emphasizing key ideas with real-world relevance.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "zero_shot_prompt_template = \"\"\"\"\n",
        "Summarize** the paper.\n",
        "{paper}\n",
        "Write in a clear, practical tone accessible to professionals who know the field but aren‚Äôt dominant experts, avoiding jargon and emphasizing key ideas with real-world relevance.\n",
        "\"\"\"\n",
        "\n",
        "paper_text = \"**Title** is: Student Classroom Behavior Management Based on Computer Vision Using Quantum Evolutionary Algorithm with DenseNet 121 Model. **Abstract** is: Student recognition and the evaluation of classroom education have both grown to depend heavily on the student classroom behavior management in recent years. The method used to assess and analyze student behavior in the classroom establishes whether the student's giving attention or not. However, because of the complexity of classroom conduct, it was difficult to identify intelligent students. Therefore, in this research Quantum Evolutionary Algorithm with DenseNet 121 (QEA-DenseNet 121) is suggested by studying computer vision of student behavior in the classroom. Usually, the recommended system design is used to evaluate the system's testing and training procedures. The final input images are then subjected to human location estimate using a camera to capture subsequent frames. The error correcting system is integrated with the body position estimate and person recognition algorithms. Lastly, a model known as QEA-DenseNet-121 is recommended as a practical resource for precisely evaluating student behavior in the classroom. Results showed that the suggested approach outperformed the existing models such as Skeleton Pose Estimation (SPE), YOLO-v4 and Intelligent Real-Time Vision (IRTV) with relative gains in Average Accuracy of 99.64%, Precision of 99.53%, Recall of99.71 %, and F1-measure of 99.49%.\"\n",
        "\n",
        "zero_shot_prompt_template = PromptTemplate(template=zero_shot_prompt_template, input_variables=[\"paper\"])\n",
        "\n",
        "zero_shot_prompt = zero_shot_prompt_template.format(paper=paper_text)\n",
        "\n",
        "print(zero_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0bcee3da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bcee3da",
        "outputId": "dd66c996-a527-4974-9a31-c0e37cafab0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='This paper introduces a new computer vision system, QEA-DenseNet 121 (Quantum Evolutionary Algorithm with DenseNet 121), designed to automatically analyze and manage student behavior in the classroom.\\n\\n**The core problem it addresses** is the increasing need for accurate student recognition and engagement assessment in educational settings. Traditional methods struggle to consistently identify whether students are attentive or disengaged due to the complex and dynamic nature of classroom interactions.\\n\\n**The proposed solution** leverages advanced artificial intelligence and camera technology. The QEA-DenseNet 121 system works by:\\n1.  Capturing video frames from a classroom camera.\\n2.  Estimating human location and body posture within those frames.\\n3.  Performing person recognition to identify individual students.\\n4.  Integrating an error correction system to refine these observations.\\n\\n**The practical goal** is to provide educators and administrators with a precise and reliable tool for evaluating student behavior in real-time. This allows for better understanding of student engagement, which can inform teaching strategies and improve overall educational outcomes.\\n\\n**Key findings** show that the QEA-DenseNet 121 model significantly outperforms existing computer vision approaches like Skeleton Pose Estimation (SPE), YOLO-v4, and Intelligent Real-Time Vision (IRTV). It achieved impressive performance metrics, including an average accuracy of 99.64%, precision of 99.53%, recall of 99.71%, and an F1-measure of 99.49%.\\n\\nIn essence, this research offers a highly effective and automated method for monitoring and understanding student behavior, providing valuable data to enhance classroom management and educational effectiveness.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--d63f26ef-e4c1-4136-973e-1c84d42fa8c8-0')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(zero_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3548f0c3",
      "metadata": {
        "id": "3548f0c3"
      },
      "source": [
        "### 2.3. Add some exmpales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b3b4378f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3b4378f",
        "outputId": "e13a507f-7737-4288-f228-b449a59b9a13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"\n",
            "Summarize** the paper. \n",
            "**Title** is: Student Classroom Behavior Management Based on Computer Vision Using Quantum Evolutionary Algorithm with DenseNet 121 Model. **Abstract** is: Student recognition and the evaluation of classroom education have both grown to depend heavily on the student classroom behavior management in recent years. The method used to assess and analyze student behavior in the classroom establishes whether the student's giving attention or not. However, because of the complexity of classroom conduct, it was difficult to identify intelligent students. Therefore, in this research Quantum Evolutionary Algorithm with DenseNet 121 (QEA-DenseNet 121) is suggested by studying computer vision of student behavior in the classroom. Usually, the recommended system design is used to evaluate the system's testing and training procedures. The final input images are then subjected to human location estimate using a camera to capture subsequent frames. The error correcting system is integrated with the body position estimate and person recognition algorithms. Lastly, a model known as QEA-DenseNet-121 is recommended as a practical resource for precisely evaluating student behavior in the classroom. Results showed that the suggested approach outperformed the existing models such as Skeleton Pose Estimation (SPE), YOLO-v4 and Intelligent Real-Time Vision (IRTV) with relative gains in Average Accuracy of 99.64%, Precision of 99.53%, Recall of99.71 %, and F1-measure of 99.49%.\n",
            "Write in a clear, practical tone accessible to professionals who know the field but aren‚Äôt dominant experts, avoiding jargon and emphasizing key ideas with real-world relevance.\n",
            "\n",
            "input -> Title is: Deep Learning for Crop Disease Detection. Abstract is: Crop diseases cause significant losses in agriculture. This paper explores a convolutional neural network (CNN)-based approach for detecting crop leaf diseases using image datasets. The proposed model achieved 97% accuracy and showed improved generalization compared to traditional image processing methods.\n",
            "output ->  The study presents a CNN-based system for identifying crop leaf diseases from images, achieving 97% accuracy. It outperforms older image processing methods by automatically learning visual disease patterns, offering a practical solution for precision agriculture.\n",
            "\n",
            "\n",
            "input -> Title is: Deep Learning for Crop Disease Detection. Abstract is: Crop diseases cause significant losses in agriculture. This paper explores a convolutional neural network (CNN)-based approach for detecting crop leaf diseases using image datasets. The proposed model achieved 97% accuracy and showed improved generalization compared to traditional image processing methods.\n",
            "output ->  The research introduces an RNN-based traffic prediction model that better captures time dependencies in data, outperforming traditional models like ARIMA and SVM. It demonstrates strong potential for real-time smart city traffic management.\n",
            "\n",
            "input -> Title is: Sentiment Analysis of Customer Reviews Using Transformer Models. Abstract is: This paper evaluates transformer-based models for analyzing sentiment in online reviews. Compared to conventional RNN and CNN approaches, the transformer model achieves superior accuracy and better handles contextual language variations in text.\n",
            "output -> This study applies transformer models for customer review sentiment analysis, showing higher accuracy and improved context understanding over older deep learning methods, making it effective for modern NLP applications.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "few_shot_prompt_template = \"\"\"\"\n",
        "Summarize** the paper.\n",
        "{paper}\n",
        "Write in a clear, practical tone accessible to professionals who know the field but aren‚Äôt dominant experts, avoiding jargon and emphasizing key ideas with real-world relevance.\n",
        "{exp1}\n",
        "{exp2}\n",
        "{exp3}\n",
        "\"\"\"\n",
        "\n",
        "paper_text = \"**Title** is: Student Classroom Behavior Management Based on Computer Vision Using Quantum Evolutionary Algorithm with DenseNet 121 Model. **Abstract** is: Student recognition and the evaluation of classroom education have both grown to depend heavily on the student classroom behavior management in recent years. The method used to assess and analyze student behavior in the classroom establishes whether the student's giving attention or not. However, because of the complexity of classroom conduct, it was difficult to identify intelligent students. Therefore, in this research Quantum Evolutionary Algorithm with DenseNet 121 (QEA-DenseNet 121) is suggested by studying computer vision of student behavior in the classroom. Usually, the recommended system design is used to evaluate the system's testing and training procedures. The final input images are then subjected to human location estimate using a camera to capture subsequent frames. The error correcting system is integrated with the body position estimate and person recognition algorithms. Lastly, a model known as QEA-DenseNet-121 is recommended as a practical resource for precisely evaluating student behavior in the classroom. Results showed that the suggested approach outperformed the existing models such as Skeleton Pose Estimation (SPE), YOLO-v4 and Intelligent Real-Time Vision (IRTV) with relative gains in Average Accuracy of 99.64%, Precision of 99.53%, Recall of99.71 %, and F1-measure of 99.49%.\"\n",
        "exp1_text = \"\"\"\n",
        "input -> Title is: Deep Learning for Crop Disease Detection. Abstract is: Crop diseases cause significant losses in agriculture. This paper explores a convolutional neural network (CNN)-based approach for detecting crop leaf diseases using image datasets. The proposed model achieved 97% accuracy and showed improved generalization compared to traditional image processing methods.\n",
        "output ->  The study presents a CNN-based system for identifying crop leaf diseases from images, achieving 97% accuracy. It outperforms older image processing methods by automatically learning visual disease patterns, offering a practical solution for precision agriculture.\n",
        "\"\"\"\n",
        "exp2_text = \"\"\"\n",
        "input -> Title is: Deep Learning for Crop Disease Detection. Abstract is: Crop diseases cause significant losses in agriculture. This paper explores a convolutional neural network (CNN)-based approach for detecting crop leaf diseases using image datasets. The proposed model achieved 97% accuracy and showed improved generalization compared to traditional image processing methods.\n",
        "output ->  The research introduces an RNN-based traffic prediction model that better captures time dependencies in data, outperforming traditional models like ARIMA and SVM. It demonstrates strong potential for real-time smart city traffic management.\"\"\"\n",
        "exp3_text = \"\"\"\n",
        "input -> Title is: Sentiment Analysis of Customer Reviews Using Transformer Models. Abstract is: This paper evaluates transformer-based models for analyzing sentiment in online reviews. Compared to conventional RNN and CNN approaches, the transformer model achieves superior accuracy and better handles contextual language variations in text.\n",
        "output -> This study applies transformer models for customer review sentiment analysis, showing higher accuracy and improved context understanding over older deep learning methods, making it effective for modern NLP applications.\n",
        "\"\"\"\n",
        "\n",
        "few_shot_prompt_template = PromptTemplate(template=few_shot_prompt_template, input_variables=[\"paper\", 'exp1', 'exp2', 'exp3'])\n",
        "\n",
        "zero_shot_prompt = few_shot_prompt_template.format(paper=paper_text, exp1=exp1_text, exp2=exp2_text, exp3=exp3_text)\n",
        "\n",
        "print(zero_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9932062f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9932062f",
        "outputId": "30f93fff-921b-419e-d3dc-90c932f5dfba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='This research tackles the complex task of accurately assessing student behavior and attention in classrooms, which is vital for effective education. It introduces a new computer vision system, called QEA-DenseNet 121, that uses cameras to analyze student actions. This system identifies students, estimates their body positions, and integrates an error correction mechanism to precisely evaluate their behavior. The model significantly outperforms existing methods like SPE, YOLO-v4, and IRTV, achieving very high accuracy (around 99.6%) in assessing student engagement. This offers a practical and highly effective tool for educators to understand and manage classroom dynamics.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--623f6534-01f8-4544-bd0b-e714580dfc35-0')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(zero_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "666ae834",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "666ae834",
        "outputId": "b276f146-f00e-4731-8dec-dae61089c270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Here are 3 random words:\\n\\n1.  **Cloud**\\n2.  **Chair**\\n3.  **Banana**' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--564ed9e6-7fde-48b7-a6ac-ddf834dbcd11'\n"
          ]
        }
      ],
      "source": [
        "# I used stream for the sake of just using it for once\n",
        "\n",
        "for chunk in llm.stream(\"write 3 random words\"):\n",
        "    print(chunk, end=\"\\n\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a46ebb7b",
      "metadata": {
        "id": "a46ebb7b"
      },
      "source": [
        "### 2.4. Compare & Analyze results\n",
        "\n",
        "#### After adding the samples, did the model better adhere to the selected sampling style?\n",
        "\n",
        "As we can see from the results the one with the few-shots (some examples) works better than the zero-shot one.\n",
        "\n",
        "#### Does it still need improvement? If yes, how many more samples can be added?\n",
        "\n",
        "If we add too much exmpales it may go in another direction that is not so desirable for us. It may overshadow the actual instrcution that we gave the prompt when we use more examples than needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168a8740",
      "metadata": {
        "id": "168a8740"
      },
      "source": [
        "## 3. The impact of few-shot prompting on generating structured outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3a25214",
      "metadata": {
        "id": "c3a25214"
      },
      "source": [
        "### 3.1. Define the Required Info\n",
        "\n",
        "### Information to Include\n",
        "\n",
        "- **Contact Information:** Name, Email, Phone Number  \n",
        "- **Education:** Degree, Major, University, Year of Graduation  \n",
        "- **Work Experience:** Job Title, Company, Duration, Main Responsibilities  \n",
        "- **Skills:** Technical and Soft Skills  \n",
        "- **Certificates and Training Courses**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0558800d",
      "metadata": {
        "id": "0558800d"
      },
      "source": [
        "### 3.2 Prompt to Extract Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e87c5173",
      "metadata": {
        "id": "e87c5173"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "zero_shot_ie_template = \"\"\"\n",
        "You are an information extraction system. Study the following resume and Extract the requested fields from the r√©sum√© below.\n",
        "\n",
        "RESUM√â:\n",
        "{resume_text}\n",
        "\n",
        "TASK:\n",
        "- Extract the following information:\n",
        "  - Contact Information: name, email, phone\n",
        "  - Education: degree, major, university, graduation year\n",
        "  - Work Experience: job title, company, duration (or start/end dates), main responsibilities\n",
        "  - Skills: technical skills, soft skills\n",
        "  - Certificates and training courses\n",
        "\n",
        "OUTPUT REQUIREMENTS:\n",
        "- Return **JSON only**, no extra text.\n",
        "- Use the following structure and keys exactly.\n",
        "- If something is missing, return an empty string \"\" or empty list [].\n",
        "- Keep arrays even if there is only one item.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "zero_shot_ie_prompt = PromptTemplate(template=zero_shot_ie_template, input_variables=[\"resume_text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "e0db932b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "e0db932b",
        "outputId": "8fa7e150-a30b-4710-833b-6dd560c76e5d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c888c9bb-0643-44a9-bbf0-c3031603f5ff\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c888c9bb-0643-44a9-bbf0-c3031603f5ff\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving resume_sample1.txt to resume_sample1.txt\n",
            "\n",
            "You are an information extraction system. Study the following persian resume and Extract the requested fields from the r√©sum√© below in english.\n",
            "\n",
            "RESUM√â:\n",
            "Full Name: Sara Moradi  \n",
            "Email: sara.moradi@example.com  \n",
            "Phone: +98 912 123 4567  \n",
            "\n",
            "üéì Education:  \n",
            "- MSc in Data Science, University of Tehran (2021)  \n",
            "- BSc in Computer Engineering, Sharif University (2018)  \n",
            "\n",
            "üíº Work Experience:  \n",
            "- Data Scientist at Snapp (2022-Present)  \n",
            "  ‚Ä¢ Developed machine learning models for customer segmentation  \n",
            "  ‚Ä¢ Optimized recommendation systems for better user experience  \n",
            "- AI Researcher at Digikala (2019-2022)  \n",
            "  ‚Ä¢ Conducted NLP research for automatic product categorization  \n",
            "\n",
            "üîß Skills: Python, TensorFlow, PyTorch, SQL, Data Visualization  \n",
            "\n",
            "üìú Certifications:  \n",
            "- Google Data Analytics Professional Certificate  \n",
            "- Deep Learning Specialization by Andrew Ng (Coursera)  \n",
            "\n",
            "\n",
            "TASK:\n",
            "- Extract the following information:\n",
            "  - Contact Information: name, email, phone\n",
            "  - Education: degree, major, university, graduation year\n",
            "  - Work Experience: job title, company, duration (or start/end dates), main responsibilities\n",
            "  - Skills: technical skills, soft skills\n",
            "  - Certificates and training courses\n",
            "\n",
            "OUTPUT REQUIREMENTS:\n",
            "- Return **JSON only**, no extra text.\n",
            "- Use the following structure and keys exactly.\n",
            "- If something is missing, return an empty string \"\" or empty list [].\n",
            "- Keep arrays even if there is only one item.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_name1 = '/content/resume_sample1.txt'\n",
        "# file_name2 = '/content/resume_sample2.txt'\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "with open(file_name1) as file:\n",
        "    resume_text = file.read()\n",
        "\n",
        "formatted_ie_prompt = zero_shot_ie_prompt.format(resume_text=resume_text)\n",
        "print(formatted_ie_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "bc03b39f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc03b39f",
        "outputId": "e54496d3-9e51-4fea-982f-b43a30c257c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The extracted information has been saved to 'zero_shot_information_extractor.json'\n",
            "```json\n",
            "{\n",
            "  \"contact_information\": {\n",
            "    \"name\": \"Sara Moradi\",\n",
            "    \"email\": \"sara.moradi@example.com\",\n",
            "    \"phone\": \"+98 912 123 4567\"\n",
            "  },\n",
            "  \"education\": [\n",
            "    {\n",
            "      \"degree\": \"MSc\",\n",
            "      \"major\": \"Data Science\",\n",
            "      \"university\": \"University of Tehran\",\n",
            "      \"graduation_year\": \"2021\"\n",
            "    },\n",
            "    {\n",
            "      \"degree\": \"BSc\",\n",
            "      \"major\": \"Computer Engineering\",\n",
            "      \"university\": \"Sharif University\",\n",
            "      \"graduation_year\": \"2018\"\n",
            "    }\n",
            "  ],\n",
            "  \"work_experience\": [\n",
            "    {\n",
            "      \"job_title\": \"Data Scientist\",\n",
            "      \"company\": \"Snapp\",\n",
            "      \"duration\": \"2022-Present\",\n",
            "      \"start_date\": \"2022\",\n",
            "      \"end_date\": \"Present\",\n",
            "      \"responsibilities\": [\n",
            "        \"Developed machine learning models for customer segmentation\",\n",
            "        \"Optimized recommendation systems for better user experience\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"job_title\": \"AI Researcher\",\n",
            "      \"company\": \"Digikala\",\n",
            "      \"duration\": \"2019-2022\",\n",
            "      \"start_date\": \"2019\",\n",
            "      \"end_date\": \"2022\",\n",
            "      \"responsibilities\": [\n",
            "        \"Conducted NLP research for automatic product categorization\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"skills\": {\n",
            "    \"technical_skills\": [\n",
            "      \"Python\",\n",
            "      \"TensorFlow\",\n",
            "      \"PyTorch\",\n",
            "      \"SQL\",\n",
            "      \"Data Visualization\"\n",
            "    ],\n",
            "    \"soft_skills\": []\n",
            "  },\n",
            "  \"certificates_and_training_courses\": [\n",
            "    \"Google Data Analytics Professional Certificate\",\n",
            "    \"Deep Learning Specialization by Andrew Ng (Coursera)\"\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "response = llm.invoke(formatted_ie_prompt)\n",
        "\n",
        "response_data = response.content\n",
        "\n",
        "with open('zero_shot_information_extractor.json', 'w') as json_file:\n",
        "    json.dump(response_data, json_file, indent=4)\n",
        "\n",
        "print(\"The extracted information has been saved to 'zero_shot_information_extractor.json'\")\n",
        "print(response_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3086e266",
      "metadata": {
        "id": "3086e266"
      },
      "source": [
        "### 3.3. Determining the Exact Json structure (few-shot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "1d4994c8",
      "metadata": {
        "id": "1d4994c8"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "few_shot_ie_template = \"\"\"\n",
        "You are an information extraction system. Study the following examples and then extract data from the given r√©sum√©.\n",
        "\n",
        "---\n",
        "\n",
        "### EXAMPLE 1\n",
        "RESUM√â:\n",
        "Full Name: Reza Jafari\n",
        "Email: reza.jafari@example.com\n",
        "Phone: +98 912 654 3210\n",
        "\n",
        "üéì Education:\n",
        "- MSc in Artificial Intelligence, Amirkabir University (2022)\n",
        "- BSc in Computer Engineering, University of Isfahan (2018)\n",
        "\n",
        "üíº Work Experience:\n",
        "- Machine Learning Engineer at Turing Technologies (2022-Present)\n",
        "  ‚Ä¢ Developed machine learning models for predictive analysis\n",
        "  ‚Ä¢ Worked on deploying models into production environments\n",
        "- Software Developer at Parsian Co. (2018-2022)\n",
        "  ‚Ä¢ Built and optimized web applications and APIs\n",
        "\n",
        "üîß Skills: Python, Scikit-learn, TensorFlow, Flask, SQL\n",
        "\n",
        "üìú Certifications:\n",
        "- TensorFlow Developer Certificate\n",
        "- Machine Learning by Stanford University (Coursera)\n",
        "\n",
        "OUTPUT:\n",
        "{{\n",
        "  \"contact\": {{\n",
        "    \"name\": \"Reza Jafari\",\n",
        "    \"email\": \"reza.jafari@example.com\",\n",
        "    \"phone\": \"+98 912 654 3210\"\n",
        "  }} ,\n",
        "  \"education\": [\n",
        "    {{\n",
        "      \"degree\": \"MSc\",\n",
        "      \"major\": \"Artificial Intelligence\",\n",
        "      \"university\": \"Amirkabir University\",\n",
        "      \"graduation_year\": \"2022\"\n",
        "    }},\n",
        "    {{\n",
        "      \"degree\": \"BSc\",\n",
        "      \"major\": \"Computer Engineering\",\n",
        "      \"university\": \"University of Isfahan\",\n",
        "      \"graduation_year\": \"2018\"\n",
        "    }}\n",
        "  ],\n",
        "  \"experience\": [\n",
        "    {{\n",
        "      \"job_title\": \"Machine Learning Engineer\",\n",
        "      \"company\": \"Turing Technologies\",\n",
        "      \"duration\": \"2022‚ÄìPresent\",\n",
        "      \"start_date\": \"\",\n",
        "      \"end_date\": \"\",\n",
        "      \"main_responsibilities\": [\n",
        "        \"Developed machine learning models for predictive analysis\",\n",
        "        \"Worked on deploying models into production environments\"\n",
        "      ]\n",
        "    }},\n",
        "    {{\n",
        "      \"job_title\": \"Software Developer\",\n",
        "      \"company\": \"Parsian Co.\",\n",
        "      \"duration\": \"2018‚Äì2022\",\n",
        "      \"start_date\": \"\",\n",
        "      \"end_date\": \"\",\n",
        "      \"main_responsibilities\": [\n",
        "        \"Built and optimized web applications and APIs\"\n",
        "      ]\n",
        "    }}\n",
        "  ],\n",
        "  \"skills\": {{\n",
        "    \"technical\": [\"Python\", \"Scikit-learn\", \"TensorFlow\", \"Flask\", \"SQL\"],\n",
        "    \"soft\": []\n",
        "  }},\n",
        "  \"certificates\": [\n",
        "    \"TensorFlow Developer Certificate\",\n",
        "    \"Machine Learning by Stanford University (Coursera)\"\n",
        "  ]\n",
        "}}\n",
        "\n",
        "---\n",
        "\n",
        "### EXAMPLE 2\n",
        "RESUM√â:\n",
        "Full Name: Ali Rezaei\n",
        "Email: ali.rezaei@example.com\n",
        "Phone: +98 912 987 6543\n",
        "\n",
        "üéì Education:\n",
        "- PhD in Computer Science, Amirkabir University of Technology (2020)\n",
        "- MSc in Software Engineering, University of Tehran (2016)\n",
        "\n",
        "üíº Work Experience:\n",
        "- Senior Software Engineer at Aparat (2020-Present)\n",
        "  ‚Ä¢ Led the development of the video streaming platform\n",
        "  ‚Ä¢ Integrated advanced search algorithms for better content discovery\n",
        "- Software Developer at Tapsi (2016-2020)\n",
        "  ‚Ä¢ Developed backend services for mobile applications\n",
        "\n",
        "üîß Skills: Java, Spring Boot, Hibernate, SQL, AWS\n",
        "\n",
        "üìú Certifications:\n",
        "- AWS Certified Solutions Architect\n",
        "- Java Programming Certificate (Oracle)\n",
        "\n",
        "OUTPUT:\n",
        "{{\n",
        "  \"contact\": {{\n",
        "    \"name\": \"Ali Rezaei\",\n",
        "    \"email\": \"ali.rezaei@example.com\",\n",
        "    \"phone\": \"+98 912 987 6543\"\n",
        "  }} ,\n",
        "  \"education\": [\n",
        "    {{\n",
        "      \"degree\": \"PhD\",\n",
        "      \"major\": \"Computer Science\",\n",
        "      \"university\": \"Amirkabir University of Technology\",\n",
        "      \"graduation_year\": \"2020\"\n",
        "    }},\n",
        "    {{\n",
        "      \"degree\": \"MSc\",\n",
        "      \"major\": \"Software Engineering\",\n",
        "      \"university\": \"University of Tehran\",\n",
        "      \"graduation_year\": \"2016\"\n",
        "    }}\n",
        "  ],\n",
        "  \"experience\": [\n",
        "    {{\n",
        "      \"job_title\": \"Senior Software Engineer\",\n",
        "      \"company\": \"Aparat\",\n",
        "      \"duration\": \"2020‚ÄìPresent\",\n",
        "      \"start_date\": \"\",\n",
        "      \"end_date\": \"\",\n",
        "      \"main_responsibilities\": [\n",
        "        \"Led the development of the video streaming platform\",\n",
        "        \"Integrated advanced search algorithms for better content discovery\"\n",
        "      ]\n",
        "    }},\n",
        "    {{\n",
        "      \"job_title\": \"Software Developer\",\n",
        "      \"company\": \"Tapsi\",\n",
        "      \"duration\": \"2016‚Äì2020\",\n",
        "      \"start_date\": \"\",\n",
        "      \"end_date\": \"\",\n",
        "      \"main_responsibilities\": [\n",
        "        \"Developed backend services for mobile applications\"\n",
        "      ]\n",
        "    }}\n",
        "  ],\n",
        "  \"skills\": {{\n",
        "    \"technical\": [\"Java\", \"Spring Boot\", \"Hibernate\", \"SQL\", \"AWS\"],\n",
        "    \"soft\": []\n",
        "  }},\n",
        "  \"certificates\": [\n",
        "    \"AWS Certified Solutions Architect\",\n",
        "    \"Java Programming Certificate (Oracle)\"\n",
        "  ]\n",
        "}}\n",
        "\n",
        "---\n",
        "\n",
        "### NOW YOUR TASK\n",
        "Extract the same type of information from the r√©sum√© below.\n",
        "\n",
        "RESUM√â:\n",
        "{resume_text}\n",
        "\n",
        "OUTPUT REQUIREMENTS:\n",
        "- Return **JSON only**, no extra text.\n",
        "- Use the exact same key names and structure as in the examples.\n",
        "- If a field is missing, use empty strings \"\" or empty lists [].\n",
        "\"\"\"\n",
        "\n",
        "few_shot_ie_prompt = PromptTemplate(\n",
        "    template=few_shot_ie_template,\n",
        "    input_variables=[\"resume_text\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "7dcbc899",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dcbc899",
        "outputId": "bda51eb9-b827-485a-877a-fc6f708ad5c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are an information extraction system. Study the following examples and then extract data from the given r√©sum√©.\n",
            "\n",
            "---\n",
            "\n",
            "### EXAMPLE 1\n",
            "RESUM√â:\n",
            "Full Name: Reza Jafari  \n",
            "Email: reza.jafari@example.com  \n",
            "Phone: +98 912 654 3210  \n",
            "\n",
            "üéì Education:  \n",
            "- MSc in Artificial Intelligence, Amirkabir University (2022)  \n",
            "- BSc in Computer Engineering, University of Isfahan (2018)  \n",
            "\n",
            "üíº Work Experience:  \n",
            "- Machine Learning Engineer at Turing Technologies (2022-Present)  \n",
            "  ‚Ä¢ Developed machine learning models for predictive analysis  \n",
            "  ‚Ä¢ Worked on deploying models into production environments  \n",
            "- Software Developer at Parsian Co. (2018-2022)  \n",
            "  ‚Ä¢ Built and optimized web applications and APIs  \n",
            "\n",
            "üîß Skills: Python, Scikit-learn, TensorFlow, Flask, SQL  \n",
            "\n",
            "üìú Certifications:  \n",
            "- TensorFlow Developer Certificate  \n",
            "- Machine Learning by Stanford University (Coursera)  \n",
            "\n",
            "OUTPUT:\n",
            "{\n",
            "  \"contact\": {\n",
            "    \"name\": \"Reza Jafari\",\n",
            "    \"email\": \"reza.jafari@example.com\",\n",
            "    \"phone\": \"+98 912 654 3210\"\n",
            "  } ,\n",
            "  \"education\": [\n",
            "    {\n",
            "      \"degree\": \"MSc\",\n",
            "      \"major\": \"Artificial Intelligence\",\n",
            "      \"university\": \"Amirkabir University\",\n",
            "      \"graduation_year\": \"2022\"\n",
            "    },\n",
            "    {\n",
            "      \"degree\": \"BSc\",\n",
            "      \"major\": \"Computer Engineering\",\n",
            "      \"university\": \"University of Isfahan\",\n",
            "      \"graduation_year\": \"2018\"\n",
            "    }\n",
            "  ],\n",
            "  \"experience\": [\n",
            "    {\n",
            "      \"job_title\": \"Machine Learning Engineer\",\n",
            "      \"company\": \"Turing Technologies\",\n",
            "      \"duration\": \"2022‚ÄìPresent\",\n",
            "      \"start_date\": \"\",\n",
            "      \"end_date\": \"\",\n",
            "      \"main_responsibilities\": [\n",
            "        \"Developed machine learning models for predictive analysis\",\n",
            "        \"Worked on deploying models into production environments\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"job_title\": \"Software Developer\",\n",
            "      \"company\": \"Parsian Co.\",\n",
            "      \"duration\": \"2018‚Äì2022\",\n",
            "      \"start_date\": \"\",\n",
            "      \"end_date\": \"\",\n",
            "      \"main_responsibilities\": [\n",
            "        \"Built and optimized web applications and APIs\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"skills\": {\n",
            "    \"technical\": [\"Python\", \"Scikit-learn\", \"TensorFlow\", \"Flask\", \"SQL\"],\n",
            "    \"soft\": []\n",
            "  },\n",
            "  \"certificates\": [\n",
            "    \"TensorFlow Developer Certificate\",\n",
            "    \"Machine Learning by Stanford University (Coursera)\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "---\n",
            "\n",
            "### EXAMPLE 2\n",
            "RESUM√â:\n",
            "Full Name: Ali Rezaei  \n",
            "Email: ali.rezaei@example.com  \n",
            "Phone: +98 912 987 6543  \n",
            "\n",
            "üéì Education:  \n",
            "- PhD in Computer Science, Amirkabir University of Technology (2020)  \n",
            "- MSc in Software Engineering, University of Tehran (2016)  \n",
            "\n",
            "üíº Work Experience:  \n",
            "- Senior Software Engineer at Aparat (2020-Present)  \n",
            "  ‚Ä¢ Led the development of the video streaming platform  \n",
            "  ‚Ä¢ Integrated advanced search algorithms for better content discovery  \n",
            "- Software Developer at Tapsi (2016-2020)  \n",
            "  ‚Ä¢ Developed backend services for mobile applications  \n",
            "\n",
            "üîß Skills: Java, Spring Boot, Hibernate, SQL, AWS  \n",
            "\n",
            "üìú Certifications:  \n",
            "- AWS Certified Solutions Architect  \n",
            "- Java Programming Certificate (Oracle)  \n",
            "\n",
            "OUTPUT:\n",
            "{\n",
            "  \"contact\": {\n",
            "    \"name\": \"Ali Rezaei\",\n",
            "    \"email\": \"ali.rezaei@example.com\",\n",
            "    \"phone\": \"+98 912 987 6543\"\n",
            "  } ,\n",
            "  \"education\": [\n",
            "    {\n",
            "      \"degree\": \"PhD\",\n",
            "      \"major\": \"Computer Science\",\n",
            "      \"university\": \"Amirkabir University of Technology\",\n",
            "      \"graduation_year\": \"2020\"\n",
            "    },\n",
            "    {\n",
            "      \"degree\": \"MSc\",\n",
            "      \"major\": \"Software Engineering\",\n",
            "      \"university\": \"University of Tehran\",\n",
            "      \"graduation_year\": \"2016\"\n",
            "    }\n",
            "  ],\n",
            "  \"experience\": [\n",
            "    {\n",
            "      \"job_title\": \"Senior Software Engineer\",\n",
            "      \"company\": \"Aparat\",\n",
            "      \"duration\": \"2020‚ÄìPresent\",\n",
            "      \"start_date\": \"\",\n",
            "      \"end_date\": \"\",\n",
            "      \"main_responsibilities\": [\n",
            "        \"Led the development of the video streaming platform\",\n",
            "        \"Integrated advanced search algorithms for better content discovery\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"job_title\": \"Software Developer\",\n",
            "      \"company\": \"Tapsi\",\n",
            "      \"duration\": \"2016‚Äì2020\",\n",
            "      \"start_date\": \"\",\n",
            "      \"end_date\": \"\",\n",
            "      \"main_responsibilities\": [\n",
            "        \"Developed backend services for mobile applications\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"skills\": {\n",
            "    \"technical\": [\"Java\", \"Spring Boot\", \"Hibernate\", \"SQL\", \"AWS\"],\n",
            "    \"soft\": []\n",
            "  },\n",
            "  \"certificates\": [\n",
            "    \"AWS Certified Solutions Architect\",\n",
            "    \"Java Programming Certificate (Oracle)\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "---\n",
            "\n",
            "### NOW YOUR TASK\n",
            "Extract the same type of information from the r√©sum√© below.\n",
            "\n",
            "RESUM√â:\n",
            "Full Name: Sara Moradi  \n",
            "Email: sara.moradi@example.com  \n",
            "Phone: +98 912 123 4567  \n",
            "\n",
            "üéì Education:  \n",
            "- MSc in Data Science, University of Tehran (2021)  \n",
            "- BSc in Computer Engineering, Sharif University (2018)  \n",
            "\n",
            "üíº Work Experience:  \n",
            "- Data Scientist at Snapp (2022-Present)  \n",
            "  ‚Ä¢ Developed machine learning models for customer segmentation  \n",
            "  ‚Ä¢ Optimized recommendation systems for better user experience  \n",
            "- AI Researcher at Digikala (2019-2022)  \n",
            "  ‚Ä¢ Conducted NLP research for automatic product categorization  \n",
            "\n",
            "üîß Skills: Python, TensorFlow, PyTorch, SQL, Data Visualization  \n",
            "\n",
            "üìú Certifications:  \n",
            "- Google Data Analytics Professional Certificate  \n",
            "- Deep Learning Specialization by Andrew Ng (Coursera)  \n",
            "\n",
            "\n",
            "OUTPUT REQUIREMENTS:\n",
            "- Return **JSON only**, no extra text.\n",
            "- Use the exact same key names and structure as in the examples.\n",
            "- If a field is missing, use empty strings \"\" or empty lists [].\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_name1 = '/content/resume_sample1.txt'\n",
        "# file_name2 = '/content/resume_sample2.txt'\n",
        "\n",
        "with open(file_name1) as file:\n",
        "    resume_text = file.read()\n",
        "\n",
        "formatted_few_ie_prompt = few_shot_ie_prompt.format(resume_text=resume_text)\n",
        "\n",
        "print(formatted_few_ie_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "5ffd28d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ffd28d4",
        "outputId": "e93b306a-7eda-46bc-f6a0-fc563b5af834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The extracted information has been saved to 'few_shot_information_extractor.json'\n",
            "```json\n",
            "{\n",
            "  \"contact\": {\n",
            "    \"name\": \"Sara Moradi\",\n",
            "    \"email\": \"sara.moradi@example.com\",\n",
            "    \"phone\": \"+98 912 123 4567\"\n",
            "  } ,\n",
            "  \"education\": [\n",
            "    {\n",
            "      \"degree\": \"MSc\",\n",
            "      \"major\": \"Data Science\",\n",
            "      \"university\": \"University of Tehran\",\n",
            "      \"graduation_year\": \"2021\"\n",
            "    },\n",
            "    {\n",
            "      \"degree\": \"BSc\",\n",
            "      \"major\": \"Computer Engineering\",\n",
            "      \"university\": \"Sharif University\",\n",
            "      \"graduation_year\": \"2018\"\n",
            "    }\n",
            "  ],\n",
            "  \"experience\": [\n",
            "    {\n",
            "      \"job_title\": \"Data Scientist\",\n",
            "      \"company\": \"Snapp\",\n",
            "      \"duration\": \"2022‚ÄìPresent\",\n",
            "      \"start_date\": \"\",\n",
            "      \"end_date\": \"\",\n",
            "      \"main_responsibilities\": [\n",
            "        \"Developed machine learning models for customer segmentation\",\n",
            "        \"Optimized recommendation systems for better user experience\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"job_title\": \"AI Researcher\",\n",
            "      \"company\": \"Digikala\",\n",
            "      \"duration\": \"2019‚Äì2022\",\n",
            "      \"start_date\": \"\",\n",
            "      \"end_date\": \"\",\n",
            "      \"main_responsibilities\": [\n",
            "        \"Conducted NLP research for automatic product categorization\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"skills\": {\n",
            "    \"technical\": [\"Python\", \"TensorFlow\", \"PyTorch\", \"SQL\", \"Data Visualization\"],\n",
            "    \"soft\": []\n",
            "  },\n",
            "  \"certificates\": [\n",
            "    \"Google Data Analytics Professional Certificate\",\n",
            "    \"Deep Learning Specialization by Andrew Ng (Coursera)\"\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "response = llm.invoke(formatted_few_ie_prompt)\n",
        "\n",
        "response_data = response.content\n",
        "\n",
        "with open('few_shot_information_extractor.json', 'w') as json_file:\n",
        "    json.dump(response_data, json_file, indent=4)\n",
        "\n",
        "print(\"The extracted information has been saved to 'few_shot_information_extractor.json'\")\n",
        "print(response_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
