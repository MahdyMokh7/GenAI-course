{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Och4DOTAjrNN"
      },
      "source": [
        "# OLLAMA and Open Source LLMs\n",
        "This course provides a comprehensive overview of open-source large language models (LLMs) and tools like **OLLAMA, Open WebUI, and vLLM**, including theory and hands-on Python examples. We also explore how to integrate these tools with **LangChain** for building real-world applications and discuss how open models stack up on popular benchmarks.\n",
        "\n",
        "## 1. Introduction to Open Source LLMs\n",
        "\n",
        "### What are Open Source LLMs?\n",
        "Open-source LLMs are **large language models** whose architectures and/or weights are **publicly available** for anyone to run or fine-tune. Unlike proprietary models (e.g., OpenAI’s **GPT-4**, which are only accessible via paid APIs), open-source LLMs can be **downloaded and run** on local hardware or private servers.\n",
        "\n",
        "#### Examples of Open Source LLMs:\n",
        "- **Meta’s LLaMA family**\n",
        "- **Falcon 180B**\n",
        "- **Mistral 7B**\n",
        "- And many more...\n",
        "\n",
        "These models allow developers to **self-host AI capabilities** without relying on an external service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwcz0dGTjrx2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zojjEydjt0g"
      },
      "source": [
        "### Why use Open Source instead of proprietary models?\n",
        "There are several key advantages to using **open-source LLMs** over closed APIs:\n",
        "\n",
        "| Advantage                | Description |\n",
        "|-------------------------|-------------|\n",
        "| **Data Privacy & Control** | Open models can run locally or on-premise, ensuring **no sensitive data** is sent to third-party servers. In contrast, APIs **transmit prompts and outputs** to external providers, raising **privacy concerns**. |\n",
        "| **Customization & Freedom** | Full access to model parameters allows developers to **fine-tune** and modify the model as needed. You are **not locked** into vendor updates or API constraints. |\n",
        "| **Cost Efficiency & Offline Access** | Open-source models are **free to use** after download. The only costs are **hardware and electricity**—no recurring API fees. Running models offline also **reduces latency** and eliminates reliance on internet/cloud services. |\n",
        "| **Community and Innovation** | The **open-source ecosystem** fosters **rapid innovation**, with researchers and developers contributing improvements, optimizations, and fine-tuned variants. |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdMjmLWdjwJQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ourc4bz2j57y"
      },
      "source": [
        "### Key Players in Open-Source LLMs\n",
        "In recent years, many **tools and frameworks** have emerged to support open LLM development and usage. Some key players include:\n",
        "\n",
        "#### **1. OLLAMA**\n",
        "An **open-source platform** for running LLMs locally with ease.\n",
        "- Bundles **model files and dependencies** for easy deployment.\n",
        "- Allows users to **create, run, and share** LLMs locally.\n",
        "- Designed for **ease of use**, even for those without deep ML expertise.\n",
        "\n",
        "#### **2. Open WebUI**\n",
        "A **self-hosted web interface** for interacting with LLMs offline.\n",
        "- Provides a **GUI** to chat with models, manage them, and run pipelines like **RAG**.\n",
        "- Supports multiple backend engines (including **OLLAMA** and other APIs).\n",
        "\n",
        "#### **3. vLLM**\n",
        "A **high-performance inference engine** optimized for LLMs.\n",
        "- Developed at **Berkeley**, focusing on **speed and efficiency**.\n",
        "- Implements **continuous batching** and improved **memory management** for faster text generation.\n",
        "- Enables serving LLMs with **low latency** on modern hardware.\n",
        "\n",
        "#### **4. Additional Tools & Frameworks**\n",
        "- **Hugging Face Transformers** → For downloading and fine-tuning models.\n",
        "- **text-generation-webui** → Another web UI for managing local LLMs.\n",
        "- **LangChain** → For building AI-powered applications (covered later).\n",
        "- **Hugging Face Hub** → A repository for pre-trained and fine-tuned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpYtCeYLj6mS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKbi_Rg_kRL-"
      },
      "source": [
        "## OLLAMA: Running Open-Source LLMs Locally\n",
        "\n",
        "### What is OLLAMA and How Does It Work?\n",
        "OLLAMA is an **open-source project** (written largely in **Go**) that simplifies running LLMs on your own machine. At its core, OLLAMA creates an **isolated environment** for each model you install, containing everything needed to run that model (**model weights, configuration files, and necessary dependencies**).\n",
        "\n",
        "This means:\n",
        "- No need to manually manage **Python environments** or **libraries** for each model.\n",
        "- OLLAMA acts as both a **model manager** and **runtime**.\n",
        "- Avoids **software conflicts** between models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5CC5qLQkRug"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HNjEwHwkVf0"
      },
      "source": [
        "### How Models Work in OLLAMA\n",
        "- Models are packaged using a **Modelfile** (similar to a **Dockerfile**).\n",
        "- A **self-contained package** is downloaded from OLLAMA’s library.\n",
        "- Runs as a **background service** (persists model weights in RAM/VRAM for fast responses).\n",
        "- Supports **CPU and GPU execution**:\n",
        "  - On **Mac** → Uses **M1/M2 GPU** (Metal Performance Shaders).\n",
        "  - On **Linux/Windows** → Supports **NVIDIA CUDA** for acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HaRP_MnkWCj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVTISCxTkc5Y"
      },
      "source": [
        "## Installation & Setup\n",
        "\n",
        "### 1. Install OLLAMA\n",
        "OLLAMA supports **macOS (>= 11 Big Sur)**, **Linux**, and has a preview for **Windows**.\n",
        "\n",
        "For **macOS**, install via **Homebrew**:\n",
        "```bash\n",
        "brew install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is87UsZCkpxJ"
      },
      "outputs": [],
      "source": [
        "ollama serve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3C1cZtyksOu"
      },
      "outputs": [],
      "source": [
        "ollama pull <model_name>\n",
        "# ollama pull llama2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9LyFgfNkdcK"
      },
      "outputs": [],
      "source": [
        "ollama run llama2:7b \"What is the capital of France?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmkIfnjUk0o4"
      },
      "outputs": [],
      "source": [
        "ollama run llama2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGyfK9SCk1rj"
      },
      "source": [
        "## OLLAMA Features and Workflow\n",
        "\n",
        "OLLAMA provides a range of features that make it powerful for **local LLM development**:\n",
        "\n",
        "### Key Features of OLLAMA\n",
        "- **Model Version Management**:  \n",
        "  - Supports multiple versions of a model.  \n",
        "  - Allows switching or rolling back versions – useful for testing new models while keeping a stable version.  \n",
        "\n",
        "- **CLI and GUI Support**:  \n",
        "  - Primarily **CLI-driven** for power users.  \n",
        "  - Works with **third-party GUIs** like **Open WebUI** for a visual chat interface.  \n",
        "\n",
        "- **Multi-Platform Support**:  \n",
        "  - Available on **Mac, Linux, and Windows** (via WSL).  \n",
        "\n",
        "- **Built-in Models Library**:  \n",
        "  - Hosts **many popular models** ready to pull.  \n",
        "  - Includes **conversation models** (e.g., LLaMA-2-Chat, Mistral Instruct).  \n",
        "  - **Coding models** (e.g., Code Llama, StarCoder).  \n",
        "  - Optimized in **GGML/gguf format** for efficient CPU/GPU usage.  \n",
        "\n",
        "- **Isolated Environment**:  \n",
        "  - Each model’s **dependencies** (tokenizers, etc.) are handled internally.  \n",
        "  - Prevents **dependency conflicts** by containing models separately.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mqrHlVslHK4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6gGnyXWlLVe"
      },
      "source": [
        "\n",
        "## Comparison with Other Open-Source Frameworks\n",
        "\n",
        "### OLLAMA vs. Hugging Face Transformers  \n",
        "| Feature                | OLLAMA | Hugging Face Transformers |\n",
        "|------------------------|--------|--------------------------|\n",
        "| **Ease of Use**        | ✅ One-line pull & run | ❌ Requires manual setup |\n",
        "| **Model Management**   | ✅ Automatic version control | ❌ Requires manual downloads |\n",
        "| **Python Environment** | ✅ No need for virtual environments | ❌ Manual Python setup required |\n",
        "| **Fine-tuning Support**| ❌ Limited fine-tuning capabilities | ✅ Full control for research |\n",
        "\n",
        "- **Hugging Face Transformers**: Requires manual **model downloads, Python environment setup, and code to load models**.  \n",
        "- **OLLAMA**: Automates most of this with **one-line pull & run** for quick deployment.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSOepZhzlL0c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaZO27H8lOWU"
      },
      "source": [
        "### OLLAMA vs. Other Local Runners (GPT4All, LM Studio, etc.)\n",
        "| Feature                  | OLLAMA | GPT4All / LM Studio |\n",
        "|-------------------------|--------|----------------------|\n",
        "| **Ease of Installation** | ✅ Simple CLI setup | ✅ GUI-based installers |\n",
        "| **Developer Experience** | ✅ Unified CLI & Modelfile | ❌ Varies by tool |\n",
        "| **Integration Support**  | ✅ Standard API for LangChain | ❌ Less seamless integration |\n",
        "| **Community & Updates**  | ✅ Active model file development | ✅ Active but fragmented |\n",
        "\n",
        "- **GPT4All / LM Studio**: Similar local inference tools.  \n",
        "- **OLLAMA’s Strength**:  \n",
        "  - **Smooth developer experience** (CLI & Modelfile concept).  \n",
        "  - **Better integration** with frameworks like **LangChain**.  \n",
        "  - **Backed by an active community** creating **modelfiles** for new models.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUvDJln7ldv2"
      },
      "source": [
        "## Integration with Other Tools\n",
        "OLLAMA’s **persistent service** and **standard API** make it easy to integrate with other tools.  \n",
        "- **Supports integration with LangChain** to build AI applications.  \n",
        "- **Seamless connectivity** compared to other local runners.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rylSDZn-lO0A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5BgBK-ImXD1"
      },
      "source": [
        "## 3. Exploring Open WebUI & vLLM\n",
        "\n",
        "In this section, we’ll explore two **complementary tools** in the **open-source LLM ecosystem**:  \n",
        "- **Open WebUI** → Provides a **user-friendly interface** for interacting with models.  \n",
        "- **vLLM** → Focuses on **optimizing model performance** for inference.  \n",
        "\n",
        "Each serves a different purpose – **one for accessibility, the other for efficiency** – and they can even be **used together**.\n",
        "\n",
        "---\n",
        "\n",
        "## Open WebUI – A User-Friendly Interface for LLMs\n",
        "\n",
        "### What is Open WebUI?\n",
        "Open WebUI (formerly **Ollama WebUI**) is an **extensible, self-hosted AI platform** that runs entirely **offline in your browser**.  \n",
        "It allows you to **load and interact** with LLMs through a **graphical interface**.\n",
        "\n",
        "### 🔹 Key Features of Open WebUI\n",
        "- **Chat Interface**  \n",
        "  - Works like **ChatGPT’s UI** – type a **prompt** and get a **reply**.\n",
        "  - Runs **locally** on your **machine**.\n",
        "  \n",
        "- **Supports Multiple LLM Backends**  \n",
        "  - Can use **OLLAMA** as a backend.  \n",
        "  - Supports **OpenAI-compatible APIs**, GPT4All, and more.  \n",
        "  - **Delegates AI work** to an engine instead of being a model itself.\n",
        "\n",
        "- **Advanced Workflows (RAG Pipeline)**  \n",
        "  - Supports **Retrieval-Augmented Generation (RAG)**.  \n",
        "  - Connects **vector databases** or **document stores** to models.  \n",
        "  - Enables **question-answering over documents** (e.g., load PDFs and ask questions).\n",
        "\n",
        "- **Runs Entirely Offline**  \n",
        "  - Ensures **privacy** – **no queries leave your machine**.  \n",
        "  - Ideal for **secure environments** where cloud access isn’t allowed.\n",
        "\n",
        "---\n",
        "\n",
        "## How to Use Open WebUI\n",
        "\n",
        "The typical way to run Open WebUI is via a **Docker container**.  \n",
        "This encapsulates the app and its dependencies for easy deployment.\n",
        "\n",
        "### 🚀 Steps to Run Open WebUI:\n",
        "1. **Ensure OLLAMA is installed** (optional but recommended).  \n",
        "2. **Run Open WebUI with Docker**:\n",
        "   ```bash\n",
        "   docker run -d -p 3000:3000 ghcr.io/open-webui/open-webui:latest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChmRERmTmXjF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdWBNH1bnHob"
      },
      "source": [
        "## Why is Open WebUI Useful?\n",
        "\n",
        "| Feature                  | Benefit                                      |\n",
        "|--------------------------|----------------------------------------------|\n",
        "| **Lowers the barrier to LLMs** | No need for CLI or Python scripts.      |\n",
        "| **Quick Prototyping**    | Test prompts before coding them into an app. |\n",
        "| **Great for teams**      | Deploy once, and teammates can connect via a browser. |\n",
        "| **Self-hosted & Private** | No external API calls – full control over data. |\n",
        "\n",
        "🔹 **Open WebUI** **“puts a face”** on your local LLM, making AI interaction easier.  \n",
        "🔹 **Works seamlessly with OLLAMA** – there's even a **one-step Docker container** that bundles them!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhz9OMPYnII0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuVEWeh2niVo"
      },
      "source": [
        "## vLLM – Optimized Inference Engine for LLMs\n",
        "\n",
        "let’s talk about **vLLM**, which addresses a different need: **making LLMs run faster and more efficiently**, especially for **serving applications**.\n",
        "\n",
        "### What is vLLM?\n",
        "**vLLM** is a **high-throughput, memory-efficient inference and serving engine** for LLMs.  \n",
        "It can be used **programmatically** or as a **server** to get **better performance** out of large models compared to naive implementations.\n",
        "\n",
        "- Originally a **research project** from **Sky Computing Lab at UC Berkeley**.  \n",
        "- Now an **open-source** project with **community contributions**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 How Does vLLM Improve Performance?\n",
        "\n",
        "vLLM introduces **several technical innovations and optimizations**:\n",
        "\n",
        "### 1️⃣ **PagedAttention**\n",
        "- A new **memory management** technique for handling the **attention key/value cache**.  \n",
        "- In LLM text generation, the model keeps an **internal state** (**KV cache**) that **grows** with each token.  \n",
        "- **vLLM’s PagedAttention**:\n",
        "  - Reduces **memory fragmentation and overhead**.  \n",
        "  - Allows handling **longer prompts** or **multiple concurrent prompts**.  \n",
        "  - Supports **very long context lengths** efficiently.  \n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ **Continuous Batching**\n",
        "- **Automatically batches incoming requests on the fly**.  \n",
        "- If multiple users or processes send prompts, **vLLM dynamically groups and runs them together**.  \n",
        "- **Why is this important?**\n",
        "  - Increases **throughput** in a server scenario.  \n",
        "  - **Better GPU utilization** – modern GPUs are **underutilized** by a single prompt.  \n",
        "  - Unlike **static batching**, **continuous batching** dynamically adjusts as new requests stream in.  \n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ **Optimized CUDA Kernels**\n",
        "- Uses **low-level optimizations** like **FlashAttention** and **CUDA graph execution**.  \n",
        "- Reduces **overhead per token step**.  \n",
        "- Supports **quantization modes** (INT8, INT4, GPTQ) to **run models faster** with lower precision.  \n",
        "\n",
        "---\n",
        "\n",
        "### 4️⃣ **Parallelism & Scalability**\n",
        "- **Supports Tensor Parallelism and Pipeline Parallelism**.  \n",
        "- Distributes a **single model across multiple GPUs**.  \n",
        "- **Why is this useful?**\n",
        "  - You can **serve large models** (e.g., **70B parameters**) using **multiple smaller GPUs**.  \n",
        "  - Example: Running a **70B model** on **two 24GB GPUs** by splitting model layers between them.  \n",
        "  - Efficient **multi-GPU serving** for high-performance applications.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5️⃣ **OpenAI-Compatible API**\n",
        "- **vLLM can launch an API server** that **mimics OpenAI’s API endpoints**.  \n",
        "- Running:\n",
        "  ```bash\n",
        "  vllm serve <model>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkPCD2xtnjB8"
      },
      "outputs": [],
      "source": [
        "# Install vLLM first: pip install vllm\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Initialize an LLM engine with a model (here we use a small OPT model for demo)\n",
        "llm = LLM(model=\"facebook/opt-125m\")  # This will download the model from Hugging Face if not present\n",
        "\n",
        "# Prepare a prompt and sampling parameters\n",
        "prompts = [\"The capital of France is\"]  # we can batch multiple prompts in the list\n",
        "params = SamplingParams(max_tokens=10, temperature=0.0)  # generate up to 10 tokens, deterministic output\n",
        "\n",
        "# Generate outputs\n",
        "outputs = llm.generate(prompts, params)\n",
        "print(\"Prompt:\", outputs[0].prompt)\n",
        "print(\"Completion:\", outputs[0].outputs[0].text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifmNtNOxnwFH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l630Seio3Rv"
      },
      "source": [
        "## 🔹 Use Cases for vLLM\n",
        "\n",
        "The **main use case** for **vLLM** is **deploying an AI service**.  \n",
        "If you want to build a **chatbot** or an **API** for an **LLM-powered feature**, you need **efficient model serving**.  \n",
        "**vLLM** excels by **maximizing throughput** and **minimizing latency per request**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Example Scenarios\n",
        "\n",
        "### 1️⃣ **Deploying a Private ChatGPT-Style Service**\n",
        "- A company deploys a **private ChatGPT** service using **Llama-2**.  \n",
        "- With **vLLM’s continuous batching**, if **5 employees** ask questions **at the same time**, their requests get **batched together**.  \n",
        "- The **GPU processes them in parallel**, achieving **higher total QPS (Questions Per Second)** than **sequential processing**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ **Handling Long Documents**\n",
        "- vLLM’s **memory optimizations** enable handling **longer inputs**.  \n",
        "- Example: **Feeding a 50-page report** into an LLM is computationally heavy.  \n",
        "- **vLLM manages memory efficiently**, making this more feasible or at least **failing gracefully** rather than crashing.\n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ **Multi-GPU Model Deployment**\n",
        "- **vLLM supports multi-GPU scaling**.  \n",
        "- If you have a **server with 4 GPUs**, vLLM can **split a 70B model** across them.  \n",
        "- This enables deploying **large models** like **Llama-70B** or **Falcon-180B** that wouldn’t fit on a **single GPU**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 **vLLM + Other Frameworks**\n",
        "vLLM is often used **alongside other tools** like **OLLAMA** or **Hugging Face**:\n",
        "\n",
        "| Use Case                   | Suggested Tool(s) |\n",
        "|---------------------------|------------------|\n",
        "| **Fine-tuning models**     | Hugging Face Transformers |\n",
        "| **Local development & testing** | OLLAMA |\n",
        "| **Optimized production serving** | vLLM |\n",
        "\n",
        "### 🌟 Example Workflow:\n",
        "1. **Fine-tune** a model using **Hugging Face Transformers**.  \n",
        "2. **Test locally** using **OLLAMA**.  \n",
        "3. **Deploy efficiently** in production with **vLLM**.  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Next Step: Connecting vLLM & OLLAMA with LangChain**\n",
        "Now that we have explored **OLLAMA (ease of use)** and **vLLM (optimized serving)**,  \n",
        "let’s see **how to connect these tools with LangChain** to build real-world **AI applications**! 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXJcBf6Wo4un"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA7nxi3JpmyG"
      },
      "source": [
        "## 5. Comparing Models with Benchmarks (10 mins)\n",
        "\n",
        "Finally, let’s discuss how **open-source LLMs** stack up against **each other** and **proprietary models**,  \n",
        "and how to **evaluate their performance** using **benchmarks**.\n",
        "\n",
        "We’ll introduce a few **popular benchmarks**:\n",
        "- **MMLU (Massive Multitask Language Understanding)**\n",
        "- **HELM (Holistic Evaluation of Language Models)**\n",
        "- **GLUE (General Language Understanding Evaluation)**  \n",
        "\n",
        "We’ll also touch on **how to benchmark your own model**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Popular LLM Benchmarks\n",
        "\n",
        "### 1️⃣ **MMLU – Testing General Knowledge & Problem-Solving**\n",
        "- **What is it?**  \n",
        "  - A benchmark with **16,000 multiple-choice questions** across **57 subjects** (math, history, law, medicine, etc.).\n",
        "  - Tests a model’s **world knowledge** and **reasoning ability**.\n",
        "  - Measures accuracy (e.g., A/B/C/D answers).\n",
        "\n",
        "- **Why is it challenging?**\n",
        "  - Covers **advanced topics**.\n",
        "  - Requires **factual recall and reasoning**.\n",
        "  - For context:\n",
        "    - **GPT-3** scored ~**43.9%** (random guess: **25%**).\n",
        "    - **Human experts** score ~**89.8%**.\n",
        "    - **GPT-4, Claude 3** reach ~**90%** (considered the ceiling due to flawed questions).\n",
        "  \n",
        "- **How do open models compare?**\n",
        "  - **LLaMA-2 70B** scores **in the 70s**.\n",
        "  - Smaller models (e.g., **LLaMA-2 7B**) might score **in the 50s**.\n",
        "  - If an **open model scores ~70%** and **GPT-4 scores ~86%**, that open model is **reasonably strong**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ **HELM – A Holistic Evaluation Framework**\n",
        "- **Developed by**: **Stanford University**  \n",
        "- **What does it measure?**\n",
        "  - Accuracy across different **tasks**.\n",
        "  - **Robustness** → Sensitivity to prompt phrasing.\n",
        "  - **Calibration** → Does it \"know when it doesn’t know?\"\n",
        "  - **Bias/Fairness** → Differences in outputs for different demographics.\n",
        "  - **Toxicity** → Measures potential harmful outputs.\n",
        "  - **Efficiency** → How computationally expensive is it?\n",
        "\n",
        "- **Why is HELM important?**\n",
        "  - Provides a **holistic picture** of a model’s **strengths and weaknesses**.\n",
        "  - Not just about **accuracy**, but **other crucial factors** like **bias and safety**.\n",
        "  - **Public leaderboard** → Helps compare **open vs. closed models** transparently.\n",
        "\n",
        "- **Example HELM insights:**\n",
        "  - **LLaMA-2 70B** performs **on par** with **some smaller proprietary models**.\n",
        "  - Among **open models**, **LLaMA-2 70B** leads in **overall scores**.\n",
        "  - If you need a **low-toxicity model**, HELM can help **guide your choice**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ **GLUE – General Language Understanding Evaluation**\n",
        "- **A classic NLP benchmark** (pre-LLM era, 2018-2019).\n",
        "- **Focuses on 9 NLP tasks**, including:\n",
        "  - **Sentiment Analysis** (Is a review positive/negative?)\n",
        "  - **Paraphrase Detection** (Are two sentences saying the same thing?)\n",
        "  - **Question Answering** (Extracting answers from text)\n",
        "  - **Textual Entailment** (Logical relationships between statements)\n",
        "\n",
        "- **Why is GLUE less relevant today?**\n",
        "  - **Fine-tuned** models (e.g., **BERT, RoBERTa**) were evaluated on GLUE.\n",
        "  - Modern **LLMs** operate in **zero-shot / few-shot modes** instead.\n",
        "  - However, GLUE tasks **still provide a sanity check** for **language understanding**.\n",
        "\n",
        "- **Successor: SuperGLUE**\n",
        "  - Introduced as a **harder version** because modern models **surpass human performance** on GLUE.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏆 Choosing the Right Benchmark\n",
        "| Benchmark | Measures | Best For |\n",
        "|-----------|---------|----------|\n",
        "| **MMLU**  | General knowledge, reasoning | Comparing factual recall across models |\n",
        "| **HELM**  | Accuracy, robustness, fairness, toxicity | Holistic model evaluation |\n",
        "| **GLUE**  | Core NLP abilities (sentiment, entailment) | Checking fundamental language understanding |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Next Steps: Benchmarking Your Own Model**\n",
        "- Use **MMLU** to test **knowledge retrieval**.\n",
        "- Use **HELM** to check for **robustness & bias**.\n",
        "- Use **GLUE tasks** when **fine-tuning** for specific NLP tasks.\n",
        "\n",
        "Now that we understand **model evaluation**, let’s move on to **practical applications** with **LangChain**! 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_K3CEVLpnZ8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIQsPruvqjj_"
      },
      "source": [
        "## 🔹 Open-Source vs. Commercial Models: Performance, Speed, Cost, and Ethics\n",
        "\n",
        "### 📊 Performance Benchmarks (MMLU, HELM, HumanEval, etc.)\n",
        "\n",
        "- **MMLU (General Knowledge & Reasoning)**\n",
        "  - GPT-4: **86.4%** (5-shot)\n",
        "  - LLaMA 2 70B: **68.9%** (Near GPT-3.5’s **70%**)\n",
        "  - LLaMA 3.2 90B: **86%** (Zero-shot CoT, nearly matches GPT-4)\n",
        "  - **Trend:** Open models **closing the gap** with proprietary models.\n",
        "\n",
        "- **HumanEval (Code Generation)**\n",
        "  - GPT-4: **67% pass@1**\n",
        "  - Base LLaMA 2: **~30%**\n",
        "  - Code Llama 34B (fine-tuned): **73-74%** (near GPT-4 level)\n",
        "  - **Takeaway:** **Fine-tuned open models** can **surpass GPT-3.5** in coding.\n",
        "\n",
        "- **HELM (Holistic Model Evaluation)**\n",
        "  - **GPT-4 tops overall rankings**, but LLaMA 3.1 **gets close (0.854 vs 0.864)**.\n",
        "  - **Open models improve with fine-tuning** (Vicuna 13B reached **90% ChatGPT quality**).\n",
        "  - Measures **bias, robustness, toxicity**, etc., beyond raw accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### 🏆 Key Insights on Open vs. Closed Models\n",
        "\n",
        "1. **Open models are rapidly improving**\n",
        "   - LLaMA 3.x, Falcon, and DeepSeek models are **approaching GPT-4** on many benchmarks.\n",
        "   - **Fine-tuning and specialization** help **narrow the gap**.\n",
        "\n",
        "2. **Domain-specific open models excel**\n",
        "   - **Medical LLMs, legal LLMs, and coding models** sometimes **outperform** general proprietary models in their niche.\n",
        "   - **Retrieval-augmented models (RAG)** allow smaller open models to **compete with closed models**.\n",
        "\n",
        "3. **Coding & Specialized Tasks**\n",
        "   - **GPT-4 still leads overall**, but **Code Llama 34B** surpasses **GPT-3.5** in code generation.\n",
        "   - **Fine-tuned open models** are becoming **more competitive**.\n",
        "\n",
        "4. **Ethics & Transparency**\n",
        "   - Open models offer **more control and transparency** but **require careful fine-tuning** to match the safety & alignment of commercial models.\n",
        "   - **Closed models excel in robustness & instruction-following** due to **extensive RLHF**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 The Future of Open Models\n",
        "\n",
        "- **Performance gap is shrinking** (from 30-40% behind GPT-4 to now **10-20% on key tasks**).\n",
        "- Open-source is catching up **faster than ever** with **smarter tuning and scaling**.\n",
        "- **Specialized open models** may **outperform** general-purpose closed models in specific use cases.\n",
        "\n",
        "🔹 **Conclusion:** While **GPT-4 remains the strongest overall**, **open-source models are now viable competitors**—and in some areas, they **surpass** closed models with fine-tuning and optimizations. The **era of open-source AI parity is near**. 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQP5lX9BqkI2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGWwsgrurQdJ"
      },
      "source": [
        "# 🔹 GPU VRAM Requirements for LLMs\n",
        "\n",
        "Developing and deploying **large language models (LLMs)** requires significant **VRAM**.  \n",
        "VRAM needs depend on **model size** and **use case** (Inference, Fine-tuning, Training).  \n",
        "\n",
        "---\n",
        "\n",
        "## 📊 VRAM Requirements by Model Size & Task\n",
        "\n",
        "| Model (Params) | Inference VRAM (FP16) | Fine-Tuning VRAM (FP16) | Training VRAM (Full) |\n",
        "|---------------|----------------------|----------------------|--------------------|\n",
        "| **3B**  | ~6 GB (FP16) / ~3 GB (Int8) | ~12–16 GB | ~20–24 GB (Single GPU) |\n",
        "| **7B (LLaMA-7B)** | ~14 GB (FP16) / ~7 GB (Int8) | ~28–30 GB | ~50–60 GB (Multi-GPU likely) |\n",
        "| **13B (LLaMA-13B)** | ~26 GB (FP16) / ~13 GB (Int8) | ~50–60 GB | ~100+ GB (Multi-GPU required) |\n",
        "| **30B (LLaMA-30B)** | ~60 GB (FP16) / ~30 GB (Int8) | ~120 GB | ~240+ GB (Cluster needed) |\n",
        "| **65B (LLaMA-65B / Llama2-70B)** | ~130 GB (FP16) / ~65 GB (Int8) / ~33 GB (4-bit) | ~250+ GB | ~500+ GB (Multi-node cluster) |\n",
        "| **90B (LLaMA 3.2 90B)** | ~180 GB (FP16) / ~90 GB (Int8) / ~45 GB (4-bit) | ~350+ GB | ~700+ GB (Multi-node cluster) |\n",
        "\n",
        "**Key Insights:**\n",
        "- **7B models** need ~**14 GB VRAM** for **inference**, **28–30 GB** for **fine-tuning**.\n",
        "- **30B+ models** **cannot** be fine-tuned on a **single consumer GPU**.\n",
        "- **Full training requires 2× to 4× model size in memory** (optimizer states & activations).\n",
        "- **Multi-GPU & quantization** are essential for large models.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏆 **Optimizing VRAM Usage**\n",
        "1. **Use Quantization for Inference**  \n",
        "   - **4-bit quantization** reduces **LLaMA-65B** inference from **130 GB → 33 GB**.  \n",
        "   - Makes **large models runnable on 48 GB GPUs or 2x 24 GB GPUs**.  \n",
        "\n",
        "2. **QLoRA for Fine-Tuning**  \n",
        "   - Quantizing base model to **4-bit**, training **small LoRA matrices**.  \n",
        "   - Enables **fine-tuning a 65B model on a single 48 GB GPU**!  \n",
        "\n",
        "3. **Gradient Checkpointing**  \n",
        "   - Saves VRAM but increases compute time (**trading memory for speed**).  \n",
        "\n",
        "4. **Multi-GPU Scaling**  \n",
        "   - **30B+ models require multiple GPUs**, often with **model parallelism**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 **Conclusion**\n",
        "- **Consumer GPUs (24–48 GB)** can handle **7B–13B models** for inference,  \n",
        "  but **30B+ models** require **multiple GPUs or quantization**.  \n",
        "- **Fine-tuning large models (65B+) on a single GPU is possible** with **QLoRA**.  \n",
        "- **Full training (scratch) requires massive VRAM (100s of GBs)** and **multi-node clusters**.\n",
        "\n",
        "By leveraging **quantization & memory-efficient training**, **open models can run on smaller hardware**! 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_g4wQUQrQ-U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDJtjLssr5UI"
      },
      "source": [
        "# 🔹 GPU Capabilities for LLMs\n",
        "\n",
        "## 📊 Key GPU Memory Limits for Running LLMs\n",
        "\n",
        "| GPU Setup | Max Model (FP16) | Max Model (4-bit Quantized) | Notes |\n",
        "|-----------|----------------|------------------------|-------|\n",
        "| **RTX 4090 (24GB)** | **13B** | **30B** | Can fine-tune 7B/13B (LoRA). 65B possible with **4-bit + CPU offload**. |\n",
        "| **Dual RTX 4090 (2 × 24GB)** | **30B+ (8-bit)** | **65B-90B (4-bit)** | Can run **LLaMA 2-70B in 4-bit**. Some fine-tuning possible. |\n",
        "| **RTX 3090 (24GB)** | **Similar to 4090** | **13B+ (8-bit), 30B (4-bit)** | Slightly lower efficiency vs. 4090. |\n",
        "| **RTX 4080 (16GB)** | **7B full**, **13B (8-bit)** | **30B (heavily quantized)** | Limited VRAM for 30B+. |\n",
        "| **4 × 24GB GPUs (96GB total)** | **70B+ (FP16 split)** | **90B+ (4-bit)** | High-end setup for large models. |\n",
        "| **8GB GPUs / Laptops** | **3B full, maybe 7B quantized** | **13B (CPU offload, slow)** | Not practical for large models. |\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 **Practical GPU Capabilities**\n",
        "### **Inference**\n",
        "- **Single 4090 (24GB)** → **Up to 13B full precision**, **30B with 4-bit quantization**.\n",
        "- **Dual 4090s (2×24GB)** → **65B-90B models possible (4-bit quantized)**.\n",
        "- **Smaller GPUs (≤ 8GB)** → Can run **3B-7B** models with **CPU offload** (slow).\n",
        "\n",
        "### **Fine-Tuning**\n",
        "- **LoRA fine-tuning**:\n",
        "  - **4090 (24GB)** can fine-tune **7B, 13B models**.\n",
        "  - **65B fine-tune possible on a 48GB GPU** (QLoRA).\n",
        "  - **Dual GPUs allow light 70B fine-tuning**.\n",
        "\n",
        "### **Full Model Training**\n",
        "- **Training large models (30B, 70B) from scratch** is infeasible on consumer GPUs.\n",
        "- **Meta’s LLaMA 3.2 90B** → **885k GPU-hours on H100s** for pretraining.\n",
        "- **Community relies on fine-tuning pre-trained models**, rather than full training.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **Key Takeaways**\n",
        "✅ **RTX 4090 (24GB)** is ideal for **up to 13B models (full precision)** and **30B quantized**.  \n",
        "✅ **Dual 4090s (48GB total)** can **run 70B models (4-bit)** and allow **some fine-tuning**.  \n",
        "✅ **Consumer GPUs are increasingly capable of handling large models** with **quantization & offload**.  \n",
        "✅ **Training from scratch (30B+) requires clusters**, but **fine-tuning is feasible on local setups**.  \n",
        "✅ **As tooling improves, the need for massive GPU clusters is decreasing for practical LLM use.**\n",
        "\n",
        "The open-source **LLM movement empowers users** to **run powerful AI models locally**—a shift from needing cloud-based APIs! 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ctdUcUF1r5-P",
        "outputId": "39995fed-bfd7-4413-fcb7-8b0fa180a5b3"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid character '→' (U+2192) (<ipython-input-3-9bc97d3bf8a0>, line 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-9bc97d3bf8a0>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Quantization is a **technique to reduce the memory footprint** and **compute requirements** of Large Language Models (LLMs) by storing weights in lower precision formats (e.g., **FP16 → INT8, INT4**).\u001b[0m\n\u001b[0m                                                                                                                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '→' (U+2192)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtlkdjzIslZ-"
      },
      "source": [
        "# 🔹 Understanding Quantization in LLMs\n",
        "\n",
        "## 📌 What is Quantization?\n",
        "Quantization is a **technique to reduce the memory footprint** and **compute requirements** of Large Language Models (LLMs) by storing weights in lower precision formats (e.g., **FP16 → INT8, INT4**).  \n",
        "This allows models to run on **smaller GPUs** while sacrificing some accuracy.\n",
        "\n",
        "### 🔹 Why Use Quantization?\n",
        "- **Reduces VRAM usage** → Enables running **larger models** on consumer GPUs.\n",
        "- **Speeds up inference** → Lower precision operations are **faster** on modern hardware.\n",
        "- **Tradeoff: Slight loss of accuracy** due to **precision reduction**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Quantization Levels & Impact\n",
        "\n",
        "| Precision | VRAM Usage | Speed | Accuracy Impact |\n",
        "|-----------|-----------|-------|----------------|\n",
        "| **FP32 (Full Precision)** | **High** | **Slowest** | **Best accuracy** |\n",
        "| **FP16 (Half Precision)** | **50% less than FP32** | **Faster** | **Minimal accuracy loss** |\n",
        "| **INT8 (8-bit Quantization)** | **~4× memory reduction** | **Much faster** | **Slight accuracy drop** |\n",
        "| **INT4 (4-bit Quantization)** | **~8× memory reduction** | **Fastest** | **Noticeable accuracy drop** |\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 **Quantization as a Source of Noise**\n",
        "Quantization introduces **rounding errors** in model weights, similar to **adding noise** to the parameters.  \n",
        "### 🔹 How does quantization affect model performance?\n",
        "1. **Loss of Precision**  \n",
        "   - LLMs rely on **precise weight values** for **text generation**.  \n",
        "   - Quantization forces weight values into **fewer discrete levels**, reducing expressiveness.  \n",
        "   \n",
        "2. **Propagation of Rounding Errors**  \n",
        "   - During **multi-layer computations**, **small quantization errors** **accumulate**, affecting outputs.\n",
        "   - This **increases hallucinations** or **makes models more sensitive to prompt variations**.\n",
        "\n",
        "3. **Noise-Like Effects on Output**  \n",
        "   - Low-bit quantized models **sometimes output different results** for **the same input**.  \n",
        "   - Similar to how **random noise** affects signals, **quantization-induced noise** impacts model **stability**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏆 **How to Mitigate Quantization Effects**\n",
        "1. **Use Mixed Precision**  \n",
        "   - Keep **key layers in higher precision** (e.g., FP16) and quantize less critical ones.\n",
        "   \n",
        "2. **Apply Fine-Tuning After Quantization**  \n",
        "   - **Post-quantization fine-tuning (PTQ)** helps recover lost accuracy.\n",
        "   \n",
        "3. **Use Adaptive Quantization**  \n",
        "   - Some techniques adjust quantization based on **layer sensitivity**.\n",
        "   \n",
        "4. **Quantization-Aware Training (QAT)**  \n",
        "   - Train models **knowing they will be quantized**, making them **more robust**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Final Thoughts**\n",
        "- **Quantization enables large models to run on smaller GPUs** but introduces **noise-like errors**.\n",
        "- **Lower precision increases computational speed but affects accuracy**.\n",
        "- **Optimized quantization strategies (e.g., QLoRA, mixed precision) help maintain performance**.\n",
        "- **Considering quantization as a noise factor helps in understanding and mitigating its effects**.\n",
        "\n",
        "🔹 **Conclusion:** While **quantization is a powerful tool for AI deployment**, careful **balancing of speed, memory, and accuracy** is key to achieving optimal performance. 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqMhIj1JskBt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
