{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8682f382",
   "metadata": {},
   "source": [
    "# 🧠 Retrieval-Augmented Generation (RAG): The Complete Guide\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 1. What is RAG?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** is an architecture that combines **retrieval** (fetching relevant external information) with **generation** (LLM responses).\n",
    "Instead of relying only on what a language model already knows (its training data), RAG **retrieves real documents** from a knowledge base and gives them as context to the LLM.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "```\n",
    "User Query → Retriever → Knowledge Base → Context → Generator (LLM)\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> You ask: “What’s the latest AI policy from OpenAI?”\n",
    "\n",
    "* The retriever searches a knowledge base or database (like Weaviate, Pinecone, or Elasticsearch) for relevant documents.\n",
    "* The generator (like GPT-4 or Claude) uses those documents to answer accurately.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 2. Why use RAG? (When to use it)\n",
    "\n",
    "| Scenario                      | Why RAG helps                                                            |\n",
    "| ----------------------------- | ------------------------------------------------------------------------ |\n",
    "| Company or domain knowledge   | LLMs don’t know your private data — RAG brings that in.                  |\n",
    "| Keeping info updated          | Retrieval allows the model to “see” recent documents without retraining. |\n",
    "| Reducing hallucinations       | The model grounds its answers in factual data.                           |\n",
    "| Compliance and explainability | You can show *where* the answer came from.                               |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 3. Core Components\n",
    "\n",
    "| Component               | Description                                         |\n",
    "| ----------------------- | --------------------------------------------------- |\n",
    "| **1️⃣ Data ingestion**  | Collect and preprocess your data (PDFs, docs, etc.) |\n",
    "| **2️⃣ Embedding model** | Converts text into vectors (numerical meaning)      |\n",
    "| **3️⃣ Vector database** | Stores embeddings and retrieves similar ones        |\n",
    "| **4️⃣ Retriever**       | Searches the database for relevant context          |\n",
    "| **5️⃣ LLM generator**   | Uses that context to produce an answer              |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 4. RAG Architecture Diagram\n",
    "\n",
    "```\n",
    "User Query\n",
    "     ↓\n",
    "[Embed Query] —→ [Vector Database (Weaviate)]\n",
    "     ↓                    ↑\n",
    "[Retrieve Contexts] ← [Embedded Documents]\n",
    "     ↓\n",
    "[LLM (e.g., GPT-4) generates grounded answer]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 5. Setting Up a Simple RAG System (with Weaviate + OpenAI)\n",
    "\n",
    "Let’s build a small RAG pipeline with **Python**, using:\n",
    "\n",
    "* **Weaviate** for vector storage\n",
    "* **OpenAI Embeddings** for encoding\n",
    "* **OpenAI GPT model** for answer generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea25523",
   "metadata": {},
   "source": [
    "### 🧩 Step 1: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install weaviate-client openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c7d98",
   "metadata": {},
   "source": [
    "### 🧩 Step 2: Import libraries and connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733027ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from openai import OpenAI\n",
    "\n",
    "# Connect to Weaviate (use your own instance)\n",
    "client = weaviate.Client(\"https://your-weaviate-instance.weaviate.network\")\n",
    "\n",
    "# Connect to OpenAI\n",
    "openai_client = OpenAI(api_key=\"YOUR_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aaff2",
   "metadata": {},
   "source": [
    "### 🧩 Step 3: Define a schema (like a table in SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"class\": \"Document\",\n",
    "            \"description\": \"A collection of text documents\",\n",
    "            \"vectorizer\": \"none\",  # We’ll add vectors manually\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"name\": \"content\",\n",
    "                    \"dataType\": [\"text\"]\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"source\",\n",
    "                    \"dataType\": [\"string\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "client.schema.delete_all()\n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1314f",
   "metadata": {},
   "source": [
    "### 🧩 Step 4: Add documents and embed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    {\"content\": \"Weaviate is an open-source vector database for AI applications.\", \"source\": \"weaviate-doc\"},\n",
    "    {\"content\": \"RAG combines retrieval with LLMs for accurate responses.\", \"source\": \"rag-paper\"},\n",
    "]\n",
    "\n",
    "for doc in docs:\n",
    "    embedding = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=doc[\"content\"]\n",
    "    ).data[0].embedding\n",
    "\n",
    "    client.data_object.create(\n",
    "        data_object=doc,\n",
    "        class_name=\"Document\",\n",
    "        vector=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549bf78",
   "metadata": {},
   "source": [
    "### 🧩 Step 5: Search the database with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9bc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Weaviate used for?\"\n",
    "\n",
    "query_vector = openai_client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=query\n",
    ").data[0].embedding\n",
    "\n",
    "results = client.query.get(\"Document\", [\"content\", \"source\"])\\\n",
    "    .with_near_vector({\"vector\": query_vector})\\\n",
    "    .with_limit(2)\\\n",
    "    .do()\n",
    "\n",
    "context_text = \"\\n\".join([d[\"content\"] for d in results[\"data\"][\"Get\"][\"Document\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336830fa",
   "metadata": {},
   "source": [
    "### 🧩 Step 6: Ask the LLM with retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Answer the question using the context below.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af859f76",
   "metadata": {},
   "source": [
    "✅ **Output Example:**\n",
    "\n",
    "> Weaviate is an open-source vector database used for semantic and AI-powered search applications.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 6. Real-World Use Cases\n",
    "\n",
    "| Domain                | Application                                                     |\n",
    "| --------------------- | --------------------------------------------------------------- |\n",
    "| **Customer support**  | Chatbots that answer from internal documentation.               |\n",
    "| **Healthcare**        | Retrieve patient data or medical research for decision support. |\n",
    "| **Legal**             | Summarize and answer from case documents.                       |\n",
    "| **Education**         | Personalized tutors grounded in textbooks.                      |\n",
    "| **Enterprise search** | Semantic search across company files.                           |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 7. Extensions & Enhancements\n",
    "\n",
    "* **Hybrid Search** → combine keyword + semantic retrieval.\n",
    "* **Chunking** → split long documents into smaller passages.\n",
    "* **Metadata filters** → retrieve by topic, date, or tags.\n",
    "* **Caching** → store recent query results for speed.\n",
    "* **Streaming** → stream LLM output for chat-like UIs.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 8. Best Practices\n",
    "\n",
    "✅ Always chunk documents (e.g., 500 tokens per chunk).\n",
    "✅ Use the same embedding model for both documents and queries.\n",
    "✅ Add metadata for filtering.\n",
    "✅ Store sources so the LLM can cite them.\n",
    "✅ Use RAG before fine-tuning — cheaper and more flexible.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 9. Common RAG Stack\n",
    "\n",
    "| Layer            | Example Tools                     |\n",
    "| ---------------- | --------------------------------- |\n",
    "| **Data storage** | Weaviate, Pinecone, Qdrant, FAISS |\n",
    "| **Embedding**    | OpenAI, Hugging Face, Cohere      |\n",
    "| **LLM**          | GPT-4, Claude, Llama 3            |\n",
    "| **Frameworks**   | LangChain, LlamaIndex, Haystack   |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 10. When NOT to Use RAG\n",
    "\n",
    "| Scenario                         | Alternative                        |\n",
    "| -------------------------------- | ---------------------------------- |\n",
    "| Need creative / generative tasks | Use LLM directly                   |\n",
    "| Private model training possible  | Fine-tune instead                  |\n",
    "| Real-time data changing rapidly  | Use live APIs or dynamic retrieval |\n",
    "| Structured data (SQL-style)      | Use querying systems, not RAG      |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
