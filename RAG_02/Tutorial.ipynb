{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8682f382",
   "metadata": {},
   "source": [
    "# ðŸ§  Retrieval-Augmented Generation (RAG): The Complete Guide\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ 1. What is RAG?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** is an architecture that combines **retrieval** (fetching relevant external information) with **generation** (LLM responses).\n",
    "Instead of relying only on what a language model already knows (its training data), RAG **retrieves real documents** from a knowledge base and gives them as context to the LLM.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "```\n",
    "User Query â†’ Retriever â†’ Knowledge Base â†’ Context â†’ Generator (LLM)\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> You ask: â€œWhatâ€™s the latest AI policy from OpenAI?â€\n",
    "\n",
    "* The retriever searches a knowledge base or database (like Weaviate, Pinecone, or Elasticsearch) for relevant documents.\n",
    "* The generator (like GPT-4 or Claude) uses those documents to answer accurately.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© 2. Why use RAG? (When to use it)\n",
    "\n",
    "| Scenario                      | Why RAG helps                                                            |\n",
    "| ----------------------------- | ------------------------------------------------------------------------ |\n",
    "| Company or domain knowledge   | LLMs donâ€™t know your private data â€” RAG brings that in.                  |\n",
    "| Keeping info updated          | Retrieval allows the model to â€œseeâ€ recent documents without retraining. |\n",
    "| Reducing hallucinations       | The model grounds its answers in factual data.                           |\n",
    "| Compliance and explainability | You can show *where* the answer came from.                               |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  3. Core Components\n",
    "\n",
    "| Component               | Description                                         |\n",
    "| ----------------------- | --------------------------------------------------- |\n",
    "| **1ï¸âƒ£ Data ingestion**  | Collect and preprocess your data (PDFs, docs, etc.) |\n",
    "| **2ï¸âƒ£ Embedding model** | Converts text into vectors (numerical meaning)      |\n",
    "| **3ï¸âƒ£ Vector database** | Stores embeddings and retrieves similar ones        |\n",
    "| **4ï¸âƒ£ Retriever**       | Searches the database for relevant context          |\n",
    "| **5ï¸âƒ£ LLM generator**   | Uses that context to produce an answer              |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© 4. RAG Architecture Diagram\n",
    "\n",
    "```\n",
    "User Query\n",
    "     â†“\n",
    "[Embed Query] â€”â†’ [Vector Database (Weaviate)]\n",
    "     â†“                    â†‘\n",
    "[Retrieve Contexts] â† [Embedded Documents]\n",
    "     â†“\n",
    "[LLM (e.g., GPT-4) generates grounded answer]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ 5. Setting Up a Simple RAG System (with Weaviate + OpenAI)\n",
    "\n",
    "Letâ€™s build a small RAG pipeline with **Python**, using:\n",
    "\n",
    "* **Weaviate** for vector storage\n",
    "* **OpenAI Embeddings** for encoding\n",
    "* **OpenAI GPT model** for answer generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea25523",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 1: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install weaviate-client openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c7d98",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 2: Import libraries and connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733027ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from openai import OpenAI\n",
    "\n",
    "# Connect to Weaviate (use your own instance)\n",
    "client = weaviate.Client(\"https://your-weaviate-instance.weaviate.network\")\n",
    "\n",
    "# Connect to OpenAI\n",
    "openai_client = OpenAI(api_key=\"YOUR_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aaff2",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 3: Define a schema (like a table in SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"class\": \"Document\",\n",
    "            \"description\": \"A collection of text documents\",\n",
    "            \"vectorizer\": \"none\",  # Weâ€™ll add vectors manually\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"name\": \"content\",\n",
    "                    \"dataType\": [\"text\"]\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"source\",\n",
    "                    \"dataType\": [\"string\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "client.schema.delete_all()\n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1314f",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 4: Add documents and embed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    {\"content\": \"Weaviate is an open-source vector database for AI applications.\", \"source\": \"weaviate-doc\"},\n",
    "    {\"content\": \"RAG combines retrieval with LLMs for accurate responses.\", \"source\": \"rag-paper\"},\n",
    "]\n",
    "\n",
    "for doc in docs:\n",
    "    embedding = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=doc[\"content\"]\n",
    "    ).data[0].embedding\n",
    "\n",
    "    client.data_object.create(\n",
    "        data_object=doc,\n",
    "        class_name=\"Document\",\n",
    "        vector=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549bf78",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 5: Search the database with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9bc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Weaviate used for?\"\n",
    "\n",
    "query_vector = openai_client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=query\n",
    ").data[0].embedding\n",
    "\n",
    "results = client.query.get(\"Document\", [\"content\", \"source\"])\\\n",
    "    .with_near_vector({\"vector\": query_vector})\\\n",
    "    .with_limit(2)\\\n",
    "    .do()\n",
    "\n",
    "context_text = \"\\n\".join([d[\"content\"] for d in results[\"data\"][\"Get\"][\"Document\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336830fa",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 6: Ask the LLM with retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Answer the question using the context below.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af859f76",
   "metadata": {},
   "source": [
    "âœ… **Output Example:**\n",
    "\n",
    "> Weaviate is an open-source vector database used for semantic and AI-powered search applications.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ 6. Real-World Use Cases\n",
    "\n",
    "| Domain                | Application                                                     |\n",
    "| --------------------- | --------------------------------------------------------------- |\n",
    "| **Customer support**  | Chatbots that answer from internal documentation.               |\n",
    "| **Healthcare**        | Retrieve patient data or medical research for decision support. |\n",
    "| **Legal**             | Summarize and answer from case documents.                       |\n",
    "| **Education**         | Personalized tutors grounded in textbooks.                      |\n",
    "| **Enterprise search** | Semantic search across company files.                           |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© 7. Extensions & Enhancements\n",
    "\n",
    "* **Hybrid Search** â†’ combine keyword + semantic retrieval.\n",
    "* **Chunking** â†’ split long documents into smaller passages.\n",
    "* **Metadata filters** â†’ retrieve by topic, date, or tags.\n",
    "* **Caching** â†’ store recent query results for speed.\n",
    "* **Streaming** â†’ stream LLM output for chat-like UIs.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ 8. Best Practices\n",
    "\n",
    "âœ… Always chunk documents (e.g., 500 tokens per chunk).\n",
    "âœ… Use the same embedding model for both documents and queries.\n",
    "âœ… Add metadata for filtering.\n",
    "âœ… Store sources so the LLM can cite them.\n",
    "âœ… Use RAG before fine-tuning â€” cheaper and more flexible.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® 9. Common RAG Stack\n",
    "\n",
    "| Layer            | Example Tools                     |\n",
    "| ---------------- | --------------------------------- |\n",
    "| **Data storage** | Weaviate, Pinecone, Qdrant, FAISS |\n",
    "| **Embedding**    | OpenAI, Hugging Face, Cohere      |\n",
    "| **LLM**          | GPT-4, Claude, Llama 3            |\n",
    "| **Frameworks**   | LangChain, LlamaIndex, Haystack   |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  10. When NOT to Use RAG\n",
    "\n",
    "| Scenario                         | Alternative                        |\n",
    "| -------------------------------- | ---------------------------------- |\n",
    "| Need creative / generative tasks | Use LLM directly                   |\n",
    "| Private model training possible  | Fine-tune instead                  |\n",
    "| Real-time data changing rapidly  | Use live APIs or dynamic retrieval |\n",
    "| Structured data (SQL-style)      | Use querying systems, not RAG      |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
