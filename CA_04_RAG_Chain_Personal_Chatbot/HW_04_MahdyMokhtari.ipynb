{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6825cdf7",
   "metadata": {},
   "source": [
    "# ğŸ¤– RAG-Powered Personal Chatbot  \n",
    "### *A Mini Project by Mehdy Mokhtari*  \n",
    "\n",
    "This notebook demonstrates the creation of a **Retrieval-Augmented Generation (RAG)** chatbot capable of answering questions based on **personal or private documents**.  \n",
    "The project leverages **LangChain**, **FAISS**, **OpenAI LLMs**, and **Python** to build an intelligent assistant that retrieves relevant information and generates contextual responses.\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“˜ Project Highlights:**\n",
    "- ğŸ§  Uses **RAG architecture** (Retriever + Generator) for accurate, data-grounded answers.  \n",
    "- ğŸ” Employs **FAISS** for efficient vector search and similarity matching.  \n",
    "- ğŸ“„ Supports **custom document ingestion** (PDFs, text files, etc.).  \n",
    "- âš™ï¸ Built with **LangChain** for modular and scalable AI pipelines.  \n",
    "- ğŸ’¬ Optional **Streamlit interface** for interactive chatting.\n",
    "\n",
    "---\n",
    "\n",
    "> Developed by *Mehdy Mokhtari* | 2025  \n",
    "> â€œEmpowering data-driven conversations through Retrieval-Augmented Generation.â€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7f1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Replace 'your_directory_path' with the path to the directory you want to delete\n",
    "# shutil.rmtree(os.path.join(\".\", \"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b6ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain faiss-cpu huggingface_hub langchain_community hazm sentence-transformers openai\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184ed43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hazm\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40963ca1",
   "metadata": {},
   "source": [
    "## 1. DataLoader â€” PDF Downloader, Text Extractor & Caching\n",
    "\n",
    "This script automates the process of:\n",
    "- **Downloading** PDF files from URLs listed in `links.txt` using the `requests` library.  \n",
    "- **Extracting text** from each PDF with `PyPDFLoader` (from `langchain_community.document_loaders`).  \n",
    "- **Caching extracted text** per file in the `cached_extracted_data/` folder as JSONL files, so future runs skip re-downloading and re-processing.  \n",
    "- **Auto-refreshing cache** only if a PDF changes or is newly added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0ace65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder already contains files â€” using cached PDFs if available.\n",
      "Loaded from cache: .\\cached_extracted_data\\doc_1.jsonl\n",
      "Loaded from cache: .\\cached_extracted_data\\doc_2.jsonl\n",
      "Loaded from cache: .\\cached_extracted_data\\doc_3.jsonl\n",
      "Loaded from cache: .\\cached_extracted_data\\doc_4.jsonl\n",
      "Loaded from cache: .\\cached_extracted_data\\doc_5.jsonl\n",
      "Loaded from cache: .\\cached_extracted_data\\doc_6.jsonl\n",
      "Loaded from cache: .\\cached_extracted_data\\doc_7.jsonl\n",
      "Loaded from cache: .\\cached_extracted_data\\doc_8.jsonl\n",
      "Total documents loaded: 23\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "TXT_PATH = \"links.txt\"\n",
    "RAW_DIR = os.path.join(\".\", \"data\")\n",
    "CACHE_DIR = os.path.join(\".\", \"cached_extracted_data\")\n",
    "\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Read links\n",
    "with open(TXT_PATH, encoding=\"utf-8\") as f:\n",
    "    urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "docs = []\n",
    "\n",
    "# 2) Download PDFs only if data folder is empty\n",
    "if not os.listdir(RAW_DIR):\n",
    "    print(\"Data folder is empty â€” downloading all PDFs.\")\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "            \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    for idx, url in enumerate(urls, start=1):\n",
    "        file_name = f\"doc_{idx}.pdf\"  # predictable names to avoid .aspx confusion\n",
    "        pdf_path = os.path.join(RAW_DIR, file_name)\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, stream=True, allow_redirects=True, timeout=60)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            # Only save if server actually returns a PDF\n",
    "            ctype = resp.headers.get(\"Content-Type\", \"\")\n",
    "            if \"application/pdf\" in ctype.lower():\n",
    "                with open(pdf_path, \"wb\") as f:\n",
    "                    for chunk in resp.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                print(f\"Downloaded {pdf_path}\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Skipped: URL did not return a PDF (Content-Type={ctype}) -> {url}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"âŒ Error downloading {url}: {e}\")\n",
    "else:\n",
    "    print(\"Data folder already contains files â€” using cached PDFs if available.\")\n",
    "\n",
    "def cache_path_for(pdf_filename: str) -> str:\n",
    "    \"\"\"Return the per-PDF cache file path (JSONL) based on the PDF file name.\"\"\"\n",
    "    base, _ = os.path.splitext(pdf_filename)\n",
    "    return os.path.join(CACHE_DIR, f\"{base}.jsonl\")\n",
    "\n",
    "def write_cache(jsonl_path: str, page_docs):\n",
    "    \"\"\"Write a list of LangChain Documents (or dicts) to JSONL cache.\"\"\"\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for d in page_docs:\n",
    "            if Document and isinstance(d, Document):\n",
    "                record = {\"page_content\": d.page_content, \"metadata\": d.metadata or {}}\n",
    "            else:\n",
    "                # assume dict-like\n",
    "                record = {\n",
    "                    \"page_content\": d.get(\"page_content\", \"\"),\n",
    "                    \"metadata\": d.get(\"metadata\", {}),\n",
    "                }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def read_cache(jsonl_path: str):\n",
    "    \"\"\"Read JSONL cache back into a list of Documents (if available) or dicts.\"\"\"\n",
    "    loaded = []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            if Document:\n",
    "                loaded.append(Document(page_content=obj[\"page_content\"], metadata=obj.get(\"metadata\", {})))\n",
    "            else:\n",
    "                loaded.append({\"page_content\": obj[\"page_content\"], \"metadata\": obj.get(\"metadata\", {})})\n",
    "    return loaded\n",
    "\n",
    "# 3) Load all PDFs from RAW_DIR, caching extracted text per file\n",
    "for file_name in sorted(os.listdir(RAW_DIR)):\n",
    "    if not file_name.lower().endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    pdf_path = os.path.join(RAW_DIR, file_name)\n",
    "    jsonl_cache = cache_path_for(file_name)\n",
    "\n",
    "    # If cache exists and is up-to-date compared to the PDF, load from cache\n",
    "    use_cache = False\n",
    "    if os.path.exists(jsonl_cache):\n",
    "        try:\n",
    "            pdf_mtime = os.path.getmtime(pdf_path)\n",
    "            cache_mtime = os.path.getmtime(jsonl_cache)\n",
    "            use_cache = cache_mtime >= pdf_mtime\n",
    "        except OSError:\n",
    "            use_cache = False\n",
    "\n",
    "    if use_cache:\n",
    "        page_docs = read_cache(jsonl_cache)\n",
    "        docs.extend(page_docs)\n",
    "        print(f\"Loaded from cache: {jsonl_cache}\")\n",
    "        continue\n",
    "\n",
    "    # Otherwise, parse PDF, then write cache\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        page_docs = loader.load()  # list of per-page Documents\n",
    "        docs.extend(page_docs)\n",
    "        write_cache(jsonl_cache, page_docs)\n",
    "        print(f\"Parsed & cached: {pdf_path} -> {jsonl_cache}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error parsing {pdf_path}: {e}\")\n",
    "\n",
    "print(f\"Total documents loaded: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48acfc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'MicrosoftÂ® Word LTSC', 'creator': 'MicrosoftÂ® Word LTSC', 'creationdate': '2024-05-14T12:02:42+03:30', 'moddate': '2024-05-14T12:03:24+03:30', 'author': 'Pishgam Rayaneh', 'source': '.\\\\data\\\\doc_3.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='3 \\n \\nÙ…Ø§Ø¯Ù‡6-Ø¯Ø±Ø®ØµØ¯Øµ Ø¹Ù‡Ù… Ø¨Ù‡Ø±Ù…Ù†Ù‡ÛŒ Ù…ØªÙ‚Ø§Ø¶   Ùˆ Ø§ÙØ±Ø§Ø¯ ØªØ­Øª ØªÚ©ÙÙ„ Ø¢Ù†Ø§Ù† Ø¯Ø± Ø²Ù…Ø§Ø§Ù† Ø§Ø®Ø§Ø° ØªØ³Ø§Ù‡ÙŠÙ„Ø§Øª Ø§Ø² Ø²Ù…Ø§ÙŠÙ† Ù…Ø³Ø§Ú©Ø¯Ù†  ÙŠØ§Ø§ ÙˆØ§Ø­Ø§Ù‡ \\nÙ…Ø³Ú©Ø¯Ù† ØŒ Ø¨Ø§Ù†Ú©Ù‡Ø§ÛŒ Ø¹Ø§Ù…Ù„ Ø¨Ù‡ Ø³Ø§Ù…Ø§Ù†Ù‡ Ø¬Ø§Ù…Ø¹ Ø·Ø±Ø­Ù‡Ø§ÛŒ Ø­Ù…Ø§ÙŠØª  ÙˆØ²Ø§Ø±Øª Ø±Ø§  Ùˆ Ø´Ù‡Ø±Ø³Ø§Ø²ÛŒ Ø¨Ù‡ Ù†Ø´Ø§Ù†  Temgt.mrud.ir Ù…Ø±Ø§Ø¬Ø¹Ø§Ù‡ Ùˆ\\nØ¯Ø± ØµÙØ­Ù‡ Ø§ØµÙ„  Ø¯Ø± Ù‚Ø³Ù…Øª \"Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø§Ù„Ú©ÙŠØª\" Ø¨Ø§ Ø¯Ø±Ø¬ Ø´Ù…Ø§Ø±Ù‡ Ú©Ø¯Ù…Ù„ÛŒ Ù…ØªÙ‚Ø§Ø¶ÛŒØ§Ù† Ù†Ø³Ø¨Øª Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØ¶Ø¹ÙŠØª Ø§ÙŠØ´Ø§Ù† Ø§Ù‚Ù‡Ø§Ù… Ù†Ù…Ø§ÙŠÙ†Ù‡. \\nÙ…Ø§Ø¯Ù‡ 7- Ø­Ù‡Ø§Ù‚Ù„ Ù…Ø³ØªÙ†Ù‡Ø§Øª Ùˆ Ù…Ù‡Ø§Ø±Ú© Ø°ÙŠÙ„ Ø¬Ù‡Øª Ù¾Ø±Ø¯Ø§Ø®Øª ØªØ³Ù‡ÙŠÙ„Ø§Øª ÙŠØ§Ø¯Ø´Ù‡  Ø§Ø² Ù…ØªÙ‚Ø§Ø¶ÙŠØ§Ù† Ø§Ø®Ø° Ø´Ø¯Ø¯: \\n1- ØªØ³Ù‡ÙŠÙ„Ø§Øª ÙˆØ¯ÙŠØ¹Ù‡: Ù„Ø²ÙˆÙ… Ø§Ø±Ø§Ø¦Ù‡ Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯ Ø§Ø¬Ø§Ø±  Ø¨Ø§ Ø¯Ø±Ø¬ Ú©Ù‡ Ø±Ù‡Ú¯ÙŠØ±ÛŒ Ø¯Ø± Ø¢Ù†. \\n2-  ØªØ³Ù‡ÙŠÙ„Ø§Øª  Ø®Ø±ÙŠÙ‡:  Ù„Ø²ÙˆÙ… Ø§Ø±Ø§Ø¦Ù‡ Ù…Ø¨Ø§ÙŠØ¹Ù‡Ù†Ø§Ù…Ù‡  Ø¨Ø§ Ø¯Ø±Ø¬ Ú©Ù‡ Ø±Ù‡Ú¯ÙŠØ±ÛŒ  Ø¯Ø± Ø¢Ù† Ùˆ Ø§Ø¹Ø·Ø§Ø¡ ØªØ³Ù‡ÙŠÙ„Ø§Øª  Ù‡Ù…Ø²Ù…Ø§Ù† Ø¨Ø§ Ø§Ù†ØªÙ‚Ø§Ù„ Ø³Ù†Ù‡ Ø±Ø³Ù…  Ù…Ù„Ú© Ø¨Ù‡ \\nÙ†Ø§Ù… ØªØ³Ù‡ÙŠÙ„Ø§ØªÚ¯ÙŠØ±Ù†Ù‡ .  \\n3-  ØªØ³Ù‡ÙŠÙ„Ø§Øª Ø³Ø§Ø®Øª: Ù„Ø²ÙˆÙ… Ø§Ø±Ø§Ø¦Ù‡ Ù¾Ø±ÙˆØ§Ù†Ù‡ Ø³Ø§Ø®Øª Ù…Ø¹ØªØ¨Ø± Ùˆ Ø§Ø±Ø§Ø¦Ù‡ Ø§Ø³Ù†Ø§Ø¯ Ù…Ø¨ÙŠÙ† Ù…Ø§Ù„Ú©ÙŠØª Ø¯Ø± Ù…Ø¯Ø§Ø±Ø¯ÛŒ Ú©Ù‡ ØªØ³Ù‡ÙŠÙ„Ø§ØªÚ¯ÙŠØ±Ù†Ù‡ ØŒ Ø®Ø¯Ø¯ØŒ Ù…Ø§Ù„Ø§Ú© \\nØ¹Ø±ØµÙ‡ Ø§Ø³Øª Ùˆ Ø§Ø±Ø§Ø¦Ù‡ Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯ Ù…Ø´Ø§Ø±Ú©Øª Ø¯Ø± Ø³Ø§Ø§Ø®Øª Ø¯Ø± Ù…Ø§Ø¯Ø§Ø±Ø¯ÛŒ Ú©Ø§Ù‡ ØªØ³Ø§Ù‡ÙŠÙ„Ø§ØªÚ¯ÙŠØ±Ù†Ù‡ ØŒ Ù…Ø§Ù„Ø§Ú© Ø¹Ø±ØµØ§Ù‡ Ù†Ù… Ø¨Ø§Ø´Ø§Ù‡ Ùˆ Ø¯Ø± ØµØ§Ø¯Ø±Øª \\nØ§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬   Ù…ØªÙ‚Ø§Ø¶  Ùˆ ØªÚ©Ù…ÙŠÙ„ Ù…Ù‡Ø§Ø±Ú© Ùˆ Ø§Ø®Ø° Ù…Ù‡Ø§Ø±Ú© Ù…ØªÙ†Ø§Ø³Ø¨ Ø§Ø² ÙˆÛŒ Ú©Ù„ Ù…Ø¨Ù„Øº ØªØ³Ù‡ÙŠÙ„Ø§Øª Ø¯Ø± ÙŠÚ© Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ù…ØªÙ‚Ø§Ø¶  Ù¾Ø±Ø¯Ø§Ø®Ø§Øª \\nØ®Ø¯Ø§Ù‡Ù‡ Ø´Ù‡. \\nØªØ¨ØµØ±Ù‡:  Ø¨Ù‡ Ù…Ù†Ø¸Ø¯Ø± Ø¬Ù„Ø¯Ú¯ÙŠØ±ÛŒ Ø§Ø² Ø¯Ø±ÙŠØ§ÙØª Ù…Ø¬Ù‡Ø¯ ØªØ³Ù‡ÙŠÙ„Ø§Øª Ù…Ø¯Ø¶Ø¯Ø¹ Ø§ÙŠØ§Ù† Ø¨Ù†Ø§Ù‡ Ø§Ø² Ù‚Ø§Ø§Ù†Ø¯Ù† ØªØ¯Ø³Ø§Ø· Ù…ØªÙ‚Ø§Ø¶Ø§ÙŠØ§Ù† Ø§Ø² Ø³Ø§Ø§ÙŠØ± Ø¨Ø§Ù†Ú©Ù‡Ø§Ø§ Ùˆ \\nÙ…Ø¤Ø³Ø³Ø§Øª Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ ØŒ Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª Ø¯Ø± Ù…Ù‚Ø·Ø¹ Ø§Ù†Ø¹Ù‚Ø§Ø¯ Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯ Ùˆ  Ù‚Ø¨Ù„ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø®Øª ØªØ³Ø§Ù‡ÙŠÙ„Ø§Øª Ø¹Ø§Ù„Ø§Ùˆ  Ø¨Ø§Ø± Ø§Ø³Ø§ØªØ¹Ù„Ø§Ù…Ù‡Ø§ÛŒ  Ø§Ø®Ø§Ø° Ø´Ø§Ù‡  \\nØ¨Ø±Ø§Ø³Ø§Ø³ Ù…Ø§Ø¯   (5) Ú©Ù‡ Ø¯Ø± Ø²Ù…Ø§Ù† Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ø±Ø®Ø¯Ø§Ø³Øª ÙˆØªØ´Ú©ÙŠÙ„ Ù¾Ø±ÙˆÙ†Ù‡  Ø§Ø®Ø° Ù…  Ø´Ø¯Ø¯ Ùˆ ØµØ±Ù Ù†Ø¸Ø± Ø§Ø² Ø¯Ø§Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ø¨Ø¯Ø¯Ù† Ø¢Ù†ØŒ Ø§Ø³Ø§ØªØ¹Ù„Ø§Ù… \\nÙ…Ø¬Ù‡Ø¯ Ø§Ø² Ø³Ø§Ù…Ø§Ù†Ù‡ Ø³Ù…Ø§Øª Ø¨Ù‡ Ù…Ù†Ø¸Ø¯Ø± Ú©Ù†ØªØ±Ù„ Ø³Ø¯Ø§Ø¨Ù‚ Ù…ØªÙ‚Ø§Ø¶  Ùˆ Ø§Ø­Ø±Ø§Ø² Ø¹Ù‡Ù… Ø§Ø³ØªÙØ§Ø¯  Ø§Ø² ØªØ³Ù‡ÙŠÙ„Ø§Øª Ù‚Ø¨Ù„  (Ø¨Ø±Ø±Ø³  ØªØ³Ù‡ÙŠÙ„Ø§Øª Ù…Ø³Ø§Ú©Ù† Ùˆ \\nØªØ³Ù‡ÙŠÙ„Ø§Øª Ø¬Ø¹Ø§Ù„Ù‡ ÙØ¹Ø§Ù„ Ù…ØªÙ‚Ø§Ø¶ÙŠØ§Ù†) Ø¯Ø± Ú†Ø§Ø±Ú†Ø¯Ø¨ Ù…Ø¯Ø¶Ø¯Ø¹ Ø§ÙŠÙ† Ø¨Ù†Ù‡ Ù‚Ø§Ù†Ø¯Ù†  ØµØ¯Ø±Øª Ù¾Ø°ÙŠØ±Ø¯. \\nÙ…Ø§Ø¯Ù‡  8-  Ù…Ø¯Ø§Ø±Ø¯ÛŒ Ú©Ù‡ Ø¯Ø± Ø§ÙŠÙ† Ø¯Ø³ØªØ¯Ø±Ø§Ù„Ø¹Ù…Ù„ Ù¾ÙŠØ´ Ø¨ÙŠÙ†  Ù†Ø´Ù‡  Ø§Ø³Øª ØªØ§Ø¨Ø¹ Ù‚Ø¯Ø§Ù†ÙŠÙ† Ùˆ  Ù…Ù‚Ø±Ø±Ø§Øª Ø¬Ø§Ø±ÛŒ Ú©Ø´Ø¯Ø± Ø§Ø² Ø¬Ù…Ù„Ù‡   Ù‚Ø§Ù†Ø¯Ù† Ù¾Ø¯Ù„  Ùˆ Ø¨Ø§Ù†Ú©   \\nÚ©Ø´Ø¯Ø±ØŒ Ù‚Ø§Ù†Ø¯Ù† Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ø§Ù†Ú©  Ø¨Ù‡ÙˆÙ† Ø±Ø¨Ø§ (Ø¨Ù‡Ø± ) Ùˆ Ø¢ÙŠÙŠÙ† Ù†Ø§Ù…Ù‡Ù‡Ø§ÛŒ Ø§Ø¬Ø±Ø§ÙŠ  Ùˆ Ø¯Ø³ØªØ¯Ø±Ø§Ù„Ø¹Ù…Ù„ Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨ Ø· Ø¨Ø§ Ø¢Ù†ØŒ Ø³Ø§ÙŠØ± Ù‚Ø¯Ø§Ù†ÙŠÙ† Ùˆ Ù…Ù‚Ø±Ø±Ø§Øª  \\nØ°ÛŒ Ø±Ø¨Ø· Ùˆ Ù‡Ù…Ú†Ù†ÙŠÙ† Ù…ØµØ¯Ø¨Ø§Øª Ø´Ø¯Ø±Ø§ÛŒ Ù¾Ø¯Ù„ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø± Ùˆ Ø¯Ø³ØªØ¯Ø±Ø§Øª Ùˆ Ø¨Ø®Ø´ Ù†Ø§Ù…Ù‡Ù‡Ø§ÛŒ Ø¨Ø§Ù†Ú© Ù…Ø±Ú©Ø²ÛŒ Ù…  Ø¨Ø§Ø´Ù‡.  \\nÂ«Ù„ Ø§Ø¬Ø±Ø§ÛŒØªØªÛŒ ØªØ¨ØµØªØªØ±Ù‡ Ù‡ØªØªØ§ÛŒ (Û±) Ùˆ (Û²) Ù…ØªØªØ§Ø¯Ù‡  (69) Ù‚ØªØªØ§Ù†ÙˆÙ† Ø­Ù…Ø§ÛŒØªØªØª Ø§Ø² Ø®ØªØªØ§Ù†ÙˆØ§Ø¯Ù‡ Ùˆ Ø¬ØªØªÙˆØ§Ù†ÛŒ Ø¬Ù…Ø¹ÛŒØªØªØª ØªØ³ØªØªÙ‡ÛŒÙ„Ø§Øª \\nÙ‚Ø±Ø¶Ø§Ù„Ø­Ø³Ù†Ù‡ ÙˆØ¯ÛŒØ¹Ù‡ ÛŒØ§ Ø³Ø§Ø®Øª ÛŒØ§ Ø®Ø±ÛŒØ¯ Ù…Ø³Ú©Ù†Â»  Ù…Ø´ØªÙ…Ù„ Ø¨Ø§Ø±8  Ù…Ø§Ø§Ø¯Ø¯Ø±  Ø§ÙˆÙ„Ø§ÙŠÙ† Ø¬Ù„Ø³Ø§Ù‡  Ù…Ø§Ø¯Ø±Ø®  28/01/1403  Ú©Ù…ÙŠØ³Ø§ÙŠØ¯Ù†\\nØ¹Ù…Ù„ÙŠØ§Øª Ù¾Ø¯Ù„  Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ Ø¨Ø§Ù†Ú© Ù…Ø±Ú©Ø²ÛŒ Ø¨Ù‡ ØªØµØ¯ÙŠØ¨ Ø±Ø³ÙŠÙ‡ Ùˆ Ø§Ø² ØªØ§Ø±ÙŠØ® Ø§Ø¨Ù„Ø§Øº Ù„Ø§Ø²Ù…Ø§Ù„Ø§Ø¬Ø±Ø§ Ù… Ø¨Ø§Ø´Ù‡.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docs[0]\n",
    "docs[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7280e0a",
   "metadata": {},
   "source": [
    "## 2. FAISS Vector Database & Retriever\n",
    "\n",
    "This step processes the documents stored in memory (`docs`) by:\n",
    "- **Normalizing Persian text** using the `hazm` library (fixing Arabic/Persian character differences and removing unnecessary characters).\n",
    "- **Chunking** long documents into smaller segments using `RecursiveCharacterTextSplitter`, with Persian-aware separators.\n",
    "- **Generating embeddings** using `HuggingFaceEmbeddings` (or optionally OpenAI embeddings).\n",
    "- **Building a FAISS vector database** to store document chunks and make them searchable.\n",
    "- **Persisting the FAISS index** to disk for future use.\n",
    "- **Creating a retriever** using the FAISS vector store to fetch relevant document chunks based on queries.\n",
    "\n",
    "This setup enables efficient retrieval of contextually relevant document parts to power the RAG chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b028c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --force-reinstall --no-cache-dir -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029ab9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY: c:\\Users\\NoteBook\\Desktop\\programing\\Gen AI\\GenAI-course\\CA_04_RAG_Chain_Personal_Chatbot\\.venv\\Scripts\\python.exe\n",
      "VER: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "SITE: ['c:\\\\Users\\\\NoteBook\\\\Desktop\\\\programing\\\\Gen AI\\\\GenAI-course\\\\CA_04_RAG_Chain_Personal_Chatbot\\\\.venv', 'c:\\\\Users\\\\NoteBook\\\\Desktop\\\\programing\\\\Gen AI\\\\GenAI-course\\\\CA_04_RAG_Chain_Personal_Chatbot\\\\.venv\\\\Lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys, site, platform\n",
    "print(\"PY:\", sys.executable)\n",
    "print(\"VER:\", sys.version)\n",
    "print(\"SITE:\", site.getsitepackages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cce3a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "nltk: 3.9.2\n"
     ]
    }
   ],
   "source": [
    "import sys, nltk\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"nltk:\", nltk.__version__)\n",
    "# print(\"hazm:\", hazm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "620e7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NoteBook\\Desktop\\programing\\Gen AI\\GenAI-course\\CA_04_RAG_Chain_Personal_Chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#  FAISS - VecDB\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import hazm\n",
    "\n",
    "INDEX_DIR = os.path.join(\".\", \"vectorstores\", \"faiss_simple\")\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "RETRIEVER_FILE = os.path.join(INDEX_DIR, \"retriever.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9217d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_safe_chunk 480\n",
      "_safe_overlap 72\n"
     ]
    }
   ],
   "source": [
    "# Chunking for Persian - token aware (not char)\n",
    "\n",
    "MODEL_NAME = \"xmanii/maux-gte-persian\"\n",
    "\n",
    "_tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "def token_len(s: str) -> int:\n",
    "    return len(_tok.encode(s, add_special_tokens=False))\n",
    "\n",
    "# derive safe chunk/overlap from tokenizer max length to avoid truncation\n",
    "_max_len = getattr(_tok, \"model_max_length\", 512) or 512\n",
    "_safe_chunk = min(480, _max_len - 32)   \n",
    "_safe_overlap = max(64, int(_safe_chunk * 0.15))\n",
    "\n",
    "print(\"_safe_chunk\", _safe_chunk)\n",
    "print(\"_safe_overlap\", _safe_overlap)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\", \"\\n\", \"Û”\", \".\", \"ØŸ\", \"!\", \"Ø›\", \":\", \"ØŒ\", \"Ù¬\", \" \", \"\"\n",
    "    ],\n",
    "    chunk_size=_safe_chunk,\n",
    "    chunk_overlap=_safe_overlap,\n",
    "    length_function=token_len,  # token-aware chunks â†’ less truncation, better retrieval\n",
    "    is_separator_regex=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b375550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoteBook\\AppData\\Local\\Temp\\ipykernel_9948\\3518595760.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "You try to use a model that was created with version 3.2.0, however, your version is 2.6.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Embedding\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=MODEL_NAME,\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    "    model_kwargs={\"trust_remote_code\": True}\n",
    ")\n",
    "\n",
    "# Option 1 â€“ balanced (Persian + English)\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"heydariAI/persian-embeddings\",\n",
    "#     encode_kwargs={\"normalize_embeddings\": True}\n",
    "# )\n",
    "\n",
    "# Option 2 â€“ optimized for Persian\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"xmanii/maux-gte-persian\",\n",
    "#     encode_kwargs={\"normalize_embeddings\": True}\n",
    "# )\n",
    "\n",
    "# Option 3 â€“ best performance (Hakim)\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"MCINext/Hakim\",\n",
    "#     encode_kwargs={\"normalize_embeddings\": True}\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f39419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Total documents: 23\n",
      "ğŸ”¢ Total tokens: 9,557\n",
      "ğŸ“Š Average tokens per document: 415.5\n",
      "ğŸ“ˆ Min: 72, Max: 861\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# use the same tokenizer you already loaded\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(_tok.encode(text, add_special_tokens=False))\n",
    "\n",
    "token_counts = [count_tokens(d.page_content) for d in docs]\n",
    "avg_tokens = mean(token_counts)\n",
    "total_tokens = sum(token_counts)\n",
    "\n",
    "print(f\"ğŸ“„ Total documents: {len(docs)}\")\n",
    "print(f\"ğŸ”¢ Total tokens: {total_tokens:,}\")\n",
    "print(f\"ğŸ“Š Average tokens per document: {avg_tokens:.1f}\")\n",
    "print(f\"ğŸ“ˆ Min: {min(token_counts)}, Max: {max(token_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602f6f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new FAISS index...\n",
      "âœ… Creating new retriever...\n",
      "âœ… FAISS index built at: .\\vectorstores\\faiss_simple\n",
      "âœ… Chunks indexed: 38\n",
      "âœ… Retriever ready: retriever.get_relevant_documents(<query>)\n"
     ]
    }
   ],
   "source": [
    "_normalizer = hazm.Normalizer(persian_numbers=True, persian_style=True)\n",
    "\n",
    "def normalize_fa(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    t = _normalizer.normalize(text)\n",
    "    t = t.replace(\"\\ufeff\", \"\") \n",
    "    t = t.replace(\"ÙŠ\", \"ÛŒ\").replace(\"Ùƒ\", \"Ú©\")\n",
    "    t = t.replace(\"Ù€\", \"\").replace(\"\\u200d\", \"\") \n",
    "    return \" \".join(t.split()) \n",
    "\n",
    "def normalize_docs(docs: List[Document]) -> List[Document]:\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        pc = normalize_fa(d.page_content)\n",
    "        out.append(Document(page_content=pc, metadata=dict(d.metadata or {})))\n",
    "    return out\n",
    "\n",
    "normalized_docs = normalize_docs(docs)\n",
    "\n",
    "# Chunk long docs\n",
    "chunked_docs = text_splitter.split_documents(normalized_docs)\n",
    "\n",
    "# Check if FAISS index exists, if so, load it; otherwise, create it\n",
    "if os.path.exists(os.path.join(INDEX_DIR, \"faiss.index\")):\n",
    "    print(\"Loading existing FAISS index...\")\n",
    "    vectorstore = FAISS.load_local(INDEX_DIR, embeddings)\n",
    "else:\n",
    "    print(\"Creating new FAISS index...\")\n",
    "    vectorstore = FAISS.from_documents(chunked_docs, embeddings)\n",
    "    vectorstore.save_local(INDEX_DIR)  # Save the newly created index\n",
    "\n",
    "# Load or create the retriever\n",
    "def save_retriever(retriever, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(retriever, f)\n",
    "\n",
    "def load_retriever(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Save retriever to file\n",
    "if os.path.exists(RETRIEVER_FILE):\n",
    "    print(\"âœ… Loading saved retriever...\")\n",
    "    retriever_faiss = load_retriever(RETRIEVER_FILE)\n",
    "else:\n",
    "    print(\"âœ… Creating new retriever...\")\n",
    "    retriever_faiss = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",  # or similarity\n",
    "        search_kwargs={\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
    "    )\n",
    "    save_retriever(retriever_faiss, RETRIEVER_FILE)  # Save retriever to file\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"âœ… FAISS index built at: {INDEX_DIR}\")\n",
    "print(f\"âœ… Chunks indexed: {vectorstore.index.ntotal}\")\n",
    "print(\"âœ… Retriever ready: retriever.get_relevant_documents(<query>)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9711b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\doc_3.pdf 2 . Ù…Ø§Ø¯Ù‡ Û´- Ø§Ø­Ø±Ø§Ø² ÙˆÙ„Ø§Ø¯Øª ÙØ±Ø²Ù†Ù‡ Ù…ØªÙ‚Ø§Ø¶ÛŒØ§Ù† Ø¯Ø±ÛŒØ§ÙØª ØªØ³Ù‡ÛŒÙ„Ø§Øª Ù…Ø¯Ø¶Ø¯Ø¹ Ù‚Ø§Ù†Ø¯Ù†ØŒ Ø¨Ø§ÛŒÙ‡ ØªØ¯Ø³Ø· Ø¨Ø§Ù†Ú© Ø¹Ø§Ù…Ù„ Û±- Ø¨Ø§ Ù…Ù„Ø§Ø­Ø¸Ù‡ Ø§ØµÙ„ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡ Ø²ÙˆØ¬ØŒ Ø²ÙˆØ¬Ù‡ ...\n",
      ".\\data\\doc_8.pdf 0 . ir Ù…Ø¯ÛŒØ±Ø§Ù† Ø¹Ø§Ù…Ù„ Ù…Ø­ØªØ±Ù… Ø¨Ø§Ù†Ú©Ù‡Ø§ÛŒ Ù…Ù„ÛŒ Ø§ ÛŒØ±Ø§Ù†ØŒ Ø¨Ø§Ù†Ú© Ø³Ù¾Ù‡ØŒ ØªØ¬Ø§Ø±ØªØŒ Ø±ÙØ§Ù‡ Ú©Ø§Ø±Ú¯Ø±Ø§Ù†ØŒ ØµØ§Ø¯Ø±Ø§Øª Ø§ÛŒØ±Ø§Ù†ØŒ Ú©Ø´Ø§ÙˆØ±Ø²ÛŒØŒ Ù…Ù„ØªØŒ Ù…Ø³Ú©Ù†ØŒ ØªÙˆØ³Ø¹Ù‡ ØªØ¹Ø§ÙˆÙ†ØŒ ...\n",
      ".\\data\\doc_7.pdf 0 . Ø¯Ù… Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ø±ÛŒÚ© Ø§Ø² Ø¨Ù†Ø¯Ù‡Ø§ÛŒ Ø§ÛŒÙ† ØªØ¨ØµØ±Ù‡ (Ù…Ø´ØªÙ…Ù„ Ø¨Ø± ØªØ£Ø®ÛŒØ± Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø®Øª Ø§ÛŒÙ† ØªØ³Ù‡ÛŒÙ„Ø§Øª ÛŒØ§ Ø¯Ø±ÛŒØ§ÙØª Ø¶Ù…Ø§Ù†Øª ÙØ±Ø§ØªØ± Ø§Ø² Ø­Ø¯ÙˆØ¯ Ø§ÛŒÙ† Ù‚Ø§Ù†ÙˆÙ†) ØªØ§Ù„Ù  ...\n",
      ".\\data\\doc_8.pdf 0 Ø› ØªØ³Ù‡ØªÙ„Ø§Øª Ù‚Ø±Ø¶Ø§Ù„Ø­Ø³Ù†Ù‡ Ø§ Ø¯ÙˆØ§Ø¬ØŒ ØªØ³Ù‡ØªÙ„Ø§Øª Ù‚Ø±Ø¶Ø§Ù„Ø­Ø³Ù†Ù‡ ÙØ± Ù†Ø¯Ø¢ÙˆØ±ÛŒØŒ ØªØ³Ù‡ØªÙ„Ø§Øª Ù‚Ø±Ø¶Ø§Ù„Ø­Ø³Ù†Ù‡ ÙˆØ¯ÛŒÛŒØªÙ‡ ÛŒØªØ§ Ø§Ø±ÛŒØªØ¯ ÛŒØªØ§ Ø³Ø§Ø§Øª Ù…Ø³Ú©Ø± Ø¨Ù‡ Ø´Ø±Ø­ ÛŒØ± Ø§Ù‚Ø¯Ø§ ...\n",
      ".\\data\\doc_1.pdf 0 ØªÙ‡Ø±Ø§Ù†â€“ Ø¨Ù„ÙˆØ§Ø± Ù…ÛŒØ±Ø¯Ø§Ù…Ø§Ø¯ â€“ Ù¾Ù„Ø§Ú© Û±Û¹Û¸ ØªÙ„ÙÙ†Û²Û¹Û¹ÛµÛ± Ú©Ø¯Ù¾Ø³ØªÛŒÛ³Û³Û±Û±Û±-Û±ÛµÛ´Û¹Û¶ ÙØ§Ú©Ø³:Û¶Û¶Û·Û³ÛµÛ¶Û·Û´ Ø³Ø§ÛŒØª Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒwww. cbi. ir Ù…Ø¯ÛŒØ±Ø§Ù† Ø¹Ø§Ù…Ù„ Ù…Ø­ØªØ±Ù… Ø¨ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoteBook\\AppData\\Local\\Temp\\ipykernel_9948\\3306283593.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  results = retriever_faiss.get_relevant_documents(\"Ù…ÛŒØªÙˆØ§Ù†ÛŒ Ú©Ù…ÛŒ Ø±Ø§Ø¬Ø¨ ØªØ³Ù‡ÛŒÙ„Ø§Øª ÙØ±Ø²Ù†Ø¯Ø¢ÙˆØ±ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯ÛŒ.\")\n"
     ]
    }
   ],
   "source": [
    "results = retriever_faiss.get_relevant_documents(\"Ù…ÛŒØªÙˆØ§Ù†ÛŒ Ú©Ù…ÛŒ Ø±Ø§Ø¬Ø¨ ØªØ³Ù‡ÛŒÙ„Ø§Øª ÙØ±Ø²Ù†Ø¯Ø¢ÙˆØ±ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯ÛŒ.\")\n",
    "for r in results:\n",
    "    print(r.metadata.get(\"source\"), r.metadata.get(\"page\"), r.page_content[:120], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39245c",
   "metadata": {},
   "source": [
    "## Parent-Document Retriever (Chroma + Local DocStore) - method 2\n",
    "\n",
    "Implements a hierarchical RAG pipeline for Persian text using **ParentDocumentRetriever**.  \n",
    "Key steps:\n",
    "- **Text Normalization (Hazm)** for Persian script cleanup.  \n",
    "- **Parentâ€“Child Chunking** with `RecursiveCharacterTextSplitter`.  \n",
    "- **Embeddings:** Persian HuggingFace model for semantic vectors.  \n",
    "- **Vector Store:** `Chroma` for child chunk embeddings.  \n",
    "- **DocStore:** Custom `PickleDocStore` (extends `BaseStore`) to persist parent documents locally.  \n",
    "- **Retriever:** MMR-based search for relevant, diverse document retrieval.  \n",
    "- **Persistence:** Saves both retriever (`retriever.pkl`) and vector index for reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- RAG Option 2: Parent-Document Retriever (Chroma + Local DocStore, Persian-aware) ---\n",
    "# import os\n",
    "# import uuid\n",
    "# from typing import List\n",
    "# from langchain.docstore.document import Document\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.retrievers import ParentDocumentRetriever\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain.storage import LocalFileStore\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# import hazm\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# INDEX_DIR = os.path.join(\".\", \"vectorstores\", \"parent_chroma\")\n",
    "# PARENT_STORE_DIR = os.path.join(\".\", \"parent_store\")\n",
    "# RETRIEVER_FILE = os.path.join(INDEX_DIR, \"retriever.pkl\")\n",
    "# os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "# os.makedirs(PARENT_STORE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _normalizer = hazm.Normalizer(persian_numbers=True, persian_style=True)\n",
    "\n",
    "# def normalize_fa(text: str) -> str:\n",
    "#     if not text:\n",
    "#         return text\n",
    "#     t = _normalizer.normalize(text)\n",
    "#     t = t.replace(\"ÙŠ\", \"ÛŒ\").replace(\"Ùƒ\", \"Ú©\")  # Arabic -> Persian forms\n",
    "#     t = t.replace(\"Ù€\", \"\").replace(\"\\u200d\", \"\")  # remove tatweel & ZWJ\n",
    "#     return \" \".join(t.split())  # Collapse extra whitespaces\n",
    "\n",
    "# def normalize_docs(docs: List[Document]) -> List[Document]:\n",
    "#     out = []\n",
    "#     for d in docs:\n",
    "#         normalized_content = normalize_fa(d.page_content)\n",
    "#         out.append(Document(page_content=normalized_content, metadata=dict(d.metadata or {})))\n",
    "#     return out\n",
    "\n",
    "# normalized_docs = normalize_docs(docs)  # <-- your in-memory docs\n",
    "\n",
    "# # ---- Splitters (Parent: bigger, Child: smaller) ----\n",
    "# parent_splitter = RecursiveCharacterTextSplitter(\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \"Û”\", \".\", \"ØŸ\", \"!\", \"Ø›\", \":\", \"ØŒ\", \"Ù¬\", \"\\u200c\", \" \", \"\"],\n",
    "#     chunk_size=1500,\n",
    "#     chunk_overlap=150,\n",
    "#     is_separator_regex=False,\n",
    "# )\n",
    "\n",
    "# child_splitter = RecursiveCharacterTextSplitter(\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \"Û”\", \".\", \"ØŸ\", \"!\", \"Ø›\", \":\", \"ØŒ\", \"Ù¬\", \"\\u200c\", \" \", \"\"],\n",
    "#     chunk_size=400,\n",
    "#     chunk_overlap=100,\n",
    "#     is_separator_regex=False,\n",
    "# )\n",
    "\n",
    "# # ---- Vector store (child vectors) ----\n",
    "# vectorstore = Chroma(\n",
    "#     collection_name=\"parent_child_index\",\n",
    "#     embedding_function=embeddings,\n",
    "#     persist_directory=INDEX_DIR,\n",
    "# )\n",
    "\n",
    "# # ---- Parent docstore ----\n",
    "# # ---- Parent docstore ----\n",
    "# # Adapter: serialize Document <-> bytes over LocalFileStore, AND subclass BaseStore\n",
    "# import pickle\n",
    "# from typing import List, Tuple, Optional, Iterator\n",
    "# from langchain.storage import LocalFileStore, BaseStore\n",
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# class PickleDocStore(BaseStore[str, Document]):\n",
    "#     def __init__(self, base_store: LocalFileStore):\n",
    "#         self._base = base_store  # stores raw bytes on disk\n",
    "\n",
    "#     # List[(key, Document)] -> bytes\n",
    "#     def mset(self, key_value_pairs: List[Tuple[str, Document]]) -> None:\n",
    "#         to_bytes = [(k, pickle.dumps(v)) for k, v in key_value_pairs]\n",
    "#         self._base.mset(to_bytes)\n",
    "\n",
    "#     # List[key] -> List[Optional[Document]]\n",
    "#     def mget(self, keys: List[str]) -> List[Optional[Document]]:\n",
    "#         raw = self._base.mget(keys)\n",
    "#         return [pickle.loads(b) if b is not None else None for b in raw]\n",
    "\n",
    "#     # delete by keys\n",
    "#     def mdelete(self, keys: List[str]) -> None:\n",
    "#         self._base.mdelete(keys)\n",
    "\n",
    "#     # iterate keys\n",
    "#     def yield_keys(self) -> Iterator[str]:\n",
    "#         yield from self._base.yield_keys()\n",
    "\n",
    "# # instantiate the byte store + wrapped BaseStore\n",
    "# _byte_store = LocalFileStore(PARENT_STORE_DIR)\n",
    "# docstore = PickleDocStore(_byte_store)\n",
    "\n",
    "\n",
    "\n",
    "# # ---- Build Parent-Document Retriever ----\n",
    "# retriever = ParentDocumentRetriever(\n",
    "#     vectorstore=vectorstore,\n",
    "#     docstore=docstore,\n",
    "#     child_splitter=child_splitter,\n",
    "#     parent_splitter=parent_splitter,\n",
    "#     search_type=\"mmr\",\n",
    "#     search_kwargs={\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5},\n",
    "# )\n",
    "\n",
    "# # Stable IDs per parent doc (use metadata if available, else UUID)\n",
    "# def doc_id(d: Document, i: int) -> str:\n",
    "#     src = (d.metadata or {}).get(\"source\")\n",
    "#     return f\"{src}::p{i}\" if src else f\"doc-{i}-{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "# # Use ALL normalized docs as parents\n",
    "# parent_docs = normalized_docs  # <-- essential change (no filtering)\n",
    "# ids = [doc_id(d, i) for i, d in enumerate(parent_docs)]  # (ok to keep even if not used)\n",
    "\n",
    "# # ---- Save/Load retriever functions ----\n",
    "# def save_retriever(retriever, file_path):\n",
    "#     with open(file_path, \"wb\") as f:\n",
    "#         pickle.dump(retriever, f)\n",
    "\n",
    "# def load_retriever(file_path):\n",
    "#     with open(file_path, \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# # ---- Check if retriever exists, otherwise create and save it ----\n",
    "# if os.path.exists(RETRIEVER_FILE):\n",
    "#     print(\"Loading saved retriever...\")\n",
    "#     retriever = load_retriever(RETRIEVER_FILE)\n",
    "# else:\n",
    "#     print(\"Creating new retriever...\")\n",
    "#     retriever.add_documents(parent_docs)  # parents â†’ split to children internally\n",
    "#     save_retriever(retriever, RETRIEVER_FILE)\n",
    "\n",
    "# # Persist child vectors on disk\n",
    "# vectorstore.persist()\n",
    "\n",
    "# # ---- Debug and Status Output ----\n",
    "# print(\"Parent-Document retriever ready.\")\n",
    "# print(f\"Child vectors: {vectorstore._collection.count()}\")\n",
    "# print(f\"Parent store dir: {PARENT_STORE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9817ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Ù…ÛŒØªÙˆØ§Ù†ÛŒ Ú©Ù…ÛŒ Ø±Ø§Ø¬Ø¨ ØªØ³Ù‡ÛŒÙ„Ø§Øª ÙØ±Ø²Ù†Ø¯Ø¢ÙˆØ±ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯ÛŒ.\"\n",
    "# results = retriever.get_relevant_documents(query)\n",
    "# for r in results:\n",
    "#     print(r.metadata.get(\"source\"), \"â€”\", r.page_content[:120], \"â€¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe75348",
   "metadata": {},
   "source": [
    "## Hybrid Persian Document Retrieval System (TF-IDF + Weaviate)\n",
    "\n",
    "This notebook demonstrates a **hybrid information retrieval pipeline** for **Persian text documents**, integrating both **sparse (TF-IDF)** and **dense (embedding-based)** search mechanisms using **LangChain**, **Weaviate**, and **Hugging Face embeddings**.\n",
    "\n",
    "Key highlights:\n",
    "- **Persian text normalization** via `hazm` for consistent linguistic preprocessing.  \n",
    "- **Hierarchical document chunking** using `RecursiveCharacterTextSplitter` to optimize retrieval granularity.  \n",
    "- **Hybrid retriever architecture** combining `TFIDFRetriever` and `Weaviate` vector store through `EnsembleRetriever`.  \n",
    "- **Persistence support** for reusing fitted vectorizers and retriever configurations using `joblib`.  \n",
    "- **Fully compatible with LangChainâ€™s retriever interface** for seamless integration into downstream QA or RAG pipelines.  \n",
    "\n",
    "This implementation reflects a production-ready approach to **hybrid semantic search** for Persian-language corpora.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2279530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from typing import List\n",
    "# import weaviate\n",
    "# import hazm\n",
    "# from transformers import AutoTokenizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import joblib\n",
    "# from langchain.docstore.document import Document\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_community.vectorstores import Weaviate\n",
    "# from langchain_community.retrievers import TFIDFRetriever\n",
    "# from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# INDEX_DIR = os.path.join(\".\", \"vectorstores\", \"weaviate\")\n",
    "# RETRIEVER_FILE = os.path.join(INDEX_DIR, \"hybrid_retriever.joblib\")\n",
    "# os.makedirs(INDEX_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963e449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weaviate ready: True\n",
      "schema: {'classes': [{'class': 'Persian_documents_index', 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2}, 'cleanupIntervalSeconds': 60, 'stopwords': {'additions': None, 'preset': 'en', 'removals': None}}, 'multiTenancyConfig': {'enabled': False}, 'properties': [{'dataType': ['text'], 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'indexFilterable': True, 'indexSearchable': True, 'name': 'source', 'tokenization': 'word'}, {'dataType': ['number'], 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'indexFilterable': True, 'indexSearchable': False, 'name': 'total_pages'}, {'dataType': ['text'], 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'indexFilterable': True, 'indexSearchable': True, 'name': 'author', 'tokenization': 'word'}, {'dataType': ['text'], 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'indexFilterable': True, 'indexSearchable': True, 'name': 'producer', 'tokenization': 'word'}, {'dataType': ['text'], 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'indexFilterable': True, 'indexSearchable': True, 'name': 'creator', 'tokenization': 'word'}, {'dataType': ['date'], 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'indexFilterable': True, 'indexSearchable': False, 'name': 'creationdate'}, {'dataType': ['date'], 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'indexFilterable': True, 'indexSearchable': False, 'name': 'moddate'}, {'dataType': ['number'], 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'indexFilterable': True, 'indexSearchable': False, 'name': 'page'}, {'dataType': ['text'], 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'indexFilterable': True, 'indexSearchable': True, 'name': 'page_label', 'tokenization': 'word'}, {'dataType': ['text'], 'description': \"This property was generated by Weaviate's auto-schema feature on Fri Nov  7 18:34:19 2025\", 'indexFilterable': True, 'indexSearchable': True, 'name': 'text', 'tokenization': 'word'}], 'replicationConfig': {'factor': 1}, 'shardingConfig': {'virtualPerPhysical': 128, 'desiredCount': 1, 'actualCount': 1, 'desiredVirtualCount': 128, 'actualVirtualCount': 128, 'key': '_id', 'strategy': 'hash', 'function': 'murmur3'}, 'vectorIndexConfig': {'skip': False, 'cleanupIntervalSeconds': 300, 'maxConnections': 64, 'efConstruction': 128, 'ef': -1, 'dynamicEfMin': 100, 'dynamicEfMax': 500, 'dynamicEfFactor': 8, 'vectorCacheMaxObjects': 1000000000000, 'flatSearchCutoff': 40000, 'distance': 'cosine', 'pq': {'enabled': False, 'bitCompression': False, 'segments': 0, 'centroids': 256, 'trainingLimit': 100000, 'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}, 'bq': {'enabled': False}}, 'vectorIndexType': 'hnsw', 'vectorizer': 'none'}]}\n"
     ]
    }
   ],
   "source": [
    "# client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# print(\"weaviate ready:\", client.is_ready())\n",
    "# print(\"schema:\", client.schema.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88993a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs: 23  â†’  chunks: 121\n",
      "Loading saved retriever...\n",
      "âœ… Hybrid (TF-IDF + Weaviate) retriever ready.\n",
      "schema classes: ['Persian_documents_index']\n"
     ]
    }
   ],
   "source": [
    "# TFIDF_PARAMS = {\"max_features\": 5000, \"ngram_range\": (1, 3)}\n",
    "\n",
    "# # ---- Persian normalization ----\n",
    "# _normalizer = hazm.Normalizer(persian_numbers=True, persian_style=True)\n",
    "# def normalize_fa(text: str) -> str:\n",
    "#     if not text:\n",
    "#         return text\n",
    "#     t = _normalizer.normalize(text)\n",
    "#     t = t.replace(\"ÙŠ\", \"ÛŒ\").replace(\"Ùƒ\", \"Ú©\")       # Arabic -> Persian forms\n",
    "#     t = t.replace(\"Ù€\", \"\").replace(\"\\u200d\", \"\")     # remove tatweel & ZWJ (keep ZWNJ)\n",
    "#     return \" \".join(t.split())                       # collapse extra whitespaces\n",
    "\n",
    "# def normalize_docs(docs: List[Document]) -> List[Document]:\n",
    "#     out = []\n",
    "#     for d in docs:\n",
    "#         out.append(Document(page_content=normalize_fa(d.page_content),\n",
    "#                             metadata=dict(d.metadata or {})))\n",
    "#     return out\n",
    "\n",
    "# # ---- splitters (parent / child) ----\n",
    "# parent_splitter = RecursiveCharacterTextSplitter(\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \"Û”\", \".\", \"ØŸ\", \"!\", \"Ø›\", \":\", \"ØŒ\", \"Ù¬\", \"\\u200c\", \" \", \"\"],\n",
    "#     chunk_size=1500,\n",
    "#     chunk_overlap=150,\n",
    "#     is_separator_regex=False,\n",
    "# )\n",
    "# child_splitter = RecursiveCharacterTextSplitter(\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \"Û”\", \".\", \"ØŸ\", \"!\", \"Ø›\", \":\", \"ØŒ\", \"Ù¬\", \"\\u200c\", \" \", \"\"],\n",
    "#     chunk_size=400,\n",
    "#     chunk_overlap=100,\n",
    "#     is_separator_regex=False,\n",
    "# )\n",
    "\n",
    "# # ---- chunking ----\n",
    "# def chunk_documents(docs: List[Document]) -> List[Document]:\n",
    "#     chunks = []\n",
    "#     for d in docs:\n",
    "#         parents = parent_splitter.split_documents([d])\n",
    "#         for p in parents:\n",
    "#             chunks.extend(child_splitter.split_documents([p]))\n",
    "#     return chunks\n",
    "\n",
    "# # 1) normalize\n",
    "# normalized_docs = normalize_docs(docs)          # <-- 'docs' must exist in your notebook\n",
    "# # 2) chunk\n",
    "# chunked_docs = chunk_documents(normalized_docs)\n",
    "# print(f\"docs: {len(docs)}  â†’  chunks: {len(chunked_docs)}\")\n",
    "\n",
    "# # ---- Weaviate VectorStore (dense) ----\n",
    "# dense_vs = Weaviate(\n",
    "#     client=client,\n",
    "#     index_name=\"persian_documents_index\",\n",
    "#     text_key=\"text\",          # where chunk text will be stored in Weaviate objects\n",
    "#     embedding=embeddings,     # embedding function (already created in your code)\n",
    "#     by_text=False \n",
    "# )\n",
    "\n",
    "# # ---- save/load helpers (persist TF-IDF + ensemble weights only) ----\n",
    "# def save_retriever(hybrid: EnsembleRetriever, file_path: str) -> None:\n",
    "#     tfidf_vec = hybrid.retrievers[0].vectorizer  # fitted TfidfVectorizer (first retriever is TF-IDF)\n",
    "#     data = {\n",
    "#         \"weights\": hybrid.weights,\n",
    "#         \"tfidf_vectorizer\": tfidf_vec,           # save fitted vectorizer directly\n",
    "#         \"index_name\": \"persian_documents_index\",\n",
    "#         \"text_key\": \"text\",\n",
    "#     }\n",
    "#     joblib.dump(data, file_path)\n",
    "\n",
    "# def load_retriever(file_path: str, chunked: List[Document]) -> EnsembleRetriever:\n",
    "#     data = joblib.load(file_path)\n",
    "\n",
    "#     # rebuild TF-IDF retriever with loaded fitted vectorizer\n",
    "#     vec: TfidfVectorizer = data[\"tfidf_vectorizer\"]\n",
    "#     texts = [d.page_content for d in chunked]\n",
    "#     tfidf_array = vec.transform(texts)  # transform (vectorizer already fitted)\n",
    "#     tfidf_docs = [Document(page_content=t, metadata=chunked[i].metadata) for i, t in enumerate(texts)]\n",
    "#     tfidf_ret = TFIDFRetriever(vectorizer=vec, docs=tfidf_docs, tfidf_array=tfidf_array)\n",
    "\n",
    "#     # reconnect to the same Weaviate index\n",
    "#     dense = Weaviate(\n",
    "#         client=client,\n",
    "#         index_name=data[\"index_name\"],\n",
    "#         text_key=data[\"text_key\"],\n",
    "#         embedding=embeddings,\n",
    "#         by_text=False \n",
    "#     )\n",
    "\n",
    "#     return EnsembleRetriever(\n",
    "#         retrievers=[tfidf_ret, dense.as_retriever()],\n",
    "#         weights=data[\"weights\"],\n",
    "#     )\n",
    "\n",
    "# # ---- build or load hybrid retriever ----\n",
    "# if os.path.exists(RETRIEVER_FILE):\n",
    "#     print(\"Loading saved retriever...\")\n",
    "#     retriever = load_retriever(RETRIEVER_FILE, chunked_docs)\n",
    "# else:\n",
    "#     print(\"Creating new retriever...\")\n",
    "\n",
    "#     # upsert chunks to Weaviate (server persists)\n",
    "#     dense_vs.add_documents(chunked_docs)\n",
    "\n",
    "#     # TF-IDF on the same chunked corpus (use params dict, not a prebuilt vectorizer)\n",
    "#     tfidf_ret = TFIDFRetriever.from_documents(chunked_docs, tfidf_params=TFIDF_PARAMS)\n",
    "\n",
    "#     # assemble hybrid (TF-IDF + dense)\n",
    "#     retriever = EnsembleRetriever(\n",
    "#         retrievers=[tfidf_ret, dense_vs.as_retriever()],\n",
    "#         weights=[0.4, 0.6],\n",
    "#     )\n",
    "#     save_retriever(retriever, RETRIEVER_FILE)\n",
    "\n",
    "# print(\"âœ… Hybrid (TF-IDF + Weaviate) retriever ready.\")\n",
    "# print(\"schema classes:\", [c[\"class\"] for c in client.schema.get().get(\"classes\", [])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Ù…ÛŒØªÙˆØ§Ù†ÛŒ Ú©Ù…ÛŒ Ø±Ø§Ø¬Ø¨ ØªØ³Ù‡ÛŒÙ„Ø§Øª ÙØ±Ø²Ù†Ø¯Ø¢ÙˆØ±ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯ÛŒ.\"\n",
    "# q_norm = normalize_fa(query)  # keep query normalization consistent with docs\n",
    "\n",
    "# results = retriever.get_relevant_documents(q_norm)\n",
    "\n",
    "# for i, r in enumerate(results, start=1):\n",
    "#     meta = r.metadata or {}\n",
    "#     src = meta.get(\"source\") or meta.get(\"file_path\") or meta.get(\"doc_id\") or \"-\"\n",
    "#     preview = (r.page_content or \"\").strip().replace(\"\\n\", \" \")[:160]\n",
    "#     print(f\"{i:02d}. {src} â€” {preview} â€¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16a043",
   "metadata": {},
   "source": [
    "## Chain\n",
    "\n",
    "This code implements a **Retrieval-Augmented Generation (RAG)** pipeline for **Persian text processing**. The architecture combines **retrieval-based search** with **generation-based models** to answer user queries by pulling relevant information from pre-loaded Persian documents.\n",
    "\n",
    "Key components:\n",
    "- **Retriever**: Uses the **FAISS** vector store to fetch relevant Persian documents based on the user's query.\n",
    "- **Prompt Template**: A **Persian-language prompt** is designed to inject the retrieved context and user question into a model, generating contextually relevant answers.\n",
    "- **LLM (Large Language Model)**: The model processes the query and relevant context, generating a **natural language response** in Persian.\n",
    "- **Output Parsing**: The raw output from the model is parsed into a readable, clean format for user-friendly display.\n",
    "\n",
    "### Workflow:\n",
    "1. **User Query**: The user's question is passed into the retrieval system.\n",
    "2. **Context Retrieval**: The relevant document chunks are retrieved based on the query.\n",
    "3. **Prompt Construction**: The retrieved context and user question are combined into a prompt.\n",
    "4. **LLM Generation**: The language model generates a response based on the provided context.\n",
    "5. **Output Parsing**: The response is cleaned and returned to the user.\n",
    "\n",
    "This approach enhances the ability to answer domain-specific queries in **Persian** by leveraging document retrieval and **fine-tuned language models**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa733ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.5\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "print(huggingface_hub.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc77cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "import getpass\n",
    "import requests\n",
    "\n",
    "class HuggingFaceInferenceRunnable(Runnable):\n",
    "    def __init__(self, model_id: str, huggingface_token: str):\n",
    "        self.model_id = model_id\n",
    "        self.huggingface_token = huggingface_token\n",
    "        self.inference_url = \"https://router.huggingface.co/hf-inference\"\n",
    "\n",
    "    def invoke(self, input_dict, config: dict = None, **kwargs) -> dict:\n",
    "        # Check if the input is a ChatPromptValue, and extract the question\n",
    "        if isinstance(input_dict, dict):\n",
    "            query = input_dict.get(\"question\")\n",
    "        elif hasattr(input_dict, 'to_string'):\n",
    "            # If it's a ChatPromptValue, convert it to string and extract the question\n",
    "            query = input_dict.to_string().strip()\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input type: {type(input_dict)}\")\n",
    "\n",
    "        # Prepare the headers for the request\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.huggingface_token}\",\n",
    "        }\n",
    "\n",
    "        # Prepare the payload (input data for the model)\n",
    "        payload = {\n",
    "            \"inputs\": query,  # The input question to the model\n",
    "            \"parameters\": {\"temperature\": 0.3, \"max_new_tokens\": 512},\n",
    "        }\n",
    "\n",
    "        # Send a POST request to the inference endpoint\n",
    "        response = requests.post(self.inference_url, headers=headers, json=payload)\n",
    "\n",
    "        # Check for successful response\n",
    "        if response.status_code == 200:\n",
    "            # Extract and return the model's output\n",
    "            output = response.json()\n",
    "            return {\"response\": output}  # Return the model's response as a dictionary\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "huggingface_token = getpass.getpass(\"Enter your Hugging Face API token: \")\n",
    "\n",
    "hf_inference_runnable = HuggingFaceInferenceRunnable(\n",
    "    model_id=\"HooshvareLab/bert-fa-base-uncased\",  ############### Whatever model i used i couldnt get it using the wrapper of the langchain for huggingface\n",
    "    huggingface_token=huggingface_token\n",
    ")\n",
    "\n",
    "# Prompt Template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡ Ø§Ø² Ø§Ø³Ù†Ø§Ø¯ØŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯.\n",
    "Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒØŒ Ø¨Ú¯ÙˆÛŒÛŒØ¯ Ú©Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ÛŒ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\n",
    "\n",
    "ğŸ§  Ù…ØªÙ† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡:\n",
    "{context}\n",
    "\n",
    "â“ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±:\n",
    "{question}\n",
    "\n",
    "âœï¸ Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ (Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ùˆ Ø±ÙˆØ§Ù†):\n",
    "\"\"\")\n",
    "\n",
    "# Output Parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# RAG Setup\n",
    "RAG = RunnableParallel(\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever_faiss,  # or 'retriever' if hybrid\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Chain\n",
    "chain = RAG | prompt | hf_inference_runnable | output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb7698ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error: 404 - Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mÙ…ÛŒØªÙˆØ§Ù†ÛŒ Ú©Ù…ÛŒ Ø±Ø§Ø¬Ø¨ ØªØ³Ù‡ÛŒÙ„Ø§Øª ÙØ±Ø²Ù†Ø¯Ø¢ÙˆØ±ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯ÛŒØŸ\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mÙ¾Ø§Ø³Ø® Ù…Ø¯Ù„:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NoteBook\\Desktop\\programing\\Gen AI\\GenAI-course\\CA_04_RAG_Chain_Personal_Chatbot\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2878\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   2876\u001b[39m             \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   2877\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2878\u001b[39m             \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[32m   2879\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   2880\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mHuggingFaceInferenceRunnable.invoke\u001b[39m\u001b[34m(self, input_dict, config, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: output}  \u001b[38;5;66;03m# Return the model's response as a dictionary\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Error: 404 - Not Found"
     ]
    }
   ],
   "source": [
    "query = \"Ù…ÛŒØªÙˆØ§Ù†ÛŒ Ú©Ù…ÛŒ Ø±Ø§Ø¬Ø¨ ØªØ³Ù‡ÛŒÙ„Ø§Øª ÙØ±Ø²Ù†Ø¯Ø¢ÙˆØ±ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯ÛŒØŸ\"\n",
    "result = chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„:\")\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
