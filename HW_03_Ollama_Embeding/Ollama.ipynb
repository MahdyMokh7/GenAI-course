{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Assignment â€” Ollama Setup & Usage\n",
        "**Course:** Gen AI  \n",
        "**Author:** Mehdy Mokhtari  \n",
        "**Date:** 1/8/1404\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“˜ Introduction\n",
        "\n",
        "In this assignment, we explore **Ollama**, an open-source platform for running and serving large language models (LLMs) locally or in Google Colab. The goal is to understand how to **install, configure, and interact with a local language model (LLaMA 3.1 â€“ 7B)** through **Ollama** and **LangChain**.\n",
        "\n",
        "By the end of this part, you will:\n",
        "- Install and run **Ollama** within Google Colab.\n",
        "- Set up and serve a **local LLM instance** (LLaMA 3.1, 7B parameters).\n",
        "- Connect to this instance using **LangChainâ€™s `langchain_ollama`** library.\n",
        "- Send and analyze **prompts in English and Persian**.\n",
        "- Understand how local LLM serving works compared to cloud-hosted APIs.\n",
        "\n",
        "---\n",
        "\n",
        "## What Weâ€™ll Learn\n",
        "\n",
        "- Basics of **Ollama installation** and running models locally.\n",
        "- How to **serve models** and keep them active in the Colab environment.\n",
        "- How to **connect to Ollama** through the **LangChain** interface.\n",
        "- How to **test model performance** by sending prompts and observing outputs.\n",
        "- Foundational skills for working with **self-hosted AI models** in constrained environments.\n",
        "\n"
      ],
      "metadata": {
        "id": "U1NRLednEc68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Install Ollama in Colab"
      ],
      "metadata": {
        "id": "ywsdRU4qF4bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the package list and install system utilities that help detect hardware (GPU)\n",
        "!apt update && apt install -y pciutils lshw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_19B8dsvGcGF",
        "outputId": "acc373b8-eb11-4f57-8f70-0f2bdc84af5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [Connecting to security.\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "lshw is already the newest version (02.19.git.2021.06.19.996aaad9c7-2build1).\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a GPU (like T4) is available in your Colab runtime\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl6utbK5HXf3",
        "outputId": "08104ca3-5dfd-40d5-db53-bce070fefacf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 24 17:41:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iktl2incHpxS",
        "outputId": "45d78abc-34d4-4ed1-c63c-7d2cb5212e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Download and install Ollama on the current Colab environment\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the Ollama server in the background so it can handle requests\n",
        "# !nohup ollama serve &\n",
        "\n",
        "\n",
        "# Start the Ollama service so other cells can talk to it\n",
        "!nohup ollama serve > /dev/null 2>&1 &\n",
        "# Give it a second to boot up\n",
        "!sleep 2\n",
        "# Check that the API is live (returns JSON, maybe empty)\n",
        "!curl -s http://localhost:11434/api/tags\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJcxBaYPHV-G",
        "outputId": "f7448b10-b053-4580-cdb4-ff727d27eef4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"models\":[{\"name\":\"llama3.1:latest\",\"model\":\"llama3.1:latest\",\"modified_at\":\"2025-10-24T17:32:47.547414907Z\",\"size\":4920753328,\"digest\":\"46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"8.0B\",\"quantization_level\":\"Q4_K_M\"}}]}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jUdJbi_YQ8KF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Download (pull) the Llama 3.1 model (7B) so itâ€™s available locally for inference\n",
        "# !ollama pull llama3.1\n",
        "\n",
        "# Trigger model download without entering chat mode\n",
        "!curl -s http://localhost:11434/api/pull -d '{\"name\":\"llama3.1\"}' | tail -n 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CljR1y28HDeg",
        "outputId": "8de964ed-b258-4ff4-d4f0-f441e8761d1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"status\":\"success\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists all downloaded models\n",
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DlJvT2RRAcn",
        "outputId": "30b7a8c4-cbc2-428a-a75d-b362644a798a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME               ID              SIZE      MODIFIED               \n",
            "llama3.1:latest    46e0c10c039e    4.9 GB    Less than a second ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show details about llama3.1\n",
        "!ollama show llama3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE_NH_fyRD3l",
        "outputId": "a487e8e5-e1aa-4e28-da3b-8e1152b7b277"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model\n",
            "    architecture        llama     \n",
            "    parameters          8.0B      \n",
            "    context length      131072    \n",
            "    embedding length    4096      \n",
            "    quantization        Q4_K_M    \n",
            "\n",
            "  Capabilities\n",
            "    completion    \n",
            "    tools         \n",
            "\n",
            "  Parameters\n",
            "    stop    \"<|start_header_id|>\"    \n",
            "    stop    \"<|end_header_id|>\"      \n",
            "    stop    \"<|eot_id|>\"             \n",
            "\n",
            "  License\n",
            "    LLAMA 3.1 COMMUNITY LICENSE AGREEMENT            \n",
            "    Llama 3.1 Version Release Date: July 23, 2024    \n",
            "    ...                                              \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm via API\n",
        "!curl -s http://localhost:11434/api/tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mf0OSwZRDz-",
        "outputId": "f58b1210-1368-49aa-8a72-b8a8da4d2090"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"models\":[{\"name\":\"llama3.1:latest\",\"model\":\"llama3.1:latest\",\"modified_at\":\"2025-10-24T17:42:03.721136849Z\",\"size\":4920753328,\"digest\":\"46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"8.0B\",\"quantization_level\":\"Q4_K_M\"}}]}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Llama 3.1 model interactively to test it â€” this also ensures itâ€™s properly loaded\n",
        "# !ollama run llama3.1\n",
        "\n",
        "\n",
        "# Run the model once with a single prompt\n",
        "!ollama run llama3.1 \"Say hello from Llama 3.1 in one sentence.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iooXYy3HUxK",
        "outputId": "7b798cec-f98f-4619-b2a2-0d8f354517c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hHello\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h I\u001b[?25l\u001b[?25h'm\u001b[?25l\u001b[?25h L\u001b[?25l\u001b[?25hlama\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h3\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h here\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h assist\u001b[?25l\u001b[?25h you\u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h any\u001b[?25l\u001b[?25h questions\u001b[?25l\u001b[?25h or\u001b[?25l\u001b[?25h tasks\u001b[?25l\u001b[?25h you\u001b[?25l\u001b[?25h may\u001b[?25l\u001b[?25h have\u001b[?25l\u001b[?25h!\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Connect to Ollama & Use it"
      ],
      "metadata": {
        "id": "50EFs1e3LLxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q langchain langchain-core langchain-community langchain-ollama"
      ],
      "metadata": {
        "id": "tuvOtI-7SGUX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "cZx8EcUKLSyN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a connection to the locally served Ollama model\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.1\",\n",
        "    base_url=\"http://localhost:11434\"\n",
        ")"
      ],
      "metadata": {
        "id": "TdIHBFQcLTpe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Simple test (English prompt) ---\n",
        "response_en = llm.invoke(\"Give me a short description of what Generative AI is.\")\n",
        "print(\"English Response:\\n\", response_en, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnIELqrnLvky",
        "outputId": "c7f7a957-07d0-4412-b427-0734855c6bbd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Response:\n",
            " content='Generative AI refers to a subset of Artificial Intelligence (AI) that uses algorithms and mathematical models to generate new, original content such as images, music, videos, text, or even code. This type of AI is trained on large datasets and learns patterns, relationships, and structures within the data, allowing it to create novel outputs that are often indistinguishable from human-created ones.\\n\\nGenerative AI has applications in various fields, including:\\n\\n* Art: generating realistic images, paintings, or sculptures\\n* Music: composing original music pieces\\n* Writing: creating short stories, articles, or even entire books\\n* Design: generating new product designs or visual concepts\\n\\nThe goal of Generative AI is to create something new and valuable from scratch, rather than simply manipulating or transforming existing content.' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-10-24T17:42:13.810841072Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4901544318, 'load_duration': 353785193, 'prompt_eval_count': 22, 'prompt_eval_duration': 37883982, 'eval_count': 159, 'eval_duration': 4124849135, 'model_name': 'llama3.1'} id='run--1f99ce00-ae13-439e-8f95-9246392d190a-0' usage_metadata={'input_tokens': 22, 'output_tokens': 159, 'total_tokens': 181} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Simple test (Persian prompt) ---\n",
        "response_fa = llm.invoke(\"ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…ÙˆÙ„Ø¯  Ø¨Ø¯Ù‡.\")\n",
        "print(\"Persian Response:\\n\", response_fa, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycuZo4rtLviE",
        "outputId": "26adfd5c-0721-4dfb-b20e-dffd94d84ecb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persian Response:\n",
            " content='Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…ÙˆÙ„Ø¯ ÛŒÚ© Ø²ÛŒØ± Ø´Ø§Ø®Ù‡ Ø§Ø² Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒà¹ƒà¸«à¸¡ Ùˆ Ø´Ø¨ÛŒÙ‡ Ø¨Ù‡ ÙˆØ§Ù‚Ø¹ÛŒØª Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø³Ø§Ù†Ù‡Ø§ Ø±ÙˆØ¨Ø±Ùˆ Ø§Ø³Øª .' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-10-24T17:42:15.07943419Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1254381756, 'load_duration': 188552385, 'prompt_eval_count': 25, 'prompt_eval_duration': 38076354, 'eval_count': 38, 'eval_duration': 930497964, 'model_name': 'llama3.1'} id='run--8e7f05dd-7719-48e2-9cc9-64bf2645f489-0' usage_metadata={'input_tokens': 25, 'output_tokens': 38, 'total_tokens': 63} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Optional: use a LangChain prompt template for more structured interaction ---\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a concise and knowledgeable AI assistant.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "WfdO4NnDLvfZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example (LangChain pipeline):\")\n",
        "print(chain.invoke({\"question\": \"Compare local and cloud-based LLMs in 3 bullet points.\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53TDbjqtLvZ0",
        "outputId": "b1a92d33-e959-4fa2-e860-14bc8d1f5e21"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example (LangChain pipeline):\n",
            "Here's a comparison of local and cloud-based Large Language Models (LLMs) in 3 bullet points:\n",
            "\n",
            "â€¢ **Processing Power**: Cloud-based LLMs have access to vast amounts of processing power, making them capable of handling complex tasks and large datasets. Local LLMs, on the other hand, are limited by the hardware specifications of their host device.\n",
            "\n",
            "â€¢ **Data Storage and Management**: Cloud-based LLMs store data externally, which allows for easier management, scalability, and collaboration. Local LLMs require local storage, which can lead to issues with data synchronization and access control.\n",
            "\n",
            "â€¢ **Connectivity and Security**: Cloud-based LLMs rely on internet connectivity, which introduces security risks if not properly managed. Local LLMs, being self-contained, are more secure but may face limitations in terms of model updates, maintenance, and integration with external services.\n"
          ]
        }
      ]
    }
  ]
}